<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-09-04T11:10:37+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seri Lee Blog</title><subtitle>This is where I write posts about my research field.</subtitle><entry><title type="html">Inverse Projection Transformation</title><link href="http://localhost:4000/inverse/" rel="alternate" type="text/html" title="Inverse Projection Transformation" /><published>2021-09-03T00:00:00+09:00</published><updated>2021-09-03T00:00:00+09:00</updated><id>http://localhost:4000/inverse</id><content type="html" xml:base="http://localhost:4000/inverse/">&lt;div class=&quot;typewriter&quot;&gt;
&lt;h2&gt; Depth and Inverse Projection&lt;/h2&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot;&gt; Test if caption works &lt;/div&gt;

&lt;p&gt;When an image of a scene is captured by camera, we &lt;span class=&quot;circle-sketch-highlight&quot;&gt; lose depth information &lt;/span&gt; as objects and points in 3D space are mapped onto a 2D image plane. This is also known as a &lt;span class=&quot;blue&quot;&gt; projective transformation &lt;/span&gt;, in which points in the world are converted to pixels on a 2D plane.&lt;/p&gt;

&lt;p&gt;However, what if we want to do the &lt;dfn&gt; inverse &lt;/dfn&gt;? That is, we want to recover and reconstruct the scene given only 2D image. To do that, we would need to know the depth or $Z$-component of each corresponding pixels. Depth can be represented as an image as shown in the figure above, with brighter intensity denoting points further away.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a class=&quot;second after&quot; href=&quot;https://towardsdatascience.com/inverse-projection-transformation-c866ccedef1c&quot;&gt; MEDIUM &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a 2D plane. But how do we do the opposite?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/12.png" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/12.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Depth Estimation: Basics and Intuition</title><link href="http://localhost:4000/depth/" rel="alternate" type="text/html" title="Depth Estimation: Basics and Intuition" /><published>2021-09-02T00:00:00+09:00</published><updated>2021-09-02T00:00:00+09:00</updated><id>http://localhost:4000/depth</id><content type="html" xml:base="http://localhost:4000/depth/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In computer vision, depth is extracted from 2 prevalent methodologies. Namely, &lt;em&gt;depth from monocular images&lt;/em&gt; (static or sequential) or &lt;em&gt;depth from stereo images&lt;/em&gt; by exploiting epipolar geometry. This article will focus on giving readers a background into depth estimation and the problems associated with it.&lt;/p&gt;

&lt;h2 id=&quot;how-we-view-the-world&quot;&gt;How We View the World&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s start with how humans perceive depth in general. This will give us some valuable insights on depth estimation since many of the methods were derived from our human vision system. Both machine and human vision share similarities in the way image is formed. Theoretically, when light rays from a source hit surfaces, it reflects off and directs towards the back of our retina, projecting them and our eye processes them as 2D just like how an image is formed on an image plane.&lt;/p&gt;

&lt;p&gt;So how do we actually measure distance and understand our environment in 3D when the projected scene is in 2D? The mechanism at work here is that our brain starts to reason about the incoming visual signals by recognizing patterns such as the size, texture and motion about the scene known as &lt;em&gt;depth cues&lt;/em&gt;. There is no distance information about the image but somehow we can interpret and recover depth information effortlessly. These cues allow us to view objects and surfaces which are supposedly on flat images as 3D.&lt;/p&gt;

&lt;h2 id=&quot;how-to-destroy-depth&quot;&gt;How to Destroy Depth&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interpreting these depth cues begins with how scenes are projected to perspective view in humans and camera vision. On the other hand, an &lt;em&gt;orthographic projection&lt;/em&gt; to front view or side view is one which destroys all depth information.&lt;/p&gt;

&lt;p&gt;Consider the above image. An observer could disentangle which aspect of the house is nearer to him/her as seen in the left image. However, it is totally impossible to distinguish relative distances from the right image. Even the background might be lying on the same plane as the house.&lt;/p&gt;

&lt;h2 id=&quot;judging-depth-using-cues&quot;&gt;Judging Depth Using Cues&lt;/h2&gt;

&lt;p&gt;There are basically 4 categories of depth cues: static monocular depth from motion, binocular and physiological cues. We subconsciously take advantage of these signals to perceive depth remarkably well.&lt;/p&gt;

&lt;h3 id=&quot;pictorial-depth-cues&quot;&gt;Pictorial Depth Cues&lt;/h3&gt;

&lt;p&gt;Our ability to perceive depth from a single image depends on the spatial arrangement of things in the scene. Below, I have summarized some of the hints that enable us to reason about the distance of different objects. It may already feel natural to you from your daily interaction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Size of objects&lt;/li&gt;
  &lt;li&gt;Texture&lt;/li&gt;
  &lt;li&gt;Linear Perspective&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An interesting study was conducted at UC, Berkeley and they show experimentally that when the horizon is viewable, there is an overwhelming tendency for us to exploit this property to quickly perceive depth.&lt;/p&gt;

&lt;h3 id=&quot;depth-cues-from-motion-motion-parallax&quot;&gt;Depth Cues from Motion (Motion Parallax)&lt;/h3&gt;

&lt;p&gt;This should not be surprising to you as well. When you, as an observer, is in motion, things around you pass by faster than the one that is farther away. The farther something appears, the slower it seems to pass away from the observer.&lt;/p&gt;

&lt;h3 id=&quot;depth-cues-from-stereo-vision-binocular-parallax&quot;&gt;Depth Cues from Stereo Vision (Binocular Parallax)&lt;/h3&gt;

&lt;p&gt;The difference in view observed by your left and right eye is known as retina disparity. This phenomenon is also known as &lt;em&gt;stereopsis&lt;/em&gt;: ability to perceive depth due to 2 different perspectives of the world. By comparing iamges from the retinas in the two eyes, the brain computes distance. The greater the disparity, the closer the things are around you.&lt;/p&gt;

&lt;h2 id=&quot;depth-estimation-in-computer-vision&quot;&gt;Depth Estimation in Computer Vision&lt;/h2&gt;

&lt;p&gt;The goal of depth estimation is to obtain a representation of the spatial structure of a scene, recovering the three-dimensional shape and appearance of objects in imagery. This is also known as the inverse problem, where we seek to recover some unknowns given insufficient information to fully specify the solution.&lt;/p&gt;

&lt;p&gt;So how do machines actually perceive depth? Can we somehow transfer some of the ideas discussed above? The earliest algorithm with impressive results begin with depth estimation using stereo vision back in the 90s. A lot of progress was made on dense stereo correspondence algorithm. Researchers were able to utilize geometry to constrain and replicate the idea of stereopsis mathemtically and at the same time running at real-time.&lt;/p&gt;

&lt;p&gt;As for monocular depth estimation, it recently started to gain popularity by using neural networks to learn representation that distills depth directly. Through neural networks, depth cues are implicitly learned through gradient-based methods. Besides this, there has been great advancement in self-supervised depth estimation. In this method, a model is trained to predict depth by means of optimizing a proxy signal. No ground truth label is needed in the training process. Most research either exploits geometrical cues such as multi-view geometry or epipolar geometry to learn depth.&lt;/p&gt;

&lt;h3 id=&quot;depth-estimation-from-stereo-vision&quot;&gt;Depth Estimation from Stereo Vision&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The main idea of solving depth using a stereo camera involves the concept of triangulation and stereo matching. The former depends on good calibration and rectification to constrain the problem so that it can be modelled on a 2D plane known as epipolar plane, which greatly reduces the latter (stereo matching) to a line search along the epipolar line.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Analogous to binocular parallax, once we are able to match pixel correspondences between the 2 views, the next task is to obtain a representation that encodes the difference. This representation is known as &lt;em&gt;disparity&lt;/em&gt;. The formula to obtain depth from disparity can be worked out from similar triangles.&lt;/p&gt;

&lt;p&gt;To summarize the steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;identify similar points from feature descriptors&lt;/li&gt;
  &lt;li&gt;Match feature correspondence using a matching cost function&lt;/li&gt;
  &lt;li&gt;Using epipolar geometry, find and match correspondence in one picture frame to the other. A matching cost function is used to measure the pixel dissimilarity.&lt;/li&gt;
  &lt;li&gt;Compute disparity from known correspondences $d = x1 - x2$ as shown in the above image.&lt;/li&gt;
  &lt;li&gt;Compute depth from the known disparity $ z = (f*b)/d$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;age-of-deep-learning&quot;&gt;Age of Deep Learning&lt;/h3&gt;

&lt;p&gt;Deep learning excels in high-level perceptual and cognitive task such as recognition, detection and scene understanding. Depth perception falls into this category and likewise should be a natural way forward.&lt;/p&gt;

&lt;p&gt;The seminal work of estimating depth directly from a monocular image started from Saxena. They learned to regress depth directly from monocular cues in 2D images via supervised learning, by minimizing a regression loss. Since then, many varieties of approaches have been proposed to improve the representation learning by proposing new architectures or loss functions.&lt;/p&gt;

&lt;h4 id=&quot;self-supervised-monocular-depth-estimation-using-stereo&quot;&gt;Self-Supervised Monocular Depth Estimation using Stereo&lt;/h4&gt;

&lt;p&gt;In this framework, the model will predict the disparities $d_l$ and $d_r$ only from the left RGB, $I_l$. Similar to the above method, a spatial transformer network warps the RGB image pair $I_l$, $I_r$ using the disparity. So the paired view can be synthesized and a reconstruction loss between the reconstructed views $I_{pred_l}$ and $I_{pred_r}$ and the target views $I_l$ and $Ir$ is used to supervise the training.&lt;/p&gt;

&lt;h4 id=&quot;self-supervised-depth-estimation-using-sfm-framework&quot;&gt;Self-Supervised Depth Estimation using SfM Framework&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This method frames the problem as learning to generate a novel view from a video sequence. The task of the neural network is to generate the target view $I_t$ from source view by taking image at different time step $I, I_{t-1}, I_{t+1}$ and applying a learnt transformation from a pose network to perform the image warping. Training was made possible by treating the warped view synthesis as supervision in a differentiable manner using a spatial transformer network. At inference time, the depth CNN would predict depth from a single RGB image. Do note that these methods do have some shortcomings such as unable to determine scale and modelling moving objects as described in the next section.&lt;/p&gt;

&lt;h4 id=&quot;cnn-depth-cues-and-bias-learnt&quot;&gt;CNN Depth Cues and Bias Learnt&lt;/h4&gt;

&lt;p&gt;Understanding and deciphering the black box has been an ongoing research in interpretable machine learning. In the context of depth estimation, a few works have started investigating what depth cues do neural network relies on or the inductive biased learnt from a particular dataset. They found out that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Position of objects relative to ground contact point provides strong contextual information&lt;/li&gt;
  &lt;li&gt;Shape does not matter but shadow does&lt;/li&gt;
  &lt;li&gt;In an experiment, by placing an arbitrary object with artificial casted shadow, the model would estimate depth reasonably even if it is not available during training.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-is-measuring-depth-so-difficult&quot;&gt;Why is Measuring Depth So Difficult?&lt;/h2&gt;

&lt;p&gt;Let’s try to understand some of the fundamental problems of depth estimation. The main culprit lies in the projection of 3D views to 2D images where depth information is lost. Another problem is deeply seeded when there are motion and moving objects.&lt;/p&gt;

&lt;h3 id=&quot;depth-estimation-is-ill-posed&quot;&gt;Depth Estimation is Ill-Posed&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On many depth estimation paper, many authors mention that the problem of estimating depth from a single RGB image is ill-posed inverse problem. What this means is that many 3D scenes observed in the world can indeed correspond to the same 2D plane.&lt;/p&gt;

&lt;h3 id=&quot;scale-ambiguity-for-monocular-depth-estimatio&quot;&gt;Scale-Ambiguity for Monocular Depth Estimatio&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall that adjusting the focal length will proportionally scale the points on the image plane. Now, suppose we scale the entire scene, $X$ by some factor $k$ and at the same time, scale the camera matrices $P$ by the factor of $1/k$, the projections of the scene points in the image remains exactly the same.&lt;/p&gt;

&lt;p&gt;$$ x = PX = (1/k)P * kX $$
That is to say, we can never recover the exact scale of the actual scene from the image alone! Note that this issue exists for monocular base techniques, as the scale can be recovered for a stero rig with a known baseline.&lt;/p&gt;

&lt;h3 id=&quot;projection-ambiguity&quot;&gt;Projection Ambiguity&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Suppose we perform a geometric transformation of the scene. It is possible that after the transformation, these points will map to the same location on the plane. There exists not only projective ambiguity, but also affine and similarity ambiguity as well.&lt;/p&gt;

&lt;h3 id=&quot;properties-that-degrade-matching&quot;&gt;Properties that Degrade Matching&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depth/10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For stereo based or multi-view depth estimation that requires triangulation, it usually involves the pipeline of &lt;em&gt;Detect-Describe-Match&lt;/em&gt;. Matching becomes very difficult when the scene is taken from an extremely different viewpoint or varying changes in illumination between images. An extreme case is given in the above picture. These problematic cases include textureless regions (where many pixels will ahve the same pixel intensity), reflective surfaces, repetitive patterns or occlusions. Also. violating the Lambertian property (Lambertian surfaces refer to surfaces that appear to have teh same brightness no matter where it is viewed from). When images that show the same scene from 2 different view, the corresponding brightness intensity may not be equivalent due to non-ideal diffuse reflection.&lt;/p&gt;

&lt;h3 id=&quot;moving-objects-violate-the-static-assumption-for-sfm-methods&quot;&gt;Moving Objects Violate the Static Assumption for SfM Methods&lt;/h3&gt;

&lt;p&gt;Dynamic objects in the scene further complicates the estimation process. Depth estimation via Structure from Motion involves a moving camera and consecutive static scenes. This assumption must hold for matching and aligning pxiels. This assumption breaks when there are moving objects in the scene.&lt;/p&gt;

&lt;p&gt;To this end, many researchers have looked into several methods to model moving objects in the scene by incorporating velocity information using optical flow or by using instance segmentation mask to model the object’s motion from one frame to another.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/depth-estimation-1-basics-and-intuition-86f2c9538cd1&quot;&gt;medium article&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">Introduction</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">3D Rigid Body Motion (Part 2)</title><link href="http://localhost:4000/rigid2/" rel="alternate" type="text/html" title="3D Rigid Body Motion (Part 2)" /><published>2021-09-01T00:00:00+09:00</published><updated>2021-09-01T00:00:00+09:00</updated><id>http://localhost:4000/rigid2</id><content type="html" xml:base="http://localhost:4000/rigid2/">&lt;h1 id=&quot;rotation-vectors-and-euler-angles&quot;&gt;Rotation Vectors and Euler Angles&lt;/h1&gt;
&lt;h2 id=&quot;rotation-vectors&quot;&gt;Rotation Vectors&lt;/h2&gt;
&lt;p&gt;With a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-freedom 3D rigid body motion? Obviously, the matrix representation has at least the following disadvantages:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$SO(3)$ has a rotation matrix of 9 qunatities, but a 3D rotation only has 3 degrees of freedom. Therefore, the matrix expression is redundant.&lt;/li&gt;
  &lt;li&gt;Similarly, the transformation matrix expresses 6 degree-of-freedom transformation with 16 quantities. So, is there a more compact representation?&lt;/li&gt;
  &lt;li&gt;The rotation matrix itself has constraints: it must be an orthogonal matrix with a determinant of 1. The same is true for the transformation matrix. These constraints make the solution more difficult when you want to estimate or optimize a rotation matrix/transform matrix.
Therefore, we hope that there is a way to describe rotation and translation more compactly. For example, is it feasible to express the rotation with a three-dimensional vector and express transformation with a six-dimensional vector?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Obviously, a rotation can be described by a rotation axis and a rotation angle. Thus, we can use a vector whose direction is parallel with the axis of rotation, and the length is equal to the angle of rotation, which is called the &lt;em&gt;rotation vector&lt;/em&gt; (or angle-axis/axis-angle).&lt;/p&gt;

&lt;p&gt;Only a three-dimensional vector is needed here to describe the rotation. Similarly, we may also use a rotation vector and a translation vector and a to express a transformation for a transformation matrix. The variable at this time is exactly six dimensions.&lt;/p&gt;

&lt;p&gt;Consider a rotation represented by $R$. If described by a rotation vector, assuming that the rotation axis is a unit-length vector $n$ and the angle is $\theta$, then the vector $\theta n$ can also describe this rotation.&lt;/p&gt;

&lt;p&gt;So, we have to ask, what is the connection between the two expressions? In fact, it is not difficult to derive their conversion relationship. The conversion from the rotation vector to the rotation matrix is shown by the &lt;em&gt;Rodrigues’ formula&lt;/em&gt;. Since the derivation process is a little complicated, it is not described here. Only the result of the conversion is given.
$$ R = \cos \theta I + (1-\cos \theta) n n^T + \sin \theta n^{\wedge}$$.
The symbol $\wedge$ is a vector to skew-symmetric conversion. Conversely, we can also calculate the conversion from a rotation matrix to a rotation vector. For the corner $\theta$, taking the &lt;em&gt;trace&lt;/em&gt; of both sides, we have:
$$\begin{split} &amp;amp; tr(R) = \cos \theta tr(I) + (1-\cos \theta) tr(nn^T) + \sin \theta tr(n^{\wedge}) \\ &amp;amp; = 3 \cos \theta + (1 - \cos \theta) \\ &amp;amp; = 1+ 2 \cos \theta \end{split}$$.
Therefore, $$\theta = \arccos (\frac{tr(R) - 1}{2})$$.
Regarding the axis $n$, since the rotation axis does not change after the rotation, we have: $$Rn=n$$. So, the axis $n$ is the eigenvector corresponding to the amtrix $R$’s eigenvalue $1$. Solving this equation and normalizing it gives the axis of rotation.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.springer.com/gp/book/9789811649387&quot;&gt;Introduction to Visual SLAM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">Rotation Vectors and Euler Angles Rotation Vectors With a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-freedom 3D rigid body motion? Obviously, the matrix representation has at least the following disadvantages: $SO(3)$ has a rotation matrix of 9 qunatities, but a 3D rotation only has 3 degrees of freedom. Therefore, the matrix expression is redundant. Similarly, the transformation matrix expresses 6 degree-of-freedom transformation with 16 quantities. So, is there a more compact representation? The rotation matrix itself has constraints: it must be an orthogonal matrix with a determinant of 1. The same is true for the transformation matrix. These constraints make the solution more difficult when you want to estimate or optimize a rotation matrix/transform matrix. Therefore, we hope that there is a way to describe rotation and translation more compactly. For example, is it feasible to express the rotation with a three-dimensional vector and express transformation with a six-dimensional vector?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/rigid/1.png" /><media:content medium="image" url="http://localhost:4000/assets/images/rigid/1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">3D CNN</title><link href="http://localhost:4000/3d-cnn/" rel="alternate" type="text/html" title="3D CNN" /><published>2021-08-31T00:00:00+09:00</published><updated>2021-08-31T00:00:00+09:00</updated><id>http://localhost:4000/3d-cnn</id><content type="html" xml:base="http://localhost:4000/3d-cnn/">&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name>seri</name></author><category term="deep learning" /><category term="featured" /><summary type="html">def print_hi(name) puts &quot;Hi, #{name}&quot; end print_hi('Tom') #=&amp;gt; prints 'Hi, Tom' to STDOUT.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cnn.png" /><media:content medium="image" url="http://localhost:4000/assets/images/cnn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Homogeneous Coordinates and Projective Geometry</title><link href="http://localhost:4000/homogeneous/" rel="alternate" type="text/html" title="Homogeneous Coordinates and Projective Geometry" /><published>2021-08-31T00:00:00+09:00</published><updated>2021-08-31T00:00:00+09:00</updated><id>http://localhost:4000/homogeneous</id><content type="html" xml:base="http://localhost:4000/homogeneous/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this article, I’m going to explain homogeneous coordinates (a.k.a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry.&lt;/p&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;
&lt;p&gt;Most of the time when working with 3D, we are thinking in terms of Euclidean geometry-that is, coordinates in three-dimensional space ($X$, $Y$ and $Z$). However, there are certain situations where it is useful to think in terms of &lt;strong&gt;projective geometry&lt;/strong&gt; instead. Projective geometry has an extra dimension, called $W$, in addition to the $X$, $Y$, and $Z$ dimensions. This four-dimensional space is called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;projective space&lt;/code&gt; and coordinates in projective space are called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homogenous coordinates&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For the purposes of 3D software, the terms &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;projective&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;homogeous&lt;/code&gt; are basically interchangeable with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4D&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;not-quaternions&quot;&gt;Not Quaternions&lt;/h2&gt;
&lt;p&gt;Quaternions look a lot like homogeneous coordinates. Both are 4D vectors, commonly depicted as $(X,Y,Z,W)$. However, quaternions and homogeneous coordinates are different concepts, with different uses.&lt;/p&gt;

&lt;h2 id=&quot;an-analogy-in-2d&quot;&gt;An Analogy in 2D&lt;/h2&gt;
&lt;p&gt;First, let’s look at how projective geometry works in 2D, before we move on to 3D.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/homo/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Imagine a projector that is projecting a 2D image onto a screen. It’s easy to identify the $X$ and $Y$ dimensions of the projected image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/homo/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, if you step back from the 2D image and look at the projector and the screen, you can see the $W$ dimension, too. The $W$ dimension is the distance from the projector to the screen.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/homo/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So what does the $W$ dimension do, exactly? Imagine what would happen to the 2D image if you increased or decreased $W$-that is, if you increased or decreased the distance between the projector and the screen. If you move the projector closer to the screen, the whole 2D image becomes smaller. If you move the projector away from the screen, the 2D image becomes larger. As you can see, the value of $W$ affects the size (a.k.a scale) of the image.&lt;/p&gt;

&lt;h2 id=&quot;applying-it-to-3d&quot;&gt;Applying it to 3D&lt;/h2&gt;
&lt;p&gt;There is no such thing as a 3D projector (yet), so its’ harder to imagine projective geometry in 3D, but the $W$ value works exactly the same as it does in 2D. When $W$ increases, the coordinates expands (scales up). When $W$ decreases, the coordinates shrinks (scales down). The $W$ is basically a scaling transformation for the 3D coordinates.&lt;/p&gt;

&lt;h2 id=&quot;when-w--1&quot;&gt;When $W = 1$&lt;/h2&gt;
&lt;p&gt;The usual advice for 3D programming beginners is to always set $W=1$ whenever converting a 3D coordinate to 4D coordinate. The reason for that is that when you scale a coordinate by a 1 it doesn’t shrink or grow, it just stays the same size. So, when $W=1$, it has no effect on the $X$, $Y$, or $Z$ values.&lt;/p&gt;

&lt;p&gt;For this reason, when it comes to 3D computer graphics, coordinates are said to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;correct&lt;/code&gt; only when $W=1$. If you tried to render with $W=0$ your program would crash when it attempted to divide by zero. With $W&amp;lt;0$ everything would flip unside-down and back-to-front.&lt;/p&gt;

&lt;p&gt;Mathematically speaking, there is no such thing as an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incorrect&lt;/code&gt; homogeneous coordinate. Using coordinates with $W=1$ is just a useful convention for the 3D computer grahics.&lt;/p&gt;

&lt;h2 id=&quot;the-math&quot;&gt;The Math&lt;/h2&gt;
&lt;p&gt;Now, let’s look at some actual numbers, to see how the math works.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/homo/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s say that the projector is $3$ meters away from the screen, and there is a dot on the 2D image at the coordinate $(15, 21)$. This gives us the projective coordinate vector $(X,Y,W) = (15,21,3)$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/homo/5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now imagine that the projector was pushed closer to the screen so that the distance was $1$ meter. The closer the project gets to the screen, the smaller the image becomes. The projector has moved three times closer, so the image becomes three times smaller. If we take the original coordinate vector and divide all the values by three, we get the new vector where $W=1$:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$$(\frac{15}{3}, \frac{21}{3}, \frac{3}{3}) = (5,7,1)$$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The dot is now at coordinate $(5,7)$. This is how an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;incorrect&lt;/code&gt; homogeneous coordinate is converted to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;correct&lt;/code&gt; coordinate: divide all the values by $W$. The process is exactly the same for 2D and 3D coordinates.&lt;/p&gt;

&lt;p&gt;Dividing all the values in a vector is done by a scalar multiplication with the reciprocal of the divisor. Here is a 4D example:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$$\frac{1}{5}(10, 20, 30, 5) = (\frac{10}{5}, \frac{20}{5}, \frac{30}{5}, \frac{5}{5}) = (2,4,6,1)$$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;uses-of-homogeneous-coordinates-in-computer-graphics&quot;&gt;Uses of Homogeneous Coordinates in Computer Graphics&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, in regard to 3D computer graphics, homogeneous coordinates are useful in certain situations. We will look at some of those situations here.&lt;/p&gt;

&lt;h3 id=&quot;translation-matrices-for-3d-coordinates&quot;&gt;Translation Matrices for 3D Coordinates&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;A four-column matrix can only be multiplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Rotation and scaling transformation matrices only require three columns. But, in order to do translation, the matrices need to have at least four columns. This is why transformations are often $4 \times 4$ matices. However, a matrix with four columns cannot be multiplied by a 3D vector, due to the rules of matrix multiplication. A four-column matrix can only be mulitplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors.&lt;/p&gt;

&lt;p&gt;The 4th dimension $W$ is usually unchanged, when using homogeneous coordinates in matrix transformation. $W$ is set to $1$ when converting 3D coordinates into 4D, and is usually still $1$ after the transformation matrices are applied, at which point it can be converted back into a 3D coordinate by ignoring $W$. This is true for all translation, rotation, and scaling transformations, which by far are the most common types of transformations. The notable exception is projection matrices, which do affect the $W$ dimension.&lt;/p&gt;

&lt;h3 id=&quot;perspective-transformation&quot;&gt;Perspective Transformation&lt;/h3&gt;
&lt;p&gt;In 3D, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perspective&lt;/code&gt; is the phenomenon where an object appears smaller the further away it is from the camera. A far-away mountain can appear to be smaller than a cat, if the cat is close enough to the camera.&lt;/p&gt;

&lt;p&gt;Perspective is implemented in 3D computer graphics by using a transformation matrix that changes the $W$ element of each vertex. After the camera matrix is applied to each vertex, but before the projection matrix is applied, the $Z$ element of each vertex represents the distance away from the camera. Therefore, the larger $Z$ is, the more the vertex should be scaled down. The $W$ dimension affects the scale, so the projection matrix just changes the $W$ based on the $Z$ value. Here is an example of a perspective projection matrix being applied to a homogeneous coordinate:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$$ \begin{bmatrix} 1&amp;amp;0&amp;amp;0&amp;amp;0 \\ 0&amp;amp;1&amp;amp;0&amp;amp;0&amp;amp; \\ 0&amp;amp;0&amp;amp;1&amp;amp;0 \\ 0&amp;amp;0&amp;amp;1&amp;amp;0 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1\end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1 \end{bmatrix} $$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Notice how the $W$ value is changed to $4$, which comes from the $Z$ value.&lt;/p&gt;

&lt;p&gt;After the perspective projection matrix is applied, each vertex undergoes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perspective division&lt;/code&gt;. Perspective division is just a specific term for converting the homogeneous coordinate back to $W=1$, as explained earlier in the article. Continuing with the example above, the perspective division step would look like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$$\frac{1}{4}(2,3,4,4) = (0.5,0.75, 1,1)$$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;After perspective division, the $W$ value is discarded, and we are left with a 3D coordinate that has been correctly scaled according to a 3D perspective projection.&lt;/p&gt;

&lt;h3 id=&quot;positioning-directional-lights&quot;&gt;Positioning Directional Lights&lt;/h3&gt;

&lt;p&gt;One property of homogeneous coordinates is that they allow you to have points at infinity (infinite length vectors), which is not possible with 3D coordinates. Points at infinity occur when $W=0$. If you try to convert a $W=0$ homogeneous coordinate into a normal $W=1$ coordinate, it results in a bunch of divide-by-zero operations:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$$ \frac{1}{0}(2,3,4,0) = (\frac{2}{0}, \frac{3}{0}, \frac{4}{0}, \frac{0}{0})$$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that homogeneous coordinates with $W=0$ can not be converted back into 3D coordinates.&lt;/p&gt;

&lt;p&gt;What use does this have? Well, directional lights can be thought of as point lights that are infinitely far away. When a point light is infinitely far away, the rays of light become parallel, and all of the light travels in a single direction. This is basically the definition of a directional light.&lt;/p&gt;

&lt;p&gt;So, traditionally, in 3D graphics, directional lights are differentiated from point lights by the value of $W$ in the position vector of the light. If $W=1$, then it is a point light. If $W=0$, then it is a directional light.&lt;/p&gt;

&lt;p&gt;This is more of a traditional convention, rather than a useful way to write lighting code. Directional lights and point lights are usually implemented with separate code, because they behave differently.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Homogeneous coordinates have an extra dimension called $W$, which scales the $X$, $Y$, and $Z$ dimensions. Matrices for translation and perspective projection can only be applied to homogeneous coordinates, which is why they are so common in 3D computer graphics. The $X$, $Y$, and $Z$ values are said to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;correct&lt;/code&gt; when $W=1$. Any homogeneous coordinates can be converted to have $W=1$ by dividing all four dimensions by the $W$ value, except if $W=0$. When $W=0$, the coordinate represents a point at infinity (a vector with infinite length), and this is often used to denote the direction of directional lights.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/&quot;&gt;Tomdalling’s Blog Post&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://staff.fnwi.uva.nl/r.vandenboomgaard/IPCV20172018/LectureNotes/MATH/homogenous.html&quot;&gt;Image Processing and Computer Vision Lecture Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="sticky" /><summary type="html">Introduction In this article, I’m going to explain homogeneous coordinates (a.k.a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/home.png" /><media:content medium="image" url="http://localhost:4000/assets/images/home.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">3D Rigid Body Motion (Part 1)</title><link href="http://localhost:4000/rigid/" rel="alternate" type="text/html" title="3D Rigid Body Motion (Part 1)" /><published>2021-08-31T00:00:00+09:00</published><updated>2021-08-31T00:00:00+09:00</updated><id>http://localhost:4000/rigid</id><content type="html" xml:base="http://localhost:4000/rigid/">&lt;blockquote&gt;
  &lt;p&gt;The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this article, I will introduce one of the fundamental problems of visual SLAM: How to describe a rigid body’s motion in 3-dimensional space? Intuitively, we certainly know that this consists of one rotation plus one translation. The translation part does not really have any problems, but the rotation part is questionable. I will introduce the meaning of rotation matrices, quaternions, Euler angles and how they are computed and transformed.&lt;/p&gt;

&lt;h2 id=&quot;rotation-matrix&quot;&gt;Rotation Matrix&lt;/h2&gt;
&lt;h3 id=&quot;points-vectors-and-coordinate-systems&quot;&gt;Points, Vectors, and Coordinate Systems&lt;/h3&gt;
&lt;p&gt;The space of our daily life is 3-dimensional, so we are born to be used to 3D movements. The 3D space consists of three axes, so the position of one spatial point can be specified by three coordinates. However, we should now consider a rigid body, which has its &lt;em&gt;position&lt;/em&gt; and &lt;em&gt;orientation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The camera can also be viewed as a rigid body in three dimensions, so what we care about in Visual SLAM are the problem of the camera’s position and orientation. Combined, we can say, “the camera is at the $(0,0,0)$ point, facing the front”. Let’s describe this in a mathematical term.&lt;/p&gt;

&lt;p&gt;We start from the basic content: &lt;em&gt;points&lt;/em&gt; and &lt;em&gt;vectors&lt;/em&gt;. Points are the basic element in space, no length, no volume. Connecting the two points forms a vector. A vector can be thought of as an arrow pointing from one point to another. Here we need to warn you not to confuse the vector with its coordinates.&lt;/p&gt;

&lt;p&gt;A vector is one thing in space, such as $a$. Here, $a$ does not need to be associated with several real numbers. We can naturally talk about the plus or minus operation of two vectors, without relating to any real numbers.&lt;/p&gt;

&lt;p&gt;Only when we specify a coordinate system in this 3D space can we talk about the vector’s coordinates in this system, finding several real numbers corresponding to this vector.&lt;/p&gt;

&lt;p&gt;With the knowledge of linear algebra, the coordinates of a point in 3D space can be described as $\mathbb{R}^3$. How to do we describe this? Suppose that in this linear space, we fined a set of base $(e_1, e_2, e_3)$, then, an arbitrary vector $a$ has a &lt;em&gt;coordinate&lt;/em&gt; under this base:&lt;/p&gt;

&lt;p&gt;$$ a = \begin{bmatrix} e_1 &amp;amp; e_2 &amp;amp; e_3 \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = a_1 e_1 + a_2 e_2 + a_3 e_3 $$.
Here, $(a_1, a_2, a_3)^T$ is called $a$’s coordinates. The coordinates’ specific values are related to the vector itself and the selection of the bases. In $\mathbb{R}^3$, the coordinate system usually consists of $3$ orthogonal coordinate axes (it can also be non-orthogonal, but it is rare in practice).&lt;/p&gt;

&lt;p&gt;For example, given $x$ and $y$ axis, the $z$ axis can be determined using the right-hand (or left-hand) rule. According to different definitions, the coordinate system is divided into left-handed and right-handed. The third axis of the left-hand rule is opposite to the right-hand rule. Most 3D libraries use right-handed coordinates.&lt;/p&gt;

&lt;p&gt;Based on basic linear algebra knowledge, we can talk about the operations between vectors/vectors, vectors/numbers, such as scalar multiplication, vector addition, subtraction, inner product, outer product and so on.&lt;/p&gt;

&lt;p&gt;For $a,b \in \mathbb{R}^3$, the inner product of $a,b$ can be written as:
$$ a \cdot b = a^Tb = \sum_{i=1}^3 a_i b_i = |a||b| \cos(&amp;lt;a,b&amp;gt;)$$, 
where $ &amp;lt;a.b&amp;gt; $ refers to the angle between the vector $a, b$. The inner product can also describe the projection relationship between vectors.&lt;/p&gt;

&lt;p&gt;$$ a \times b = \begin{Vmatrix} e_1 &amp;amp; e_2 &amp;amp; e_3 \\ a_1 &amp;amp; a_2 &amp;amp; a_3 \\ b_1 &amp;amp; b_2 &amp;amp; b_3 \end{Vmatrix} = \begin{bmatrix} a_2b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{bmatrix} = \begin{bmatrix} 0 &amp;amp; -a_3 &amp;amp; a_2 \\ a_3 &amp;amp; 0 &amp;amp; -a_1 \\ -a_2 &amp;amp; a_1 &amp;amp; 0 \end{bmatrix} b =  a \wedge b$$.
The result of the outer product is a vector whose direction is perpendicular to the two vectors, and the length is 
$|a||b|\sin(&amp;lt;a,b&amp;gt;)$
, which is also the area of the quadrilateral of the two vectors.&lt;/p&gt;

&lt;p&gt;From the outer product operation, we introduce the $\wedge$ operator here, which means writing $a$ as a &lt;em&gt;skew-symmetric matrix&lt;/em&gt;. You can take $\wedge$ as a skew-symmetric symbol. It turns the outer product $a \times b$ into the multiplication of the matrix and the vector $a \wedge b$ is a linear operation.&lt;/p&gt;

&lt;p&gt;This symbol will be used frequently in the following sections. It is a one-to-one mapping, meaning that for any vector, it corresponds to a unique anti-symmetric matrix, and vice versa:&lt;/p&gt;

&lt;p&gt;$$ a \wedge = \begin{bmatrix} 0 &amp;amp; -a_3 &amp;amp; a_2 \\ a_3 &amp;amp; 0 &amp;amp; -a_1 \\ -a_2 &amp;amp; a_1 &amp;amp; 0 \end{bmatrix}$$.
At the same time, note that the vector operations such as addition, subtraction, inner and outer products can be calculated even when we do not have their coordinates. For example, although the inner product can be expressed by the sum of the two vectors’ product when we know the coordinates, the length and angle can also be calculated even if their coordinates are unknown. Therefore, the inner product result of the two vectors is independent of the selection of the coordinate system.&lt;/p&gt;

&lt;h3 id=&quot;euclidean-transforms-between-coordinate-systems&quot;&gt;Euclidean Transforms between Coordinate Systems&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rigid/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We often define a variety of coordinate systems in the real scene. In robotics, you define one coordinate system for each link and joint; in 3D mapping, we also define a coordinate system for each cuboid and cylinder.&lt;/p&gt;

&lt;p&gt;If we consider a moving robot, it is common practice to set a stationary inertial coordinate system (or world coordinate system), such as the $x_W, y_W, z_W$ defined in the picture above.&lt;/p&gt;

&lt;p&gt;Meanwhile, the camera or robot is a moving coordinate system, such as coordinate system defined by $x_C, y_C, z_C$. We might ask: a vector $p$ in the camera system may have coordinates $p_c$; and in the world coordinate system, its coordinates maybe $p_w$. Then what is the conversion between two coordinates?&lt;/p&gt;

&lt;p&gt;It is necessary to first obtain the coordinate values of the point in the camera system and then use the transform rule to do the coordinate transform. We need a mathematical way to describe this transformation. As we will see later, we can describe it with a transform matrix $T$.&lt;/p&gt;

&lt;p&gt;Intuitively, the motion between two coordinate systems consists of a rotation plus a translation, which is called &lt;em&gt;rigid body motion&lt;/em&gt;. Obviously, the camera movement is rigid. During the rigid body motion, the length and angle of the vector will not change.&lt;/p&gt;

&lt;p&gt;Imagine that you throw your phone into the air and there may be differences in spatial position and orientation. But the length and the angle of each face will not change. At this point, we say that the phone’s motion is &lt;em&gt;Euclidean&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The Euclidean transform consists of rotation and translation. Let’s first consider the rotation. We have a unit-length orthogonal base $(e_1, e_2, e_3)$. After a rotation it becomes $(e_1’, e_2’, e_3’)$. Then, for the same vector $a$ (the vector does not move with the rotation of the coordinate system). its coordinates in these two coordinate systems are $[a_1, a_2, a_3]^T$ and $[a_1’, a_2’, a_3]^T$. Because the vector itself has not changed, according to the definition of coordinates, there are:&lt;/p&gt;

&lt;p&gt;$$ [e_1, e_2, e_3] \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = [e_1’, e_2’, e_3’] \begin{bmatrix} a_1’ \\ a_2’ \\ a_3’ \end{bmatrix}$$.
To describe the relationship between the two coordinates, we multiply the left and right side of the above equation by $\begin{bmatrix} e_1^T \\ e_2^T \\ e_3^T \end{bmatrix}$, then the matrix on the left becomes an identity matrix, so:&lt;/p&gt;

&lt;p&gt;$$\begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} \triangleq Ra’$$.
We take the intermediate matrix out and define it as a matrix $R$. This matrix consists of the inner product between the two sets of bases, describing the same vector’s coordinate transformation relationship before and after the rotation.&lt;/p&gt;

&lt;p&gt;It can be said that the matrix $R$ describes the rotation itself. So we call it the &lt;em&gt;rotation matrix&lt;/em&gt;. Meanwhile, the components of the matrix are the inner product of the two coordinate system bases.&lt;/p&gt;

&lt;p&gt;Since the base vector’s length is $1$, it is actually the cosine of the angle between the base vectors. So this matrix is also called &lt;em&gt;direction cosine matrix&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The rotation matrix has some special properties. In fact, it is an &lt;em&gt;orthogonal&lt;/em&gt; matrix with a determinant of $1$. Conversely, an orthogonal matrix with a determinant of 1 is also a rotation matrix. So you can define a set of $n$ dimensional rotation matrices as follows:&lt;/p&gt;

&lt;p&gt;$$SO(n) = \{ R \in \mathbb{R}^{n \times n}| RR^T = I, det(R) =1 \} $$.
$SO(n)$ refers to the special orthogonal group. This set consists of a rotation matrix of $n$ dimensional space, in particular, $SO(3)$ refers to the rotation of the three-dimensional space. In this way, we can talk directly about the rotation transformation between the two coordinate systems without having to start from the bases.&lt;/p&gt;

&lt;p&gt;Since the rotation matrix is orthogonal, its inverse (i.e., transpose) describes an opposite rotation. According to the above definition, there are:
$$a’ = R^{-1}a = R^Ta$$.
Obviously, the $R^T$ represents an opposite rotation.&lt;/p&gt;

&lt;p&gt;In the Euclidean transformation, there is a translation in addition to rotation. Consider the vector $a$ in the world coordinate system. After a rotation (depicted by $R$) and a translation of $t$, we get $a’$. Then we can put the rotation and translation together, and have:
$$a’= Ra+t$$,
where $t$ is called a translation vector.&lt;/p&gt;

&lt;p&gt;Compared to the rotation, the translation part simply adds the translation vector to the coordinates after the rotation, which si very simple. By the above formula, we completely describe the coordinate transformation relationship using a rotation matrix $R$ and a translation vector $t$.&lt;/p&gt;

&lt;p&gt;In practice, we may define the coordinate system 1 and 2, then the vector $a$ under the two coordinates is $a_1, a_2$. The relationship between the two systems should be: 
$$a_1 = R_{12} a_2 + t_{12}$$.
Here, $R_{12}$ means the “rotation of the vector from system 2 to system 1”. About $t_{12}$, readers may just take it as a translation vector without wondering about its physical meaning. In fact, it corresponds to a vector from the system 1’s origin pointing to system 2’s origin, and the coordinates are taken under tsystem 1. So I suggest you to understand it as “a vector from 1 to 2”.&lt;/p&gt;

&lt;p&gt;But the reverse $t_{21}$, which is a vector from $2$’s origin to $1$’s origin, whose &lt;em&gt;coordinates are taken in system $2$&lt;/em&gt;, is not equal to $-t_{12}$. It is also related to the rotation of the two systems.&lt;/p&gt;

&lt;p&gt;Therefore, when beginners ask the question “What are my coordinates?”, we need to clearly explain this sentence’s meaning. Here, “my coordinates” normally refers to the vector from the world system $W$ pointing to the origin of the camera system $C$, and then take the coordinates in the world’s base. Corresponding to the mathematical symbol, it should be the value of $t_{WC}$. For the same reason, it is not $-t_{CW}$ but actually $-R^T_{CW} t_{CW}$.&lt;/p&gt;

&lt;h3 id=&quot;transform-matrix-and-homogeneous-coordinates&quot;&gt;Transform Matrix and Homogeneous Coordinates&lt;/h3&gt;
&lt;p&gt;The formula $a’ = Ra+t$ fully expresses the rotation and the translation of Euclidean space, but there is still a small problem: the transformation relationship here is not a linear relationship.&lt;/p&gt;

&lt;p&gt;Suppose we made two transformations: $R_1,t_1$ and $R_2,t_2$:
$$b = R_1 a + t_1, c = R_2 b + t_2$$. 
So the transformation from $a$ to $c$ is: $$c = R_2 (R_1 a + t_1) + t_2$$.
This form is not elegant after multiple transformations. Therefore, we introduce homogeneous coordinates and transformation matrices, rewriting the formula:
$$ \begin{bmatrix} a’ \\ 1 \end{bmatrix} = \begin{bmatrix} R &amp;amp; t \\ 0^T &amp;amp; 1 \end{bmatrix} \begin{bmatrix} a \\ 1 \end{bmatrix} \triangleq T \begin{bmatrix} a \\ 1 \end{bmatrix}$$. 
This is a mathematical trick: we add $1$ at the end of the 3D vector and turn it into a 4D vector called &lt;em&gt;homogeneous coordinates&lt;/em&gt;. For this four-dimensional vector, we can write the rotation and translation matrix, making the whole relationship a linear relationship. In this formula, the matrix $T$ is called &lt;em&gt;transform matrix&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We temporarily use $\tilde{a}$ to represent the homogeneous coordinates of $a$. Then, relying on homogeneous coordinates and transformation matrices, the superposition of the two transformations can have a good form: 
$$\tilde{b} = T_1 \tilde{a}, \tilde{c} = T_2 \tilde{b} \Rightarrow T_2 T_1 \tilde{a}$$.
But the symbols that distinguish between homogeneous and non-homogeneous coordinates are annoying, because here we only need to add 1 at the end of the vector or remove 1 to turn it into a normal vector. So, without ambiguity, we will write it directly as $b = Ta$ and by default we just assume a homogeneous coordinate conversion is made if needed.&lt;/p&gt;

&lt;p&gt;The transformation matrix $T$ has a special structure: the upper left corner is the rotation matrix, the right side is the translation vector, the lower-left corner is $0$ vector, and the lower right corner is $1$. This set of transform matrix is also known as the &lt;em&gt;special Euclidean group&lt;/em&gt;: 
$$SE(3) = \{ T = \begin{bmatrix} R &amp;amp; t \\ 0^T &amp;amp; 1 \end{bmatrix} \in \mathbb{R}^{4 \times 4}| R \in SO(3), t \in \mathbb{R}^3 \}$$.
Like $SO(3)$, the inverse of the transformation matrix represents an inverse transformation: 
$$T^{-1} = \begin{bmatrix} R^T &amp;amp;  -R^T t \\ 0^T &amp;amp; 1 \end{bmatrix}$$.
Again, we use the notation of $T_{12}$ to represent a transformation from 2 to 1. Because the conversion between homogeneous and non-homogeneous coordinates is actually very easy, it is assumed that the conversion from homogeneous coordinates to normal coordinates is already done.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;First, we introduced the vector and its coordinate representation and introduced the operation between the vectors; then, the motion between the coordinate systems is described by the Euclidean transformation, which consists of translation and rotation. The rotation can be described by the rotation matrix $SO(3)$, while the translation is directly described by an $\mathbb{R}^3$ vector. Finally, if the translation and rotation are placed in a matrix, the transformation matrix $SE(3)$ is formed.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.springer.com/gp/book/9789811649387&quot;&gt;Introduction to Visual SLAM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/rigid.png" /><media:content medium="image" url="http://localhost:4000/assets/images/rigid.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>