<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-09-14T15:54:35+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seri Lee Blog</title><subtitle>This is where I write posts about my research field.</subtitle><entry><title type="html">All About Training GAN</title><link href="http://localhost:4000/gan.html" rel="alternate" type="text/html" title="All About Training GAN" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/gan</id><content type="html" xml:base="http://localhost:4000/gan.html">&lt;h2&gt; Generative Adversarial Networks &lt;/h2&gt;

&lt;p&gt;Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. The discriminator should not be able to distinguish between real and fake images.&lt;/p&gt;

&lt;p&gt;Another way to look at the GAN setup is that the discriminator is trying to guide the generator by telling what real images look like. The two networks try to achieve what is called the Nash Equilibrium with respect to each other.&lt;/p&gt;

&lt;h2&gt; Training GANs &lt;/h2&gt;

&lt;p&gt;GAN networks are a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. There are no good objective metrics for evaluating whether a GAN is performing well during training, e.g. reviewing the loss is not sufficient. Instead the best approach is to visually inspect the generated examples and use subjective evaluation.&lt;/p&gt;

&lt;p&gt;Other quantitative measures, such as Inception Score (IS) or Frechet Inception Distance (FID) rely on pretrained models with a specific set of object classes. They lack an upper bound (which means hypothetically the highest possible score is infinity).&lt;/p&gt;

&lt;h3&gt; Look at the Loss &lt;/h3&gt;

&lt;p&gt;In a discriminative model,the loss measures the accuracy of the prediction and we use it to monitor the progress of training. However, the loss in GAN measures how well we are doing compared with our opponent. Often, the generator cost increases but the image quality is actually improving.&lt;/p&gt;

&lt;p&gt;If you see the discriminator loss rapidly approaching, there is probably no chance of recovery and it is time to change something.&lt;/p&gt;

&lt;h3&gt; Look at the Gradients &lt;/h3&gt;
&lt;p&gt;Monitor the gradients along with the losses in the networks. These can give you a good idea about the progress of training and can even help in debugging if things are not really working well.&lt;/p&gt;

&lt;p&gt;Ideally, the generator should receive large gradients early in the training because it needs to learn how to generate real-looking data. The discriminator on the other hand does not always get large gradients early on, because it can easily distinguish real and fake images.&lt;/p&gt;

&lt;p&gt;If the gradients at the layer of generator are too small, learning might be slow or not happening at all. The generator should get large gradients early on and the discriminator getting consistently high gradients at the top layer once the generator has been trained enough.&lt;/p&gt;

&lt;h2&gt; Detecting GAN Failure Modes &lt;/h2&gt;
&lt;p&gt;The reason why GANs are difficult to train is that both generator and the discriminator are trained simultaneously in a zero-sum game. This means that improvements to one model come at the expense of the other model. 
The goal of training two models involves finding a point of equilibrium between the two competing concerns. It also means that everytime the parameters of one model are updated the nature of the optimization problem that is being solved is updated as well. The technical challenge of training two competing neural networks at the same time is that they can fail to converge.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Convergence Failure&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;indent&quot;&gt; The fact that GANs are composed by two networks, and each of them has its loss function leads to GANs unstability. In GAN architecture, the discriminator tries to minimize a cross-entropy while the generator tries to maximize it. When discriminator confidence is high and the discriminator starts to reject the samples that are produced by the generator, generator's gradient vanishes. 

This scenario happens when the generator score reaches near zero and the discriminator score reaches near one. The discriminator is overpowering the generator. If the score does not recover from these values for many iterations, it is better to stop training. &lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Mode Collapse&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;indent&quot;&gt; Mode collapse is when the GAN produces a small variety of images with many duplicates. This happens when the generator is unable to learn a rich feature representation because  it learns to associate similar outputs to multiple different inputs. The most promising way to check for mode collapse is to inspect the generated images. If there is little diversity in the output and some of them are almost identical, there is likely mode collapse. If you observe this happening, you should try to increase the ability of the generator to create more diverse outputs or impair the discriminator by randomly giving false labels to real images.

Another type of behavior you should look out for is when the generator oscillates between generating specific examples in the domain. They progress from generating one kind of sample to generating another kind of sample without eventually reaching equilibrium.
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Diminisheed Gradient&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;indent&quot;&gt; This situation happens when the discriminator gets too successful that the generator gradient vanishes and learns nothing.

&lt;!--more--&gt;
&lt;h2&gt; Lessons I Learned &lt;/h2&gt;
&lt;div class=&quot;theorem&quot;&gt; Use a batch size smaller than or equal to 64.&lt;/div&gt;
&lt;div class=&quot;indent&quot;&gt; In my experience, using bigger batch sizes often hurt the performance. I suspect it fuels the problem of discriminator getting too good at discriminating the real and fake images, since large batch size means providing a lot of examples to train on. &lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt; Add noise to both real and synthetic data. &lt;/div&gt;
&lt;div class=&quot;indent&quot;&gt; It is well known that making the training of discriminator more difficult is beneficial for the overall stability. Adding noise increases the complexity of the discriminator training and stabilizes the data distribution of the two competing networks. &lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt; Use Label Smoothing &lt;/div&gt;
&lt;div class=&quot;indent&quot;&gt; If the label for real images is set to 1, change it to a lower value like 0.9. This solution discourages the discriminator from being overconfident. &lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt; Different learning rates for the generator and discriminator a.k.a. Two Time-Scale Update Rule &lt;/div&gt;
&lt;div class=&quot;indent&quot;&gt; In my experience, choosing a higher learning rate for the discriminator(i.e. 0.0004) and a lower one(i.e. 0.0001) for the generator works well in practice. I guess the reason is that the generator has to make small steps to fool the discriminator so it does not choose fast but not precise solutions to win the adversarial game. &lt;/div&gt;

&lt;div class=&quot;theorem&quot;&gt; Use some kind of normalization method &lt;/div&gt;
&lt;div class=&quot;indent&quot;&gt; For me, applying Spectral Normalization, a particular kind of normalization applied on the convolutional kernels, greatly helped the stability of training.&lt;/div&gt;

&lt;blockquote&gt; I learned that hyperparameter tuning takes a lot of time and patience especially for training GANs. &lt;/blockquote&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/disparity.png&quot; /&gt;&lt;/picture&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&amp;lt;a=href=&quot;&quot;&amp;gt; TheAILearner &amp;lt;/a&amp;gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">Generative Adversarial Networks Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. The discriminator should not be able to distinguish between real and fake images. Another way to look at the GAN setup is that the discriminator is trying to guide the generator by telling what real images look like. The two networks try to achieve what is called the Nash Equilibrium with respect to each other. Training GANs GAN networks are a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. There are no good objective metrics for evaluating whether a GAN is performing well during training, e.g. reviewing the loss is not sufficient. Instead the best approach is to visually inspect the generated examples and use subjective evaluation. Other quantitative measures, such as Inception Score (IS) or Frechet Inception Distance (FID) rely on pretrained models with a specific set of object classes. They lack an upper bound (which means hypothetically the highest possible score is infinity). Look at the Loss In a discriminative model,the loss measures the accuracy of the prediction and we use it to monitor the progress of training. However, the loss in GAN measures how well we are doing compared with our opponent. Often, the generator cost increases but the image quality is actually improving. If you see the discriminator loss rapidly approaching, there is probably no chance of recovery and it is time to change something. Look at the Gradients Monitor the gradients along with the losses in the networks. These can give you a good idea about the progress of training and can even help in debugging if things are not really working well. Ideally, the generator should receive large gradients early in the training because it needs to learn how to generate real-looking data. The discriminator on the other hand does not always get large gradients early on, because it can easily distinguish real and fake images. If the gradients at the layer of generator are too small, learning might be slow or not happening at all. The generator should get large gradients early on and the discriminator getting consistently high gradients at the top layer once the generator has been trained enough. Detecting GAN Failure Modes The reason why GANs are difficult to train is that both generator and the discriminator are trained simultaneously in a zero-sum game. This means that improvements to one model come at the expense of the other model. The goal of training two models involves finding a point of equilibrium between the two competing concerns. It also means that everytime the parameters of one model are updated the nature of the optimization problem that is being solved is updated as well. The technical challenge of training two competing neural networks at the same time is that they can fail to converge. Convergence Failure The fact that GANs are composed by two networks, and each of them has its loss function leads to GANs unstability. In GAN architecture, the discriminator tries to minimize a cross-entropy while the generator tries to maximize it. When discriminator confidence is high and the discriminator starts to reject the samples that are produced by the generator, generator's gradient vanishes. This scenario happens when the generator score reaches near zero and the discriminator score reaches near one. The discriminator is overpowering the generator. If the score does not recover from these values for many iterations, it is better to stop training. Mode Collapse Mode collapse is when the GAN produces a small variety of images with many duplicates. This happens when the generator is unable to learn a rich feature representation because it learns to associate similar outputs to multiple different inputs. The most promising way to check for mode collapse is to inspect the generated images. If there is little diversity in the output and some of them are almost identical, there is likely mode collapse. If you observe this happening, you should try to increase the ability of the generator to create more diverse outputs or impair the discriminator by randomly giving false labels to real images. Another type of behavior you should look out for is when the generator oscillates between generating specific examples in the domain. They progress from generating one kind of sample to generating another kind of sample without eventually reaching equilibrium. Diminisheed Gradient This situation happens when the discriminator gets too successful that the generator gradient vanishes and learns nothing.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/gan.png" /><media:content medium="image" url="http://localhost:4000/assets/images/gan.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Image Processing: the Basics</title><link href="http://localhost:4000/template.html" rel="alternate" type="text/html" title="Image Processing: the Basics" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/template</id><content type="html" xml:base="http://localhost:4000/template.html">&lt;picture&gt;&lt;img src=&quot;/assets/images/disparity.png&quot; /&gt;&lt;/picture&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&amp;lt;a=href=&quot;&quot;&amp;gt; TheAILearner &amp;lt;/a&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">References &amp;lt;a=href=&quot;&quot;&amp;gt; TheAILearner &amp;lt;/a&amp;gt;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dissecting the Camera Matrix (Part 2)</title><link href="http://localhost:4000/extrinsic.html" rel="alternate" type="text/html" title="Dissecting the Camera Matrix (Part 2)" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/extrinsic</id><content type="html" xml:base="http://localhost:4000/extrinsic.html">&lt;!--more--&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/cali.png&quot; /&gt;&lt;/picture&gt;
&lt;div class=&quot;caption&quot;&gt; Overview of the Camera Calibration Parameters &lt;/div&gt;

&lt;h2&gt; The Extrinsic Camera Matrix &lt;/h2&gt;

&lt;p&gt;The extrinsic matrix takes the form of a rigid transformation matrix: a $3 \times 3$ rotation matrix in the left-block, and $3 \times 1$ translation column-vector in the right.&lt;/p&gt;

\[\begin{bmatrix}
\begin{array}{ccc|c}
  r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; t_1 \\
  r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; t_2 \\
  r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; t_3
\end{array}
\end{bmatrix}\]

&lt;p&gt;It is common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation:&lt;/p&gt;

\[\begin{bmatrix}
\begin{array}{ccc|c}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; t_1 \\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; t_2 \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; t_3 \\
\hline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{array}
\end{bmatrix}

\times

\begin{bmatrix}
\begin{array}{ccc|c}
r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; 0 \\
r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; 0 \\
r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; 0 \\
\hline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{array}
\end{bmatrix}\]

&lt;p&gt;The matrix describes how to transform points in world coordinates to camera coordinates. The important thing to remember about the extrinsic matrix is that it describes how the &lt;span class=&quot;blue&quot;&gt; world &lt;/span&gt; is transformed &lt;span class=&quot;blue&quot;&gt; relative to the camera &lt;/span&gt;. This if often counter-intuitive, because we usually want to specify how the &lt;span class=&quot;red&quot;&gt; camera &lt;/span&gt; is transformed &lt;span class=&quot;red&quot;&gt; relative to the world &lt;/span&gt;.&lt;/p&gt;

&lt;h2&gt; Building the Extrinsic Matrix from Camera Pose &lt;/h2&gt;

&lt;p&gt;Like I said before, it is often more natural to &lt;span class=&quot;highlight-yellow&quot;&gt; specify the camera’s pose directly &lt;/span&gt; rather than specifying &lt;span class=&quot;highlight-pink&quot;&gt; how world points should transform to camera coordinates &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Luckily, building an extrinsic camera matrix this way is easy: just &lt;span class=&quot;highlight-green&quot;&gt; build a rigid transformation matrix that describes the camera’s pose &lt;/span&gt; and then &lt;span class=&quot;rainbow&quot;&gt; take its inverse &lt;/span&gt;.&lt;/p&gt;

\[\begin{bmatrix}
\begin{array}{c|c}
R &amp;amp; t \\
0 &amp;amp; 1 
\end{array}
\end{bmatrix}

= 

\begin{bmatrix}
\begin{array}{c|c}
R_c &amp;amp; C \\
0 &amp;amp; 1 
\end{array}
\end{bmatrix}^{-1}\]

&lt;p&gt;Let $C$ be a column vector describing the location of the camera-center in world coordinates, and let $R_c$ be the rotation matrix describing the camera’s orientation with respect to the world coordinate axes. Then extrinsic matrix is obtained by inverting the camera’s pose matrix.&lt;/p&gt;

&lt;blockquote&gt; Algebraically a rotation matrix in $n$-dimensions is a $n \times n$ special orthogonal matrix, i.e. an orthogonal matrix whose determinant is 1. &lt;/blockquote&gt;

&lt;div class=&quot;sidenote&quot;&gt; We can define matrix $R$ that rotates in the $xy$-Cartesian plane counterclock-wise through an angle $\theta$ about the origin of the Cartesian system as follows:

$$
R = \begin{bmatrix}
\cos\theta &amp;amp; -\sin\theta \\
\sin\theta &amp;amp; \cos\theta
\end{bmatrix}
$$

&lt;/div&gt;

&lt;div class=&quot;sidenote&quot;&gt; The set of all rotation matrices form a group, known as the special orthogonal group. The inverse of a rotation matrix is its transpose, which is also a rotation matrix. 

$$
\displaylines{
R^T = R^{-1} \\
det(R) = 1
}
$$
&lt;/div&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/rotation.png&quot; /&gt;&lt;/picture&gt;
&lt;div class=&quot;caption&quot;&gt; the extrinsic matrix is obtained by inverting the camera's pose matrix &lt;/div&gt;

&lt;p&gt;We here use the fact that the inverse of a rotation matrix is its transpose, and inverting a translation matrix simply negates the translation vector. Relationship between the extrinsic matrix parameters and the camera’s pose is straightforward:&lt;/p&gt;

\[\displaylines{
R = R^T_c \\
t = -RC
}\]

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://ksimek.github.io/2012/08/14/decompose/&quot;&gt; ksimek blog &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prateekvjoshi.com/2014/05/31/understanding-camera-calibration/&quot;&gt; prateekvjoshi blog &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">This is the second part of our journey to master the camera matrix. In this blog post, we will study the extrinsic camera parameters. Extrinsic matrix describes the camera's location in the world.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/extrinsic.png" /><media:content medium="image" url="http://localhost:4000/assets/images/extrinsic.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Advanced PyTorch: Things You Didn’t Know</title><link href="http://localhost:4000/pytorch.html" rel="alternate" type="text/html" title="Advanced PyTorch: Things You Didn’t Know" /><published>2021-09-12T00:00:00+09:00</published><updated>2021-09-12T00:00:00+09:00</updated><id>http://localhost:4000/pytorch</id><content type="html" xml:base="http://localhost:4000/pytorch.html">&lt;!--more--&gt;

&lt;h2&gt; Flatten Operation for a Batch of Image Inputs to a CNN 
&lt;/h2&gt;
&lt;p&gt;Flattening specific tensor axis is often required with CNNs because we work with batches of inputs opposed to single inputs. A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flattened out so that the fully connected layer can accept them as the input. A flatten operation is a specific type of reshaping operation where by all of the axes are smooshed or squashed together.&lt;/p&gt;

&lt;p&gt;To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something that is not already flat. For example, in the MNIST dataset, we will look at an handwritten image of eight. This image has 2 distinct dimensions, height and width.&lt;/p&gt;

&lt;p&gt;The height and width are $18 \times 18$ respectively. These dimensions tell use that this is a cropped image becaue the MNIST dataset contains $28 \times 28$ images. Let’s see how these two axes of height and width are flattened out into a single axis of length 324 (c.f. 324 what we get when multiplying 18 with 18).&lt;/p&gt;

&lt;h3&gt; Flattening Specific Axes of a Tensor &lt;/h3&gt;

&lt;p&gt;Tensor inputs to a convolutional neural network typically have 4 axes, one for batch size, one for color channels, and one each for height and width.&lt;/p&gt;

\[[B,C,H,W]\]

&lt;p&gt;Suppose we have the following three tensors:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Each of these has a shape of $4 \times 4$, so we have three rank-2 tensors. For our purpose, we’ll consider these to be three $4 \times 4$ images that we will use to create a batch that can be passed to a CNN. Batches are represented using a single tensor, so we’ll need to combine these three tensors into a single larger tensor that has 3 axes instead of 2.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here, we used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stack()&lt;/code&gt; method to concatenate our sequence of tensors along a new axis. Since we have three tensors along a new axis, we know that the length of this axis should be 3. At this point, we have a rank-3 tensor that contains a batch of three $4 \times 4$ images. All we need to do now to get this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit single color channel for each of these image tensors, so in practice, these would be grayscale images.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice how the additional axis of length 1 doesn’t change the number of elements in the tensor. This is because the product of the components values doesn’t change when we multiply by one.&lt;/p&gt;

&lt;p&gt;The first axis has 3 elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain 4 arrays that contain 4 numbers or scalar components.&lt;/p&gt;

&lt;h3&gt; Flattening the Tensor Batch &lt;/h3&gt;

&lt;p&gt;Let’s see how to flatten images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, &lt;span class=&quot;underline&quot;&gt; we don’t want to flatten the whole thing &lt;/span&gt; We only want to &lt;span class=&quot;glow&quot;&gt; flatten the image tensors &lt;/span&gt; within the batch tensor.&lt;/p&gt;

&lt;p&gt;For example, if we do the following operations on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# this is the same operation as t.flatten()
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the batches together into a single axis. The flattened batch won’t work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess.&lt;/p&gt;

&lt;p&gt;The solution here, is to flatten each image while &lt;span color=&quot;blink&quot;&gt; still maintaining the batch axis &lt;/span&gt;. This means we want to &lt;span class=&quot;underline&quot;&gt; flatten only part of the tensor &lt;/span&gt;. We want to flatten the color channel axis with the height and width axes.&lt;/p&gt;

&lt;blockquote&gt; The Axes that Need to be Flattened: $[C,H,W]$ &lt;/blockquote&gt;
&lt;p&gt;This can be done with PyTorch’s built in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flatten()&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice how we specified the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start_dim&lt;/code&gt; parameter.This tells the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flatten()&lt;/code&gt; method which axis it should start the flatten operation. Now we have a rank-2 tensor with three single color channel images that have been flattened out into 16 pixels.&lt;/p&gt;

&lt;h3&gt; Flattening an RGB Image &lt;/h3&gt;
&lt;p&gt;If we flatten an RGB image, what happens to the color? Each color channel will be flattened first, then the flattened channels will be lined up side by side on a single axis of the tensor.&lt;/p&gt;

&lt;p&gt;For example, we build an RGB image tensor like the following code:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;By flattening the image tensor, this is how it is going to look like.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2&gt; Broadcasting and Element-Wise Operations with PyTorch &lt;/h2&gt;

&lt;blockquote&gt; Remember, all these rules apply to PyTorch Tensors! Python built-in types such as list will not behave this way.&lt;/blockquote&gt;
&lt;h3&gt; Element-Wise Operations &lt;/h3&gt;
&lt;p&gt;An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element. Therefore, we can deduce that tensors must have the same shape in order to perform an element-wise operation.&lt;/p&gt;

&lt;h3&gt; Broadcasting Tensors &lt;/h3&gt;
&lt;p&gt;Broadcasting describes how tensors with different shapes are treated during element-wise operations. For example, suppose we have the following two tensors:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What will be the result of this two tensors’ element-wise addition operation? Even though these two tensors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t2&lt;/code&gt; will be transformed via broadcasting to match the shape of the higher rank tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t1&lt;/code&gt;, and the element-wise operation will be performed as usual.&lt;/p&gt;

&lt;p&gt;The concept of broadcasting is the key to understanding how this operation will be carried out. We can check the broadcast transformation using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcast_to()&lt;/code&gt; numpy function.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcast_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing and especially during normalization routines.&lt;/p&gt;

&lt;h3&gt; Element-Wise Operation Applies to Comparision and Functions &lt;/h3&gt;
&lt;p&gt;Comparison operations are also element-wise operations. For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.bool&lt;/code&gt; value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt;. It is also fine to assume that the function is applied to each element of the tensor.&lt;/p&gt;

&lt;div class=&quot;sidenote&quot;&gt; there are other ways to refer to element-wise operations, such as component-wise or point-wise &lt;/div&gt;

&lt;h2&gt; Argmax and Reduction Operations for Tensors &lt;/h2&gt;
&lt;p&gt;Now, we will focus in on the frequently used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;argmax()&lt;/code&gt; function, and we’ll see how to access the data inside our tensors.&lt;/p&gt;

&lt;h3&gt; Tensor Reduction Operation &lt;/h3&gt;

&lt;blockquote&gt; A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.&lt;/blockquote&gt;

&lt;p&gt;Reduction operations allow us to perform operations on element within a single tensor. Let’s look at an example. Suppose we have the following rank-2 tensor:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The sum of our tensor’s scalar components is calculated using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum()&lt;/code&gt; tensor method. The result of this call is a &lt;span class=&quot;rainbow&quot;&gt; scalar-valued tensor &lt;/span&gt;. Since the number of elements have been reduced by the operation, we can conclude that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum()&lt;/code&gt; method is a reduction operation.&lt;/p&gt;

&lt;p&gt;Other common reduction functions include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.sum()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.prod()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.mean()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.std()&lt;/code&gt;. All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor’s elements.&lt;/p&gt;

&lt;p&gt;Reduction operations in general allow us to compute aggregate values across data structures. But do reduction operations always reduce to a tensor with a single element? The answer is no. In fact, we often reduce specific axes at a time. This process is important.&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://deeplizard.com/learn/video/K3lX3Cltt4c&quot;&gt; deeplizard &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content><author><name>seri</name></author><category term="PyTorch" /><category term="featured" /><summary type="html">This blog post is for those who know the basics of PyTorch but want to go a step further. We will be diving into principles and applications of deep learning via PyTorch.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/pytorch.png" /><media:content medium="image" url="http://localhost:4000/assets/images/pytorch.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Guide to Linear Algebra (Part 1)</title><link href="http://localhost:4000/no-bullshit.html" rel="alternate" type="text/html" title="Guide to Linear Algebra (Part 1)" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>http://localhost:4000/no-bullshit</id><content type="html" xml:base="http://localhost:4000/no-bullshit.html">&lt;!--more--&gt;

&lt;h2&gt; Computational Linear Algebra &lt;/h2&gt;

&lt;p&gt;This chapter covers the computational aspects of performing matrix calculations. Understanding matrix computations is important because all later chapters depend on them. Suppose we’re given a huge matrix $ A \in R^{n \times n} $ with $ n=1000 $. Hidden behind the innocent-looking mathematical notation of the matrix inverse $A^{-1}$, the matrix product $AA$, and the matrix determinant $ | A |$, lie monster coputations involving all the $1000 \times 1000 = 1$ million entries of the matrix $A$. Millions of arithmetic operations must be performed, so I hope you have at least a thousand pencil ready!&lt;/p&gt;

&lt;p&gt;Okay, calm down. I won’t actually make you calculate millions of arithmetic operations. In fact, to learn linear algebra, it is sufficient to know how to carry out calculations with $3 \times 3$ and $4 \times 4$ matrices. Even for such moderately sized matrices, computing products, inverses, and determinants by hand are serious computational tasks. If you’re ever required to take a linear algebra final exam, you need to make sure you can do these calculations quickly. Even if no exam looms in your imminent future, it’s important to practice matrix operations by hand to get a feel for them.&lt;/p&gt;

&lt;p&gt;This chapter will introduce you to the following computational tasks involving matrices:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span class=&quot;blue&quot;&gt;Gauss-Jordan elimination &lt;/span&gt; Suppose we're trying to solve two equations in two unknowns $x$ and $y$:

$$ 
\displaylines{
ax+by = c \\\
dx+ ey= f
}
$$

If we add $\alpha$\times the first equation to the second equation, we obtain an equivalent system of equations:

$$
\displaylines{
ax + by = c \\\
(d + \alpha a)x + (e + \alpha b)y = f + \alpha c
}
$$

This is called a &lt;span class=&quot;highlight-sketch&quot;&gt; row operation &lt;/span&gt;: we added $\alpha$-times the first row to the second row. Row operations change the coefficient of the system of equations, but leave the solution unchanged. Gauss-Jordan elimination is a systematic procedure for solving systems of linear equations using row operations. &lt;/li&gt;
&lt;li&gt; &lt;span class=&quot;blue&quot;&gt; Matrix product &lt;/span&gt; The product $AB$ between matrices $A \in \mathbb{R}^{m \times l}$ and $B \in \mathbb{R}^{l \times n}$ is the matrix $C \in \mathbb{R}^{m \times n}$ whose coefficients $c_{ij}$ are defined by the formula $c_{ij} = \sum_{k=1}^{l}a_{ik}b_{kj}$ for all $i \in \lbrack 1, \dots, m \rbrack $ and $j \in \lbrack 1, \dots, n \rbrack $. We'll soon unpack this formula and learn about its intuitive interpretation: that computing $C = AB$ is computing all the dot products between the rows of $A$ and the columns of $B$. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;blue&quot;&gt; Determinant &lt;/span&gt; The determinant of a matrix $A$, denoted $|A|$ is an operation that gives us useful information about the linear independence of the rows of the matrix. The determinant is connected to many notions of linear algebra: linear independence, geometry of vectors, solving systems of equations, and matrix invertibility. We'll soon discuss these aspects. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;blue&quot;&gt; Matrix inverse &lt;/span&gt; We'll build upon our knowledge of Gauss-Jordan elimination, matrix products, and determinants to derive three different procedures for computing the matrix inverse $A^{-1}$.&lt;/li&gt;&lt;/ul&gt;

&lt;h3&gt; Reduced Row Echelon Form &lt;/h3&gt;
&lt;p&gt;In this section, we’ll learn to solve systems of linear equations using the Gauss-Jordan elimination procedure. A system of equations can be represented as a matrix of coefficients. The Gauss-Jordan elimination procedure converts any matrix into its &lt;span class=&quot;highlight-sketch&quot;&gt; &lt;b&gt; reduced row echelon form (RREF) &lt;/b&gt;&lt;/span&gt;. We can easily find the solution (or solutions of the system of equations from the RREF.&lt;/p&gt;

&lt;p&gt;Listen up: the material covered in this section requires your full on, caffeinated attention, as the procedures you’ll learn are somewhat tedious. Gauss-Jordan elimination involves many repetitive mathematical manipulations of arrays of numbers. It’s important you hang in there and follow through the step-by-step manipulations, as well as verify each step I present on your own with pen and paper.&lt;/p&gt;

&lt;h4&gt; Solving Equations &lt;/h4&gt;
&lt;p&gt;Suppose you’re asked to solve the following system of equations:&lt;/p&gt;

\[\displaylines{
1x_1 + 2x_2  = 5 \\\
3x_1 + 9x_2 = 21}\]

&lt;p&gt;The standard approach is to use one of the equation-solving tricks we learned to combine the equations and find the values of the two unknowns $x_1$ and $x_2$.&lt;/p&gt;

&lt;p&gt;Observe that the names of the two unknowns are irrelevant to the solution of the system of equations. Indeed, the solution $(x_1, x_2)$ to the above system of equations is the same as the solution $(s,t)$ to the system of equations&lt;/p&gt;

\[\displaylines{
1s+ 2t = 5 \\\
3s+ 9t = 21}\]

&lt;p&gt;The important parts of a system of linear equations are the &lt;span class=&quot;highlight-yellow&quot;&gt; coefficients &lt;/span&gt; in front of the variables and the constants on the right-hand side of each equation.&lt;/p&gt;

&lt;h4&gt; Augmented Matrix &lt;/h4&gt;
&lt;p&gt;The system of linear equations can be written as an &lt;span class=&quot;highlight-green&quot;&gt; augmented matrix &lt;/span&gt;:&lt;/p&gt;

\[\begin{pmatrix} 1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \\\
                3 &amp;amp; 9 &amp;amp;\bigm | &amp;amp; 21
\end{pmatrix}\]

&lt;p&gt;The first column corresponds to the coefficients of the first variable, the second column is for the second variable, and the last column corresponds to the constants of the right-hand side. It is customary to draw a vertical line where the equal signs in the equations would normally appear. This line helps distinguish the coefficients of the equations from the column of constants on the right-hand side.&lt;/p&gt;

&lt;p&gt;Once we have the augmented matrix, we can simplify it by using &lt;span class=&quot;highlight-yellow&quot;&gt; row operations &lt;/span&gt; (which we’ll discuss shortly) on its entries. After simplification by row operations, the augmented matrix will be transformed to&lt;/p&gt;

\[\begin{pmatrix}
1 &amp;amp; 0 &amp;amp;\bigm | &amp;amp; 5 \\\
0 &amp;amp; 1 &amp;amp;\bigm | &amp;amp; 2
\end{pmatrix}\]

&lt;p&gt;which corresponds to the system of equations&lt;/p&gt;

\[\displaylines{
x_1 = 1 \\\
x_2 = 2}\]

&lt;p&gt;This is a &lt;span class=&quot;monospace&quot;&gt; trivial &lt;/span&gt; system of equations; there is nothing left to solve and we can see that the solutions are $x_1 = 1$ and $x_2 = 2$. This example illustrates the general idea of the Gauss-Jordan elimination procedure for solving the system of equations by manipulating an augmented matrix.&lt;/p&gt;

&lt;h4&gt; Row Operations &lt;/h4&gt;
&lt;p&gt;We can manipulate the rows of an augmented matrix without changing its solutions. We’re allowed to perform the following three types of row operations:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt; Add a multiple of one row to another row &lt;/li&gt;
&lt;li&gt; Swap the position of the two rows &lt;/li&gt;
&lt;li&gt; Multiply a row by a constant &lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Let’s trace the sequence of row operations needed to solve the system of equations&lt;/p&gt;

\[\displaylines{x_1 + 2x_2 = 5 \\\
3x_1 + 9x_2 = 21}\]

&lt;p&gt;starting from its augmented matrix:&lt;/p&gt;

\[\begin{pmatrix}
1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \\\
3 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 21
\end{pmatrix}\]

&lt;ol&gt;
  &lt;li&gt;As a first step, we eliminate the first variable in the second row by subtracting three times the first row from the second row.&lt;/li&gt;
&lt;/ol&gt;

\[\begin{pmatrix}
1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \\\
0 &amp;amp; 3 &amp;amp;\bigm | &amp;amp; 6
\end{pmatrix}\]

&lt;p&gt;We denote this row operation as $R_2 \leftarrow R_2 - 3R_1$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To simplify the second row, we divide it by 3 to obtain&lt;/li&gt;
&lt;/ol&gt;

\[\begin{pmatrix}
1 &amp;amp; 2 &amp;amp;\bigm | &amp;amp; 5 \\\
0 &amp;amp; 1 &amp;amp;\bigm | &amp;amp; 2 
\end{pmatrix}\]

&lt;p&gt;This row operation is denoted $R_2 \leftarrow \frac{1}{3}R_2$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The final step is to eliminate the second variable from the first row. We do this by subtracting two times the second row from the first row $R_1 \leftarrow R_1 - 2 R_2$:&lt;/li&gt;
&lt;/ol&gt;

\[\begin{pmatrix}
1 &amp;amp; 0 &amp;amp;\bigm | &amp;amp; 1 \\\
0 &amp;amp;  1 &amp;amp;\bigm | &amp;amp; 2
\end{pmatrix}\]

&lt;p&gt;We can now read off the solution: $x_1 =1$ and $x_2 = 2$.&lt;/p&gt;

&lt;p&gt;Note how we simplified the augmented matrix through a specific procedure: we followed the &lt;span class=&quot;rainbow&quot;&gt; Gauss-Jordan elimination algorithm &lt;/span&gt; to bring the matrix into its reduced row echelon form.&lt;/p&gt;

&lt;p&gt;The reduced row echelon form (RREF) is the simplest form for an augmented matrix. Each row contains a &lt;span class=&quot;circle-sketch-highlight&quot;&gt;&lt;b&gt; leading one &lt;/b&gt;&lt;/span&gt; (a numeral 1) also known as a &lt;span class=&quot;circle-sketch-highlight&quot;&gt; &lt;b&gt;pivot &lt;/b&gt;&lt;/span&gt;. Each column’s pivot is used to eliminate the numbers that lie below and above it in the same column. The end result of this procedure is the reduced row echelon form:&lt;/p&gt;

\[\begin{pmatrix}
1 &amp;amp; 0 &amp;amp; \ast &amp;amp; 0 &amp;amp;\bigm | \ast \\\
0 &amp;amp; 1 &amp;amp; \ast &amp;amp; 0 &amp;amp;\bigm | \ast \\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp;\bigm | \ast
\end{pmatrix}\]

&lt;p&gt;Note the matrix contains only zero entries below and above the pivots. The asterisks $\ast$ denote arbitrary numbers that could not be eliminated because no leading one is present in these columns.&lt;/p&gt;

&lt;div class=&quot;definition&quot;&gt;The solution to a system of linear equations in the variables $x_1, x_2, \dots, x_n$ is the set of values $\{(x_1, x_2, \dots, x_n)\}$ that satisfy all the equations.&lt;/div&gt;

&lt;div class=&quot;definition&quot;&gt; Gaussian elimination is the process of bringing a matrix into row echelon form. &lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; A matrix is said to be in row echelon form (REF) if all entries below the leading ones are zero. This form can be obtained by adding or subtracting the row with the leading one from the rows below it. &lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; Gaussian-Jordan elimination is the process of bringing a matrix into reduced row echelon form. &lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; A matrix is said to be in reduced row echelon form (RREF) if all the entries below and above the pivots are zero. Starting from the REF, we obtain the RREF by subtracting the row containing the pivots from the rows above them.&lt;/div&gt;
&lt;div class=&quot;definition&quot;&gt; the rank of the matrix $A$ is the number of pivots in the RREF of $A$. &lt;/div&gt;

&lt;h3&gt; Number of Solutions &lt;/h3&gt;

&lt;p&gt;A system of linear equations in three variables could have:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; one solution &lt;/span&gt; If the RREF of a matrix has a pivot in each row, we can read off the values of the solution by inspection. 

$$

\begin{bmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; c_1 \\\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; c_2 \\\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; c_3
\end{bmatrix}

$$

The unique solution is $x_1 = c_1$, $x_2 = c_2$, and $x_3 = c_3$. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; Infinitely many solutions 1 &lt;/span&gt; If one of the equations is redundant, a row of zeros will appear when the matrix is brought to the RREF. This happens when one of the original equations is a linear combination of the other two. In such cases, we're really solving two equations  in three variables, so can't pin down one of the unknown variables. We say the solution contains &lt;span class=&quot;underline&quot;&gt; a free variable &lt;/span&gt;. For example, consider the following RREF:

$$
\begin{bmatrix} 
1 &amp;amp; 0 &amp;amp; a_1 &amp;amp; c_1 \\\
0 &amp;amp; 1 &amp;amp; a_2 &amp;amp; c_2 
\end{bmatrix}
$$

The column that doesn't contain a leading one corresponds to the free variable. To indicate that $x_3$ is a free variable, we give it a special label $x_3 \equiv t$. The variable $t$ could be any number $t \in \mathbb{R}$. In other words, when we say $t$ is free, it means $t$ can take on any value from $-\infty$ to $+\infty$. The information in the augmented matrix can now be used to express $x_1$ and $x_2$ in terms of the right-hand constants and the free variable $t$:

$$
\begin{Bmatrix} x_1 = c_1 - a_1 t \\ x_2 = c_2 - a_2 t \\ x_3 = t, \forall t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} c_1 \\ c_2 \\ 0 \end{bmatrix} + t \begin{bmatrix} -a_1 \\ -a_2 \\ 1 \end{bmatrix}, \forall t \in \mathbb{R}\end{Bmatrix}
$$.

The solution corresponds to the equation of a line passing through the point $(c_1, c_2, 0)$ with direction vector $(-a_1, -a_2, 1)$. We'll discuss the geometry of lines in the next section. For now, it's important that you understand that a system of equations can have more than one solution; any point on the line $l \equiv \{(c_1, c_2, 0) + t(-a_1, -a_2, 1), \forall t \in \mathbb{R}\}$ is a solution to the above system of equations. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; Infinitely many solutions 2 &lt;/span&gt; It's also possible to obtain a two-dimensional solution space. This happens when two of the three equations are redundant. In this case, there will be a single leading one, and thus two free variables. For example, in the RREF

$$
\begin{bmatrix} 
0 &amp;amp; 1 &amp;amp; a_1 &amp;amp; c_1 \\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 
\end{bmatrix}
$$

the variables $x_1$ and $x_3$ are free. As in the previous infinitely-many-solutions case, we define new labels for the free variables $x_1 \equiv s$ and $x_3 \equiv t$, where $ s \in \mathbb{R}$ and $t \in \mathbb{R}$ are two arbitrary numbers. The solution to this system of equations is 

$$ 
\begin{Bmatrix} 
x_1 = s \\ x_2 = c_2 - a_2 t \\ x_3 = t , \\ \forall s,t \in \mathbb{R} \end{Bmatrix} = \begin{Bmatrix} \begin{bmatrix} 0 \\ c_2 \\ 0 \end{bmatrix} + s \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + t \begin{bmatrix} 0 \\ -a_2 \\ 1 \end{bmatrix}, \forall s,t \in \mathbb{R} 
\end{Bmatrix} 
$$

This solution set corresponds to the parametric equation of a plane that contains the point $(0,c_2, 0)$ and the vectors $(1,0,0)$ and $(0, -a_2, 1)$. 

The general equation for the solution plane is $0x+1y+a_2z = c_2$, as can be observed from the first row of the augmented matrix. In the next section, we'll learn more about the geometry of planes and how to convert between their general and parametric forms. &lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;rainbow&quot;&gt; no solutions &lt;/span&gt; If there are no numbers $(x_1, x_2, x_3)$ that simultaneously satisfy all three equations, the system of equations has no solution. An example of a system of equations with no solution is the pair $ s+t = 4$ and $s+t = 44$. There are no numbers $(s,t)$ that satisfy both these equations. 

A system of equations has no solution if its reduced row echelon form contains a row of zero coefficients with a nonzero constant in the right-hand side:

$$
\begin{Bmatrix}
\begin{array}{ccc|c}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; c_1 \\\ 
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; c_2 \\\ 
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; c_3 
\end{array}
\end{Bmatrix}
$$

If $c_3 \neq 0$ this system of equations is impossible to satisfy. There is no solution because there are no numbers $(x_1, x_2, x_3)$ such that $0x_1 + 0x_2 + 0x_3 = c_3$. 

Dear reader, we've reached the first moment in this book where you'll need to update your math vocabulary. The solution to an individual equation is a finite set of points. The solution to a system of equations can be an entire space containing infinitely many points, such as a line or a plane. 

The solution set of a system of three linear equations in three unknowns could be either the empty set $\{0\}$ (no solution), a set with one element $\{(x_1, x_2, x_3)\}$, or a set with infinitely many elements like a line $\{p_o + t \overrightarrow{v}, t \in \mathbb{R}\}$ or a plane $\{p_o + s \overrightarrow{v} + t \overrightarrow{w}, s,t \in \mathbb{R}\}$. Another possible solution set is all of $\mathbb{R}^3$; every vector $ \overrightarrow{x} \in \mathbb{R}^3 $ is a solution to the equation 

$$
\begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 \\ 0 &amp;amp; 0 &amp;amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
$$

Note the distinction between the three types of infinite solution sets. A line is one-dimensional, a plane is two-dimensional, and $\mathbb{R}^3$ is three-dimensional. Describing all points on a line requires one parameter, describing all points on a plane takes two parameters, and-of course-describing a point in $\mathbb{R}^3$ takes three parameters. 

&lt;h3&gt; Geometric Interpretation &lt;/h3&gt;
We can gain some intuition about solution sets by studying the geometry of the intersections of lines in $\mathbb{R}^2$ and planes in $\mathbb{R}^3$. 

&lt;h4&gt; Lines in two dimensions &lt;/h4&gt;
Equations of the form $ax+by = c$ corresponds to lines in $\mathbb{R}^2$. Solving systems of linear equations of the form

$$
\displaylines{
a_1 x + b_1 y = c_1 \\\
a_2 x + b_2 y = c_2
}
$$

requires finding the point $(x,y) \in \mathbb{R}^2$ where these lines intersect. There are three possibilities for the solution set:
- &lt;span class=&quot;highlight-sketch&quot;&gt; one solution &lt;/span&gt; if the two lines intersect at a point. 
- &lt;span class=&quot;highlight-sketch&quot;&gt; infinitely many solutions &lt;/span&gt; if the lines are superimposed.
- &lt;span class=&quot;highlight-sketch&quot;&gt; no solution &lt;/span&gt; if the two lines are parallel and never intersect.

&lt;h4&gt; Planes in three dimensions &lt;/h4&gt;
Equations of the form $ax+by+cz = d$ correspond to planes in $\mathbb{R}^3$. When solving three such equations, 

$$ 
\displaylines{
a_1 x + b_1 y + c_1 z = d_1 \\\
a_2 x + b_2 y + c_2 z = d_2 \\\
a_3 x + b_3 y + c_3 z = d_3
}
$$

we want to find a set of points $(x,y,z)$ that satisfy all three equations simultaneously. There are four possibilities for the solution set:

1. &lt;span class=&quot;highlight-green&quot;&gt; one solution &lt;/span&gt; three non-parallel planes intersect at a point. 
2. &lt;span class=&quot;highlight-yellow&quot;&gt; infinitely many solutions 1 &lt;/span&gt; if only one of the plane equations is redundant, the solution corresponds to the intersection of two planes which is a line.
3. &lt;span class=&quot;highlight-green&quot;&gt; infinitely many solutions 2 &lt;/span&gt; if two of the equations are redundant, then the solution space is a two-dimensional space. 
4. &lt;span class=&quot;highlight-yellow&quot;&gt; if two (or more) of the planes are parallel, they will never intersect. &lt;/span&gt;

&lt;h3&gt; Determinants &lt;/h3&gt;
&lt;h4&gt; Overview &lt;/h4&gt; 
What is the volume of a rectangular box of length $1m$, width $2$ and height $3m$? It's easy to compute the volume of this box because its shape is right rectangular prism. The volume of this prism is $V = l \times w \times h = 6m^3$. What if the shape of the box was a parallelpiped instead? A parallelpiped is a box whose opposite faces are parallel but whose sides are slanted. How do we compute the volume of a parallelpiped? The determinant operation, specifically the $3 \times 3$ determinant, is the perfect tool for this purpose. 

The determinant of a matrix, denoted $det(A)$ or $|A|$, is a particular way to multiply the entries of the matrix to produce a single number. We use determinants for all kinds of tasks: to compute areas and volumes, to solve systems of linear equations, to check whether a matrix is invertible or not, etc.

We can interpret the determinant of a matrix intuitively as a geometrical calculation. The &lt;span class=&quot;blue&quot;&gt; determinant &lt;/span&gt; is the &lt;span class=&quot;blue&quot;&gt; volume &lt;/span&gt; of the geometric shape whose edges are the rows of the matrix. For $2 \times 2$ matrices, the determinant corresponds to the area of a parallelogram. For $3 \times 3$ matices, the determinant corresponds to the volume of a parallelpiped. For dimensions $d &amp;gt; 3$, we say the determinant measures a &lt;span class=&quot;blue&quot;&gt; $d$-dimensional hyper-volume &lt;/span&gt;.

Consider the linear transformation $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined through the matrix-vector product with a matrix $A_T: T(\overrightarrow{x}) \equiv A_T \overrightarrow{x}$. The determinant of the matrix $A_T$ is the &lt;span class=&quot;blue&quot;&gt; scale factor &lt;/span&gt; associated with the linear transformation $T$. The scale factor of the linear transformation $T$ describes &lt;mark&gt; how the area of a unit square in the input space (a square with dimensions $1 \times 1$) is transformed by $T$ &lt;/mark&gt;. After passing through $T$, the unit square is transformed to a parallelogram with with area $det(A_T)$. Linear transformations that shrink areas have $det(A_T) &amp;lt; 1$, while linear transformations that enlarge areas have $det(A_T) &amp;gt; 1$. A linear transformation that is area preserving has $det(A+T) = 1$. 

The determinant is also used to check &lt;span class=&quot;blue&quot;&gt; linear independence &lt;/span&gt; for a given set of vectors. We &lt;span class=&quot;underline&quot;&gt; construct a matrix using the vectors as the matrix rows &lt;/span&gt;, and compute its determinant. 

The determinant of a matrix tells us whether or not that matrix is &lt;span class=&quot;blue&quot;&gt; invertible &lt;/span&gt;. If $det(A) \neq 0$, then $A$ is invertible; if $det(A) = 0$, $A$ is not invertible. 

The determinant shares a connection with the &lt;span class=&quot;blue&quot;&gt; vector cross product &lt;/span&gt;, and is used in the definition of the &lt;span class=&quot;blue&quot;&gt; eigenvalue equation &lt;/span&gt;.

&lt;h4&gt; Formulas &lt;/h4&gt;
The determinant of a $2 \times 2$ matrix is 

$$
det(\begin{bmatrix} a_{11} &amp;amp; a_{12} \\ a_{21} &amp;amp; a_{22} \end{bmatrix}) = \begin{vmatrix} a_{11} &amp;amp; a_{12} \\ a_{21} &amp;amp; a_{22} \end{vmatrix} = a_{11}a_{22} - a_{12}a{21}
$$

The formulas for the determinants of larger matrices are defined recursively. For example, the determinant of $3 \times 3$ matirx is defined in terms of $2 \times 2$ determinants:

$$ 
\begin{vmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{vmatrix} = a_{11} 

= a_{11} \begin{vmatrix}a_{22} &amp;amp; a_{23} \\ a_{32} &amp;amp; a_{33} \end{vmatrix} -a_{12} \begin{vmatrix} a_{21} &amp;amp; a_{23} \\ a_{31} &amp;amp; a_{33} \end{vmatrix}+ a_{13} \begin{vmatrix} a_{21} &amp;amp; a_{22} \\ a_{31} &amp;amp; a_{32} \end{vmatrix}
$$ 

There's a neat computational trick for computing $3 \times 3$ determinants by hand. The trick consists of extending the matrix $A$ into a $3 \times 5$ array that contains copies the columns of $A$: the $1^{st}$ column of $A$ is copied to the $4^{th}$ column of the extended array, and the $2^{nd}$ column of $A$ is copied to the $5^{th}$ column. The determinant is then computed by summing the products of the entries on the three positive diagonals and subtracting the products of the entries on the three negative diagonals. 

&lt;picture&gt;&lt;img src=&quot;/assets/images/3x3.png&quot; /&gt;&lt;/picture&gt;

The general formula for the determinant of an $n \times n$ matrix is

$$
det(A)  = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}
$$

where $M_{ij}$ is called the &lt;mark&gt; minor &lt;/mark&gt; associated with the entry $a_{ij}$. The minor $M_{ij}$ is the determinant of the submatrix obtained by removing the $i^{th}$ row and the $j^{th}$ column of the matrix $A$. Note the alternating factor $(-1)^{i+j}$ that changes value between $+1$ and $-1$ for different terms in the formula.

The determinant of a $4 \times 4$ matrix $B$ is 

$$ 
det(B) = b_{11}M_{11} - b_{12}M_{12} + b_{13}M_{13} - b_{14}M_{14}
$$

The general formula for determinants $det(A) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} M_{1j}$ assumes we're expanding the determinant along the first row of the matrix. In fact, a determinant formula can be obtained by expanding the determinant along anyrow or column of the matrix.

The expand-along-any-row-or-column nature of determinants can be very handy: if you need to calculate the determinant of a matrix with one row (or column) containing many zero entries, it makes sense to expand along that row since many of the terms in the formula will be zero. If a matrix contains a row (or column) consisting entirely of zeros, we can immediately tell its determinant is zero. 

&lt;h4&gt; Geometric interpretation &lt;/h4&gt;
&lt;h5&gt; Area of a parallelogram &lt;/h5&gt;
Suppose we're given vectors $overrightarrow{v} = (v_1, v_2)$ and $\overrightarrow{w} = (w_1, w_2)$ in $\mathbb{R}^2$ and we construct a parallelogram with corner points $(0,0), \overrightarrow{v}, \overrightarrow{w}$ and $\overrightarrow{v} + \overrightarrow{w}$.

The area of this parallelogram is equal to the determinant of the matrix that contains $(v_1, v_2)$ and $(w_1, w_2)$ as rows:

$$
area = \begin{vmatrix} v_1 &amp;amp; v_2 \\ w_1 &amp;amp; w_2 \end{vmatrix} = v_1 w_2 - v_2 w_1 
$$

&lt;h5&gt; Volume of a parallelpiped &lt;/h5&gt; 
&lt;h5&gt; Sign and absolute value of the determinant &lt;/h5&gt;
Calculating determinants can produce positive or negative numbers. 
&lt;/li&gt;&lt;/ul&gt;</content><author><name>seri</name></author><category term="linear algebra" /><category term="featured" /><summary type="html">This article goes through the fundamentals of linear algebra. Linear algebra is the branch of mathematics concerning linear equations and their representations in vector spaces. This ultimate guide will prove very useful or understanding deep learning techniques.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/linear.png" /><media:content medium="image" url="http://localhost:4000/assets/images/linear.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision</title><link href="http://localhost:4000/point-cloud.html" rel="alternate" type="text/html" title="Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>http://localhost:4000/point-cloud</id><content type="html" xml:base="http://localhost:4000/point-cloud.html">&lt;h2&gt; Introduction &lt;/h2&gt;
&lt;p&gt;In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is affected by a variety of components. From the electro-optic sensors to the image resolution, contrast, exposure and blurriness variables, all add to the complexity of analyzing a scene and processing the imagery.&lt;/p&gt;

&lt;h2&gt; Monocular Vision &lt;/h2&gt;
&lt;p&gt;Traditional stereoscopy systems, such as human visual system, utilize multiple viewing angles of the same object in order to do triangulation and get a depth perception. Monocular vision, better described as vision through a single camera source, presents new challenges when compared to stereo vision or a multi-camera system. In a stereo system, similar to human vision, distances between the cameras (the baseline) and their orientation is known and in most circumstances remains constant. In order to generate various viewing angles with a monocular system, the camera must continuously be moving. With a moving camera, the system obtains two different viewing angles form two points in time. The challenge becomes to accurately compute the distance the camera has traveled or the exact location of the camera at each frame of video. In addition, the orientation of the camera at each point in time must be computed from the scene.&lt;/p&gt;

&lt;h2&gt; Visual Structure from Motion &lt;/h2&gt;
&lt;p&gt;In his work, Wu et al. describes the methodology of Visual Structure from Motion as following. Using a set of image feature locations and correspondences, the goal of bundle adjustment is to find 3D point positions and camera parameters that minimize the re-projection error. This optimization problem is constructed as a non-linear least squares problem, where the error is the squared $L_2$ norm of the difference between the observed feature location and the projection of the corresponding 3D point on the image plane of the camera.&lt;/p&gt;

&lt;p&gt;Wu explains by letting $x$ be a vector of parameters and $f(x) = [f_1(x), /dots, f_k(x)]$ be the vector of residual errors for 3D reconstruction. Then the optimization problem he wishes to solve is the non-linear least squares problem shown in the below equation.
\(x* = \argmin_{x} \sum_{i=1}^{k} \lVert f_i(x) \rVert^2\)&lt;/p&gt;

&lt;p&gt;The Levenberg-Marquardt (LM) algorithm is an extremely popular algorithm for solving non-linear least squares problems and is the algorithm choice for bundle adjustment. LM operates by computing a series of regularized linear approximations to the original nonlinear problem. Let $J(x)$ be the Jacobian of $f(x)$, then in each iteration LM solves a linear least squares problem of the form
\(\delta* = \argmin_{\delta} \lVert J(x)\delta + f(x) \rVert^2 + \lambda \lVert D(x) \delta \rVert^2\)
and updates $x \lefta.rrow x + \delta&lt;em&gt;$ if $\lVert f(x+\delta&lt;/em&gt;)\rVert &amp;lt; \lVert f(x) \rVert$. Here $D(x)$ is a non-negative diagonal matrix, typically the square root of the diagonal of the matrix $J(x)^TJ(x)$ and $\lambda$ is a nonnegative parameter that controls the strength of regularization. Wu explains that the regularization is needed to ensure a convergent algorithm. LM updates the value of $\lambda$ at each step based on how well the Jacobian $J(x)$ approximates $f(x)$.&lt;/p&gt;

&lt;h2&gt; SIFT Features &lt;/h2&gt;

&lt;p&gt;The SIFT algorithm can be broken down into four main stages:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;scale-space peak selection&lt;/li&gt;
  &lt;li&gt;point localization&lt;/li&gt;
  &lt;li&gt;orientation assignment&lt;/li&gt;
  &lt;li&gt;point descriptor
The first stage is to search for interest points over location and scale. The image is constructed in a Gaussian Pyramid, where the image is downsampled and blurred at each level. These blurred images at each level are used to compute the Difference of Gaussians (DoG), which locate edges and corners within an image. Interesting points are then extracted in stage 2 by locating the maxima/minimal pixels within different scales of DoG at sub-pixel accuracy. Once interest points have been located, an orientation is assigned based on the gradient orientation of the pixels around the interest point.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once orientation and scale have been addressed, the final stage is the generation of point descriptors.&lt;/p&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">Introduction In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is affected by a variety of components. From the electro-optic sensors to the image resolution, contrast, exposure and blurriness variables, all add to the complexity of analyzing a scene and processing the imagery. Monocular Vision Traditional stereoscopy systems, such as human visual system, utilize multiple viewing angles of the same object in order to do triangulation and get a depth perception. Monocular vision, better described as vision through a single camera source, presents new challenges when compared to stereo vision or a multi-camera system. In a stereo system, similar to human vision, distances between the cameras (the baseline) and their orientation is known and in most circumstances remains constant. In order to generate various viewing angles with a monocular system, the camera must continuously be moving. With a moving camera, the system obtains two different viewing angles form two points in time. The challenge becomes to accurately compute the distance the camera has traveled or the exact location of the camera at each frame of video. In addition, the orientation of the camera at each point in time must be computed from the scene. Visual Structure from Motion In his work, Wu et al. describes the methodology of Visual Structure from Motion as following. Using a set of image feature locations and correspondences, the goal of bundle adjustment is to find 3D point positions and camera parameters that minimize the re-projection error. This optimization problem is constructed as a non-linear least squares problem, where the error is the squared $L_2$ norm of the difference between the observed feature location and the projection of the corresponding 3D point on the image plane of the camera. Wu explains by letting $x$ be a vector of parameters and $f(x) = [f_1(x), /dots, f_k(x)]$ be the vector of residual errors for 3D reconstruction. Then the optimization problem he wishes to solve is the non-linear least squares problem shown in the below equation. \(x* = \argmin_{x} \sum_{i=1}^{k} \lVert f_i(x) \rVert^2\) The Levenberg-Marquardt (LM) algorithm is an extremely popular algorithm for solving non-linear least squares problems and is the algorithm choice for bundle adjustment. LM operates by computing a series of regularized linear approximations to the original nonlinear problem. Let $J(x)$ be the Jacobian of $f(x)$, then in each iteration LM solves a linear least squares problem of the form \(\delta* = \argmin_{\delta} \lVert J(x)\delta + f(x) \rVert^2 + \lambda \lVert D(x) \delta \rVert^2\) and updates $x \lefta.rrow x + \delta$ if $\lVert f(x+\delta)\rVert &amp;lt; \lVert f(x) \rVert$. Here $D(x)$ is a non-negative diagonal matrix, typically the square root of the diagonal of the matrix $J(x)^TJ(x)$ and $\lambda$ is a nonnegative parameter that controls the strength of regularization. Wu explains that the regularization is needed to ensure a convergent algorithm. LM updates the value of $\lambda$ at each step based on how well the Jacobian $J(x)$ approximates $f(x)$. SIFT Features The SIFT algorithm can be broken down into four main stages: scale-space peak selection point localization orientation assignment point descriptor The first stage is to search for interest points over location and scale. The image is constructed in a Gaussian Pyramid, where the image is downsampled and blurred at each level. These blurred images at each level are used to compute the Difference of Gaussians (DoG), which locate edges and corners within an image. Interesting points are then extracted in stage 2 by locating the maxima/minimal pixels within different scales of DoG at sub-pixel accuracy. Once interest points have been located, an orientation is assigned based on the gradient orientation of the pixels around the interest point. Once orientation and scale have been addressed, the final stage is the generation of point descriptors.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cnn.png" /><media:content medium="image" url="http://localhost:4000/assets/images/cnn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Depth Estimation - An Introduction</title><link href="http://localhost:4000/depth-intro.html" rel="alternate" type="text/html" title="Depth Estimation - An Introduction" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>http://localhost:4000/depth-intro</id><content type="html" xml:base="http://localhost:4000/depth-intro.html">&lt;h3&gt; Paradigms for 3D Images Representation over a Plane &lt;/h3&gt;
&lt;p&gt;As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the depth information should be able to be represented in a plane, for printing purposes, for example.&lt;/p&gt;

&lt;p&gt;There are three widely used modes for depth representation:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt; Gray scale 2.5D representation. This paradigm uses the gray scale intensity to represent the depth of each pixel in the image. Thus, the color, texture and luminosity of the original image are lost in this representation. The name &quot;2.5D&quot; refers to the fact that this kind of images has the depth information directly in each pixel, while it is represented over a 2D space. In this paradigm, the gray level represents the inverse of the distance. Thus, more a pixel is bright, closer is the point represented. Vice versa, the darker is a pixel, further is the represented point. This is most commonly used way for depth representation. &lt;/li&gt;
&lt;li&gt; Color 2.5D representation. This representation is similar to the previous one. The difference is the use of colors to represent depth. In the following image, red-black colors represent closer points, and blue-dark colors the further points. However, other color representations are available in the literature. &lt;/li&gt;
&lt;li&gt; Pseudo-3D representation. This representation provides different points of view of the reconstructed space. &lt;/li&gt;

The main advantage of the first two methods is the possibility of implementing objective comparison among algorithms, as it is done in the Middlebury database and test system. 

&lt;h3&gt; Important terms and issues in depth estimation &lt;/h3&gt;
The depth estimation world is quite complex research field, where many techniques and setups have been proposed. The set of algorithms which solve the depth map estimation problem deals with many different mathematical concepts which should be briefly explained for a minimum overall comprehension of the matter. In this section, we will review some important points about image processing applied to depth estimation.

&lt;h4&gt; Standard Test Beds &lt;/h4&gt;
The availability of common tests and comparable results is a mandatory constraint in active and widely explored fields. Likewise, the possibility of objective comparisons make easier to classify different proposals.

In depth estimation, and more specifically in stereo vision, one of the most important test bed is the Middlebury database and test bed. The test beds provides both eyes images of a 3D scene, as well as the ground truth map. The same test allow, as said, algorithms classification.

&lt;h4&gt; Color or Grayscale Images? &lt;/h4&gt;
The first point when we want to process an image, whichever is the goal, is to decide what to process. In this case, color or grayscale images. As it can be seen in the following figure, color images have much more information than gray scale images. Color images should, hence, be more appropriate for data extraction, among them, depth information. 

However, the color images have an important disadvantage: for a 256 level definition, they are represented by 3 bytes (24-bit representation), while grayscale images with the same level only require one single byte. The consequence is obvious: color image processing requires much more time and operations.

&lt;h4&gt; The Epipolar Geometry &lt;/h4&gt;
When dealing with stereo vision setups, we have to face the epipolar geometry problem. Let $C_l$ and $C_r$ be the focal centers of the left and right sensors (or eyes), and $L$ and $R$ the left and right image planes. Finally, $P$ will be a physical point of the scene and $p_l$ and $p_r$ the projections of $P$ over $L$ and $R$, respectively. 

&lt;picture&gt;
&lt;img src=&quot;/assets/images/epipolar.png&quot; /&gt;
&lt;/picture&gt;

In this figure, we can also see both &quot;epipoles&quot;, i.e., the points where the line connecting both focal centers intersect the image planes. They are noted as $e_l$ and $e_r$.

The geometrical properties of this setup force that every point of the line $Pp_l$ lies on the line $p_re_r$ which is called &quot;epipole line&quot;. The correspondence ofa poitn seen in one image must be searched in the corresponding epipolar line in the other one. A simplified version of the geometry arise when the image planes are parallel. This is the base of the so-called fronto-parallel hypothesis.

&lt;h4&gt; The Fronto-Parallel Hypothesis &lt;/h4&gt;
The epipolar geometry of two sensors can be simplified, as said, positioning both planes parallel, arriving to the following setup:

&lt;picture&gt;
&lt;img src=&quot;/assets/images/fronto.png&quot; /&gt;
&lt;/picture&gt;

The epipoles are placed in the infinite, and the epipolar (and search) lines become horizontal. The point (except the occluded ones) are only decaled horizontally. 

This geometrical setup can be implemented by properly orienting the sensors, or by means of mathematical transformation fo the original images. If the last option is the case, the result is called &quot;rectified image&quot;. 

The most important consequences of this geometry, regarding the Cartesian plane can be written as follows:
&lt;ul&gt;&lt;li&gt; $y_l = y_r$. The height of a physical point is the same in both images. &lt;/li&gt;
&lt;li&gt; $x_l = x_r + \nabla d$. The abcissa of a physical point is decaled by the so-called &lt;span class=&quot;blue&quot;&gt; parallax &lt;/span&gt; or &lt;span class=&quot;blue&quot;&gt; disparity &lt;/span&gt;, which is inversely related to the depth. &lt;/li&gt;
&lt;li&gt; A point in the infinite has identical abscissa coordinates in both image planes. &lt;/li&gt;&lt;/ul&gt;

&lt;h4&gt; Matching &lt;/h4&gt;
When different viewpoints from the same scene are compared, a further problem arises that is associated with the mutual identification of images. The solution to this problem is commonly referrred to as matching. The matching process consists of identifying each physical points within different images. However, matching techniques are not only used in stereo or multivision procedures but also widely used for image retrieval or fingerprint identification where it is important to allow rotational and scalar distortions.

There are also various constraints that are generally satisfied by true matches thus simplifying the depth estimation algorithm, such as similarity, smoothness, ordering and uniqueness. 

As we will see, the matching process is a conceptual approach to identify similar characteristics in different images. It is, then, subjected to errors. The matching is, hence, implemented by means of comparators allowing different identification strategies such as minimum square errors (MSE), sum of absolute differences (SAD) or sum of squared differences (SSD). The characteristic compared through the matching process can be anything quantifiable. Thus, we will see algorithms matching points, edges, regions or other image cues. 








&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://ksimek.github.io/2012/08/14/decompose/&quot;&gt; ksimek blog &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prateekvjoshi.com/2014/05/31/understanding-camera-calibration/&quot;&gt; prateekvjoshi blog &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">Paradigms for 3D Images Representation over a Plane As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the depth information should be able to be represented in a plane, for printing purposes, for example. There are three widely used modes for depth representation: Gray scale 2.5D representation. This paradigm uses the gray scale intensity to represent the depth of each pixel in the image. Thus, the color, texture and luminosity of the original image are lost in this representation. The name &quot;2.5D&quot; refers to the fact that this kind of images has the depth information directly in each pixel, while it is represented over a 2D space. In this paradigm, the gray level represents the inverse of the distance. Thus, more a pixel is bright, closer is the point represented. Vice versa, the darker is a pixel, further is the represented point. This is most commonly used way for depth representation. Color 2.5D representation. This representation is similar to the previous one. The difference is the use of colors to represent depth. In the following image, red-black colors represent closer points, and blue-dark colors the further points. However, other color representations are available in the literature. Pseudo-3D representation. This representation provides different points of view of the reconstructed space. The main advantage of the first two methods is the possibility of implementing objective comparison among algorithms, as it is done in the Middlebury database and test system. Important terms and issues in depth estimation The depth estimation world is quite complex research field, where many techniques and setups have been proposed. The set of algorithms which solve the depth map estimation problem deals with many different mathematical concepts which should be briefly explained for a minimum overall comprehension of the matter. In this section, we will review some important points about image processing applied to depth estimation. Standard Test Beds The availability of common tests and comparable results is a mandatory constraint in active and widely explored fields. Likewise, the possibility of objective comparisons make easier to classify different proposals. In depth estimation, and more specifically in stereo vision, one of the most important test bed is the Middlebury database and test bed. The test beds provides both eyes images of a 3D scene, as well as the ground truth map. The same test allow, as said, algorithms classification. Color or Grayscale Images? The first point when we want to process an image, whichever is the goal, is to decide what to process. In this case, color or grayscale images. As it can be seen in the following figure, color images have much more information than gray scale images. Color images should, hence, be more appropriate for data extraction, among them, depth information. However, the color images have an important disadvantage: for a 256 level definition, they are represented by 3 bytes (24-bit representation), while grayscale images with the same level only require one single byte. The consequence is obvious: color image processing requires much more time and operations. The Epipolar Geometry When dealing with stereo vision setups, we have to face the epipolar geometry problem. Let $C_l$ and $C_r$ be the focal centers of the left and right sensors (or eyes), and $L$ and $R$ the left and right image planes. Finally, $P$ will be a physical point of the scene and $p_l$ and $p_r$ the projections of $P$ over $L$ and $R$, respectively. In this figure, we can also see both &quot;epipoles&quot;, i.e., the points where the line connecting both focal centers intersect the image planes. They are noted as $e_l$ and $e_r$. The geometrical properties of this setup force that every point of the line $Pp_l$ lies on the line $p_re_r$ which is called &quot;epipole line&quot;. The correspondence ofa poitn seen in one image must be searched in the corresponding epipolar line in the other one. A simplified version of the geometry arise when the image planes are parallel. This is the base of the so-called fronto-parallel hypothesis. The Fronto-Parallel Hypothesis The epipolar geometry of two sensors can be simplified, as said, positioning both planes parallel, arriving to the following setup: The epipoles are placed in the infinite, and the epipolar (and search) lines become horizontal. The point (except the occluded ones) are only decaled horizontally. This geometrical setup can be implemented by properly orienting the sensors, or by means of mathematical transformation fo the original images. If the last option is the case, the result is called &quot;rectified image&quot;. The most important consequences of this geometry, regarding the Cartesian plane can be written as follows: $y_l = y_r$. The height of a physical point is the same in both images. $x_l = x_r + \nabla d$. The abcissa of a physical point is decaled by the so-called parallax or disparity , which is inversely related to the depth. A point in the infinite has identical abscissa coordinates in both image planes. Matching When different viewpoints from the same scene are compared, a further problem arises that is associated with the mutual identification of images. The solution to this problem is commonly referrred to as matching. The matching process consists of identifying each physical points within different images. However, matching techniques are not only used in stereo or multivision procedures but also widely used for image retrieval or fingerprint identification where it is important to allow rotational and scalar distortions. There are also various constraints that are generally satisfied by true matches thus simplifying the depth estimation algorithm, such as similarity, smoothness, ordering and uniqueness. As we will see, the matching process is a conceptual approach to identify similar characteristics in different images. It is, then, subjected to errors. The matching is, hence, implemented by means of comparators allowing different identification strategies such as minimum square errors (MSE), sum of absolute differences (SAD) or sum of squared differences (SSD). The characteristic compared through the matching process can be anything quantifiable. Thus, we will see algorithms matching points, edges, regions or other image cues. References ksimek blog prateekvjoshi blog</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/camera/camera.jpeg" /><media:content medium="image" url="http://localhost:4000/assets/images/camera/camera.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">3D Packing for Self-Supervised Depth Estimation</title><link href="http://localhost:4000/3d-packing.html" rel="alternate" type="text/html" title="3D Packing for Self-Supervised Depth Estimation" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>http://localhost:4000/3d-packing</id><content type="html" xml:base="http://localhost:4000/3d-packing.html">&lt;h2&gt; Self-Supervised Scale-Aware SfM &lt;/h2&gt;

&lt;p&gt;In self-supervised monocular SfM training, we aim to learn: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt; a monocular depth model $f_D = I \rightarrow D$ that predicts the scale-ambiguous depth $\hat{D} = f_D(I(p))$ for every pixel $p$ in the target image $I$;&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt; a monocular ego-motion estimator $f_x: (I_t, I_s) \rightarrow x_{t \rightarrow s}$ that predicts the set of 6-DoF rigid transformations for all $s \in S$ given by $x_{t \rightarrow s} = \begin{pmatrix} R &amp;amp; t \\ 0 &amp;amp; 1 \end{pmatrix} \in SE(3)$, between the target image $I_t$ and a set of source images $I_s \in I_S$ considered as part of the temporal context.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt; In practice, we use the frames $I_{t-1}$ and $I_{t+1}$ as source images, although using a larger context is possible. Note that in the case of monocular SfM, both depth and pose are estimated up to an unknown scale factor, due to the inherent ambiguity of the photometric loss.&lt;/p&gt;

&lt;h3&gt; Self-Supervised Objective &lt;/h3&gt;
&lt;p&gt;Following the work of Zhou et al., we train the depth and pose network simultaneously in a self-supervised manner. In this work, however, we learn to recover the inverse depth $f_d: I \rightarrow f_D^{-1}(I)$ instead, along with the ego-motion estimator $f_x$. The overall self-supervised objective consists of an appearance matching loss term $L_p$ that is imposed between the synthesized image $\hat{I_t}$ and the target image $I_t$, and a depth regularization term $L_s$ that ensures edge-aware smoothing in the depth estimates $\hat{D_t}$. The objective takes the following form:&lt;/p&gt;

\[L(I_t, \hat{I_t}) = L_p(I_t, I_s) \bigodot M_p \bigodot M_t + \lambdda_1 L_s(\hat{D_t})\]

&lt;p&gt;where $M_t$ is a binary mask that avoids computing the photometric loss on the pixels that do not have a valid mapping, and $\bigodot$ denotes element-wise multiplication. Additionally, $\lambda_1$ enforces a weighted depth regularization on the objective. The overall loss is averaged per-pixel, pyramid-scale and image batch during training.&lt;/p&gt;

&lt;picture&gt;
&lt;img src=&quot;/assets/images/packnet.png&quot; /&gt;
&lt;/picture&gt;
&lt;p&gt;&lt;span class=&quot;caption&quot;&gt; The proposed scale-aware self-supervised monocular structure-from-motion architecture. This paper introduces PackNet as a novel depth network, and optionally include weak velocity supervision at training time to produce scale-aware depth and pose models.&lt;/span&gt;&lt;/p&gt;

&lt;h4&gt; Appearance Matching Loss &lt;/h4&gt;
&lt;p&gt;The pixel-level similarity between the target image $I_t$ and the synthesized target image $\hat{I_t}$ is estimated using the Structural Similarity (SSIM) term combined with a L1 pixel-wise loss term, inducing an overall photometric loss given by the equation below.&lt;/p&gt;

\[L_p(I_t, \hat{I_t}) = \alpha \frac{1-SSIM(I_t, \hat{I_t})}{2} + (1 - \alpha) \lVert I_t - \hat{I_t} \rVert\]

&lt;p&gt;While multi-view projective geometry provides strong cues for self-supervision, errors due to parallax in the scene have an undesirable effect incurred on the photometric loss. We mitigate these undesirable effects by calculating the minimum photometric loss per pixel for each source image in the context $I_S$, so that:&lt;/p&gt;

\[L_p(I_t, I_S) = \min_{I_S} (I_t, \hat{I_t})\]

&lt;p&gt;The intuition is that the same pixel level will not be occluded or out-of-bounds in all context images, and that the association with minimal photometric loss should be the correct one. Furthermore, we also mask out static pixels by removing those which have a warped $L_p(I_t, \hat{I_t})$ higher than their corresponding unwarped photometric loss $L_p(I_t, I_s)$, calculated using the original source image without view synthesis. This auto-mask removes pixels whose appearance does not change between frames, which include static scenes and dynamic objects with no relative motion, since these will have smaller photometric loss when assuming no ego-motion.&lt;/p&gt;

\[M_p = \min_{I_S} L_p(I_t, I_s) \geq L_p(I_t, \hat{I_t})\]

&lt;h4&gt; Depth Smoothness Loss &lt;/h4&gt;
&lt;p&gt;In order to regularize the depth in texture-less low-image gradient regions, we incorporate an edge-aware term. The loss is weighted for each of the pyramid-levels, and is decayed by a factor of 2 on down-sampling, starting with a weight of 1 for the 0th pyramid level.&lt;/p&gt;

\[L\_s\(\hat{D\_t}) = |\delta\_x \hat{D\_t}|e^{-|\delta\_x I\_t|} + |\delta\_y \hat{D\_t}| e^{|\delta\_y I\_t|}\]

&lt;h3&gt; Scale-Aware SfM &lt;/h3&gt;
&lt;p&gt;As previously mentioned, both the monocular depth and ego-motion estimators $f_d$ and $f_x$ predict scale-ambiguous values, due to the limitations of the monocular SfM training objective. In other words, the scene depth and the camera ego-motion can only be estimated up to an unknown and ambiguous scale factor. This is also reflected in the overall learning objective, where the photometric loss is agnostic to the metric depth of the scene. Furthermore, we note that all previous approaches which operate in the self-supervised monocular regime suffer from this limitation, and resort to artificially incorporating this scale factor at test-time, using LiDAR measurements.&lt;/p&gt;

&lt;h4&gt; Velocity Supervision Loss &lt;/h4&gt;

&lt;h2&gt; PackNet: 3D Packing for Depth Estimation &lt;/h2&gt;
&lt;p&gt;Standard convolutional architectures use aggresive striding and pooling to increase their receptive field size. However, this potentially decreases the model performance for tasks requiring fine-grained representations. Similarly, traditional upsampling strategies fail to propagate and preserve sufficient details at the decoder layers to recover accurate depth predictions. In contrast, we propose a novel encoder-decoder architecture, called PackNet, that introduces new 3D packing and unpacking blocks to leran to jointly preserve and recover important spatial information for depth estimation. This is in alignments with recent observations that information loss is not a necessary condition to learn representations capable of generalizing to different scenarios. In fact, progressive expansion and contraction in a fully invertible manner, without discarding &lt;span class=&quot;highlight-pink&quot;&gt; uninformative &lt;/span&gt; input variability, has been shown to increase performance in a variety of tasks. We first describe the different blocks of our proposed architecture, and then proceed to show how they are integrated together in a single model for monocular depth estimation.&lt;/p&gt;

&lt;h3&gt; Packing Block &lt;/h3&gt;
&lt;p&gt;The packing block starts by folding the spatial dimensions of convolutional feature maps into extra feature channels via a &lt;span class=&quot;monospace&quot;&gt; Space2Depth &lt;/span&gt; operation. The resulting tensor is at a reduced resolution, but in contrast to striding or pooling, this transformation is invertible and comes at no loss. Next, we learn to compress this concatenated feature space in order to reduce its dimensionality to a desired number of output channels. As shown in experiments, 2D convolutions are not designed to directly leverage the tiled structure of this feature space. Instead, we propose to first learn to expand this structured representation via a 3D covolutional layer. The resulting higher dimensional feature space is then flattened (by simple reshaping) before a final 2D convolutional contraction layer. This structured feature expansion-contraction, inspired by invertible networks, although we do not ensure invertibility, allows our architecture to dedicate more parameters to learn how to compress key spatial details that need to be preserved for high resolution depth decoding.&lt;/p&gt;

&lt;h3&gt; Unpacking Block &lt;/h3&gt;
&lt;p&gt;Symmetrically, the unpacking block learns to decompress and unfold packed convolutional feature channels back into higher resolution spatial dimensions during the decoding process. The unpacking block replaces convolutional feature upsampling, typically performed via nearest-neighbor or with learnable transposed convolutional weights.&lt;/p&gt;</content><author><name>seri</name></author><category term="paper" /><category term="featured" /><summary type="html">Self-Supervised Scale-Aware SfM In self-supervised monocular SfM training, we aim to learn: &amp;lt;ul&amp;gt;&amp;lt;li&amp;gt; a monocular depth model $f_D = I \rightarrow D$ that predicts the scale-ambiguous depth $\hat{D} = f_D(I(p))$ for every pixel $p$ in the target image $I$;&amp;lt;/li&amp;gt; &amp;lt;li&amp;gt; a monocular ego-motion estimator $f_x: (I_t, I_s) \rightarrow x_{t \rightarrow s}$ that predicts the set of 6-DoF rigid transformations for all $s \in S$ given by $x_{t \rightarrow s} = \begin{pmatrix} R &amp;amp; t \ 0 &amp;amp; 1 \end{pmatrix} \in SE(3)$, between the target image $I_t$ and a set of source images $I_s \in I_S$ considered as part of the temporal context.&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt; In practice, we use the frames $I_{t-1}$ and $I_{t+1}$ as source images, although using a larger context is possible. Note that in the case of monocular SfM, both depth and pose are estimated up to an unknown scale factor, due to the inherent ambiguity of the photometric loss. Self-Supervised Objective Following the work of Zhou et al., we train the depth and pose network simultaneously in a self-supervised manner. In this work, however, we learn to recover the inverse depth $f_d: I \rightarrow f_D^{-1}(I)$ instead, along with the ego-motion estimator $f_x$. The overall self-supervised objective consists of an appearance matching loss term $L_p$ that is imposed between the synthesized image $\hat{I_t}$ and the target image $I_t$, and a depth regularization term $L_s$ that ensures edge-aware smoothing in the depth estimates $\hat{D_t}$. The objective takes the following form: \[L(I_t, \hat{I_t}) = L_p(I_t, I_s) \bigodot M_p \bigodot M_t + \lambdda_1 L_s(\hat{D_t})\] where $M_t$ is a binary mask that avoids computing the photometric loss on the pixels that do not have a valid mapping, and $\bigodot$ denotes element-wise multiplication. Additionally, $\lambda_1$ enforces a weighted depth regularization on the objective. The overall loss is averaged per-pixel, pyramid-scale and image batch during training. The proposed scale-aware self-supervised monocular structure-from-motion architecture. This paper introduces PackNet as a novel depth network, and optionally include weak velocity supervision at training time to produce scale-aware depth and pose models. Appearance Matching Loss The pixel-level similarity between the target image $I_t$ and the synthesized target image $\hat{I_t}$ is estimated using the Structural Similarity (SSIM) term combined with a L1 pixel-wise loss term, inducing an overall photometric loss given by the equation below. \[L_p(I_t, \hat{I_t}) = \alpha \frac{1-SSIM(I_t, \hat{I_t})}{2} + (1 - \alpha) \lVert I_t - \hat{I_t} \rVert\] While multi-view projective geometry provides strong cues for self-supervision, errors due to parallax in the scene have an undesirable effect incurred on the photometric loss. We mitigate these undesirable effects by calculating the minimum photometric loss per pixel for each source image in the context $I_S$, so that: \[L_p(I_t, I_S) = \min_{I_S} (I_t, \hat{I_t})\] The intuition is that the same pixel level will not be occluded or out-of-bounds in all context images, and that the association with minimal photometric loss should be the correct one. Furthermore, we also mask out static pixels by removing those which have a warped $L_p(I_t, \hat{I_t})$ higher than their corresponding unwarped photometric loss $L_p(I_t, I_s)$, calculated using the original source image without view synthesis. This auto-mask removes pixels whose appearance does not change between frames, which include static scenes and dynamic objects with no relative motion, since these will have smaller photometric loss when assuming no ego-motion. \[M_p = \min_{I_S} L_p(I_t, I_s) \geq L_p(I_t, \hat{I_t})\] Depth Smoothness Loss In order to regularize the depth in texture-less low-image gradient regions, we incorporate an edge-aware term. The loss is weighted for each of the pyramid-levels, and is decayed by a factor of 2 on down-sampling, starting with a weight of 1 for the 0th pyramid level. \[L_s(\hat{D_t}) = |\delta_x \hat{D_t}|e^{-|\delta_x I_t|} + |\delta_y \hat{D_t}| e^{|\delta_y I_t|}\] Scale-Aware SfM As previously mentioned, both the monocular depth and ego-motion estimators $f_d$ and $f_x$ predict scale-ambiguous values, due to the limitations of the monocular SfM training objective. In other words, the scene depth and the camera ego-motion can only be estimated up to an unknown and ambiguous scale factor. This is also reflected in the overall learning objective, where the photometric loss is agnostic to the metric depth of the scene. Furthermore, we note that all previous approaches which operate in the self-supervised monocular regime suffer from this limitation, and resort to artificially incorporating this scale factor at test-time, using LiDAR measurements. Velocity Supervision Loss PackNet: 3D Packing for Depth Estimation Standard convolutional architectures use aggresive striding and pooling to increase their receptive field size. However, this potentially decreases the model performance for tasks requiring fine-grained representations. Similarly, traditional upsampling strategies fail to propagate and preserve sufficient details at the decoder layers to recover accurate depth predictions. In contrast, we propose a novel encoder-decoder architecture, called PackNet, that introduces new 3D packing and unpacking blocks to leran to jointly preserve and recover important spatial information for depth estimation. This is in alignments with recent observations that information loss is not a necessary condition to learn representations capable of generalizing to different scenarios. In fact, progressive expansion and contraction in a fully invertible manner, without discarding uninformative input variability, has been shown to increase performance in a variety of tasks. We first describe the different blocks of our proposed architecture, and then proceed to show how they are integrated together in a single model for monocular depth estimation. Packing Block The packing block starts by folding the spatial dimensions of convolutional feature maps into extra feature channels via a Space2Depth operation. The resulting tensor is at a reduced resolution, but in contrast to striding or pooling, this transformation is invertible and comes at no loss. Next, we learn to compress this concatenated feature space in order to reduce its dimensionality to a desired number of output channels. As shown in experiments, 2D convolutions are not designed to directly leverage the tiled structure of this feature space. Instead, we propose to first learn to expand this structured representation via a 3D covolutional layer. The resulting higher dimensional feature space is then flattened (by simple reshaping) before a final 2D convolutional contraction layer. This structured feature expansion-contraction, inspired by invertible networks, although we do not ensure invertibility, allows our architecture to dedicate more parameters to learn how to compress key spatial details that need to be preserved for high resolution depth decoding. Unpacking Block Symmetrically, the unpacking block learns to decompress and unfold packed convolutional feature channels back into higher resolution spatial dimensions during the decoding process. The unpacking block replaces convolutional feature upsampling, typically performed via nearest-neighbor or with learnable transposed convolutional weights.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cnn.png" /><media:content medium="image" url="http://localhost:4000/assets/images/cnn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Batch Normalization and Group Normalization</title><link href="http://localhost:4000/group.html" rel="alternate" type="text/html" title="Batch Normalization and Group Normalization" /><published>2021-09-08T00:00:00+09:00</published><updated>2021-09-08T00:00:00+09:00</updated><id>http://localhost:4000/group</id><content type="html" xml:base="http://localhost:4000/group.html">&lt;!--more--&gt;

&lt;h2&gt; Batch Normalization: the Principles &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/bn2.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Batch Normalization is an algorithmic method which makes the training of Deep Neural Networks &lt;span class=&quot;blue&quot;&gt; faster and more stable &lt;/span&gt;. Batch normalization is computed differently during the training and the testing phase.&lt;/p&gt;

&lt;p&gt;At &lt;span class=&quot;circle-sketch-highlight&quot;&gt; training &lt;/span&gt;, the BN layer determines the mean and standard deviation of the activation values across the batch. It then &lt;span class=&quot;underline&quot;&gt; normalizes the activation vector &lt;/span&gt; with $\mu$ and $\sigma$. That way, each neuron’s output follows a standard &lt;span class=&quot;blue&quot;&gt; normal distribution &lt;/span&gt; across the &lt;span class=&quot;blue&quot;&gt; batch &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;It finally applies a &lt;span class=&quot;blue&quot;&gt; linear transformation &lt;/span&gt; with $\gamma$ and $\beta$ which are the two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trainable parameters&lt;/code&gt;. Such step allows the model to choose the optimum distribution for each hidden layer. $\gamma$ allows to adjust the standard deviation while $\beta$ allows to adjust the bias, shifting the curve on the right or on the left side.&lt;/p&gt;

&lt;p&gt;At each iteration, the network computes the mean $\mu$ and the standard deviation $\sigma$ corresponding to the current batch. Then it trains $\gamma$ and $\beta$ through &lt;mark&gt; gradient descent &lt;/mark&gt; using an &lt;mark&gt; Exponential Moving Average (EMA) &lt;/mark&gt; to give more importance to the latest iterations.&lt;/p&gt;

&lt;div class=&quot;sidenote&quot;&gt; We mostly use &lt;mark&gt; Exponential Moving Average algorithm &lt;/mark&gt; to reduce the noise or to smooth the data. The weight of each element decreases progressively over time, meaning &lt;span class=&quot;underline&quot;&gt; the EMA gives greater weight to recent data points &lt;/span&gt;. EMA reacts faster to changes compared to Simple Moving Average. &lt;/div&gt;

&lt;p&gt;At the &lt;span class=&quot;rainbow&quot;&gt; evaluation phase &lt;/span&gt;, we may not have a full batch to feed into the model. To tackle this issue, &lt;span class=&quot;underline&quot;&gt; we compute $\mu_{pop}$ and $\sigma_{pop}$ as the estimated mean and standard deviation of the studied population &lt;/span&gt;. Those values are computed using all the $\mu_{batch}$ and $\sigma_{batch}$ during training, and directly fed during the evaluation phase.&lt;/p&gt;

&lt;h2&gt; Why Normalization? &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/bn4.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;What we can conclude from the original Batch Normalization paper is that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Adding BN layers leads to &lt;b&gt; better convergence and higher accuracy &lt;/b&gt;&lt;/li&gt;
  &lt;li&gt;Adding BN layers allows us to &lt;b&gt; use higher learning rate &lt;/b&gt; without compromising convergence.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To quote Ian Goodfellow about the use of batch normalization:&lt;/p&gt;
&lt;blockquote&gt; Before BN, we thought it was almost &lt;span class=&quot;underline&quot;&gt; impossible to efficiently train deep models using sigmoid &lt;/span&gt; in the hidden layers. Batch Normalization makes those unstable networks trainable. &lt;/blockquote&gt;

&lt;p&gt;In practice, it is widely admitted that:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt; For &lt;span class=&quot;highlight-green&quot;&gt; CNNs, Batch Normalization is better &lt;/span&gt; &lt;/li&gt;&lt;li&gt; &lt;span class=&quot;highlight-yellow&quot;&gt; For Recurrent Networks, Layer Normalization is better &lt;/span&gt; &lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;While BN uses the current batch to normalize every single value, LN uses all the current layer to do so. &lt;span class=&quot;underline&quot;&gt; The normalization is performed using other features from a single example &lt;/span&gt; instead of using the same feature across all current batch examples.&lt;/p&gt;

&lt;p&gt;The best way to understand why BN works is to understand &lt;span class=&quot;highlight-sketch&quot;&gt; the optimization landscape smoothness &lt;/span&gt;. BN &lt;u&gt; reparameterizes the underlying optimization problem &lt;/u&gt;, making the training faster and easier. In additional recent studies, researchers observed that this effect is not unique to BN, but applies to other normalization methods (i.e. L1 normalization or L2 normalization).&lt;/p&gt;

&lt;h2&gt; The Drawbacks of BN &lt;/h2&gt;

&lt;p&gt;For BN to work, the &lt;span class=&quot;rainbow&quot;&gt; batch size is required to be sufficiently large &lt;/span&gt;, usually at least &lt;span class=&quot;circle-sketch-highlight&quot;&gt; 32 &lt;/span&gt;. However, there are situations when we have to settle for a small batch size. For example, when each &lt;span class=&quot;underline&quot;&gt; data sample is highly memory consuming &lt;/span&gt; or when we train a &lt;span class=&quot;underline&quot;&gt; very large neural network &lt;/span&gt; which leaves little GPU memory for processing data. For computer vision applications other than image classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes.&lt;/p&gt;

&lt;h2&gt; Comparisions of Normalization Methods &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/bn3.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Group Normalization is one of the latest normalization methods that &lt;u&gt; avoids exploiting the batch dimension&lt;/u&gt;, thus is &lt;u&gt;independent of batch size&lt;/u&gt;. But there are other normalization methods as well.&lt;/p&gt;

&lt;h3&gt; Layer Normalization &lt;/h3&gt;

&lt;p&gt;Layer Normalization computes $\mu_i$ and $\sigma_i$ along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(C,H,W)&lt;/code&gt; axes. The computation for an input feature is entirely independent of other input features in a batch.&lt;/p&gt;

&lt;h3&gt; Instance Normalization &lt;/h3&gt;

&lt;p&gt;Instance Normalization computes $\mu_i$ and $\sigma_i$ along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(H,W)&lt;/code&gt; axes. Since the computation of IN is the same as that of BN with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size=1&lt;/code&gt;, IN actually makes the situation worse in most cases. However, for &lt;span class=&quot;highlight-pink&quot;&gt; style transfer tasks &lt;/span&gt;, IN is better at discarding contrast information of an image, thus having superior performance than BN.&lt;/p&gt;

&lt;h3&gt; Group Normalization &lt;/h3&gt;

&lt;p&gt;Also notice that IN can be viewed as applying Layer Normalization to each channel individually as if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_channels = 1&lt;/code&gt;. Group Normalization is the middle ground between IN and LN. It &lt;span class=&quot;gif&quot;&gt; organizes the channels into different groups &lt;/span&gt; and &lt;span class=&quot;highlight-yellow&quot;&gt; computes $\mu_i$ and $\sigma_i$ along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(H,W)&lt;/code&gt; axes and along a group of channels.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;First, the batch with dimension &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(N,C,H,W)&lt;/code&gt; is reshaped to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(N,G,C//G,H,W)&lt;/code&gt;. The number of group &lt;span class=&quot;underline&quot;&gt; $G$ is a pre-defined hyperparameter &lt;/span&gt;. Then we normalize along the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(C//G,H,W)&lt;/code&gt; dimension and return the result after reshaping the batch back to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(N,C,H,w)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Group Normalization is better than Layer Normalization as GN allows different distribution to be learned for each group of channels. GN is also thought to be better than IN because GN can exploit the dependence across channels. If &lt;mark&gt;`C = G`&lt;/mark&gt;, that is, if the number of groups are set to be equal to the number of channels, &lt;mark&gt; GN becomes IN &lt;/mark&gt; . Likewise, if &lt;mark&gt;`G = 1`&lt;/mark&gt; &lt;mark&gt; GN becomes LN &lt;/mark&gt;.&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://amaarora.github.io/2020/08/09/groupnorm.html&quot;&gt; blog post &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/what-is-group-normalization-45fe27307be7&quot;&gt; medium article &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338&quot;&gt; medium article2 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="deep learning" /><category term="featured" /><summary type="html">Batch normalization is used in most state-of-the-art computer vision techniques to stabilize training, but it also suffers from drawbacks. Group normalization can be an awesome alternative when the batch size is too small.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/bn.png" /><media:content medium="image" url="http://localhost:4000/assets/images/bn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">An Intuitive Overview of Linear Algebra Fundamentals</title><link href="http://localhost:4000/linear.html" rel="alternate" type="text/html" title="An Intuitive Overview of Linear Algebra Fundamentals" /><published>2021-09-08T00:00:00+09:00</published><updated>2021-09-08T00:00:00+09:00</updated><id>http://localhost:4000/linear</id><content type="html" xml:base="http://localhost:4000/linear.html">&lt;h2&gt; Introduction &lt;/h2&gt;
&lt;h3&gt; Why Learn Linear Algebra? &lt;/h3&gt;
&lt;p&gt;This write up is an overview of some of the linear algebra fundamentals. It focuses on providing an intuitive/geometric review of some of the main concepts. The coverage is by no means comprehensive or complete. It’s meant to serve as review material for those who haven’t touched it in a very long time, or as a brief intro for those who want an over-the-top view of the landscape.&lt;/p&gt;

&lt;blockquote&gt;Linear algebra is the study of vectors and linear functions &lt;/blockquote&gt;

&lt;p&gt;Functions are computations/transformations taht take a set of inputs and produce an output. Functions of several variables are often presented in one line such as:&lt;/p&gt;

\[f(x,y) = 3x + 5y\]

&lt;p&gt;In mathematics, the concept of linearity plays a very important role. Mathematically, a linear function, or linear map, or linear operator, $f$ is a function that satisfies:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$f(x,y) = f(x) + f(y)$ for any input $x$ and $y$&lt;/li&gt;
  &lt;li&gt;$f(cx) = cf(x)$ for any input $x$ and any scalar $c$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Put into words, the first condition means that the output of a function acting on a sum of inputs is just equal to the sum of the individual outputs. The second implies that the output of a scaled input is just the scaled output of the original input.&lt;/p&gt;

&lt;p&gt;Linear functions are those whose graph is a straight line. A linear function has the following form:&lt;/p&gt;

\[y = f(x) = a+bx\]

&lt;p&gt;Here, $x$ is our independent variable and $y$ is our dependent variable (since the result $y$ depends on the transformation we perform on $x$).&lt;/p&gt;

&lt;p&gt;Notice that although we classify one variable as being dependent on the other, in reality, both variables are linked and dependent on each other. We simply choose to classify the variable which takes an input as the ‘independent’ one. In our example above, we can just as easily choose to exchange our variables and make $y$ the independent variable and $x$ the dependent one.&lt;/p&gt;

&lt;p&gt;Let’s take $y = f(x) = 3x+1$ as an example. We can choose to model the above function in terms of $y$ by taking the inverse, so we get:
\(y = 3x+1
y-1 = 3x
x = f(y) = y/3 - 1/3\)&lt;/p&gt;

&lt;p&gt;In either case, you get the point. Our linear function can be viewed as representing a linear relationship between two or more variables. The relationship between these variables are the relationships we model in linear algebra.&lt;/p&gt;

&lt;p&gt;To make things easier to grasp, we usually represent our variable relationship visually by projecting input variable $x$ on a horizontal axis (which we call the $x$-axis) and showing its mapping to variable $y$ on a vertical axis (which we call the $y$-axis). This lets us get a visual intuition of how our function works in transforming inputs to outputs, and lets us model our linear transformations geometrically.&lt;/p&gt;

&lt;p&gt;Some graphical examples of linear functions are provided below:&lt;/p&gt;
&lt;picture&gt;
&lt;img src=&quot;/assets/images/linear.png&quot; /&gt;
&lt;/picture&gt;

&lt;p&gt;Of course, linear functions aren’t the only types of functions. Non-linear functions don’t have a linear mapping and are not represented by straight lines. Some examples of non-linear functions are provided below:&lt;/p&gt;
&lt;picture&gt;
&lt;img src=&quot;/assets/images/non.png&quot; /&gt;
&lt;/picture&gt;

&lt;h2&gt; Vectors &lt;/h2&gt;
&lt;p&gt;The key starting point of linear algebra is the concept of vectors. In high school physics, chances are you’ve already seen that concept as being nothing more than “a number and a direction”. This isn’t false, but it’s definitely not the whole story. It’s rather intuitive way of introducing vectors, hence why we’ll use this analogy extensively.&lt;/p&gt;

&lt;p&gt;From a mathematical perspective, vectors are just a way of stacking numbers together in a column or a row. For example, 
\(\begin{bmatrix} 2 \\ 3 \end{bmatrix}
\begin{bmatrix} i \\ 3 \\ -3 \end{bmatrix}
\begin{bmatrix} 5 &amp;amp; 2.1 \end{bmatrix}
\begin{bmatrix} -3 &amp;amp; \frac{1}{2} &amp;amp; 4 \end{bmatrix}\)&lt;/p&gt;

&lt;p&gt;The first two vector examples are naturally referred to as column vectors and the last two as row vectors.&lt;/p&gt;

&lt;p&gt;The amount of numbers is referred to as the dimension of the vector. Even if you’ve never explicitly learned about vectors until now, you’ve already seen them. A 2-dimensional vector of real numbers is analogous to the Cartesian coordinates. For example, a 2-dimensional vector looks like:&lt;/p&gt;
&lt;picture&gt;
&lt;img src=&quot;/assets/images/2dim.png&quot; /&gt;
&lt;/picture&gt;

&lt;p&gt;A 3-dimensional vector of real numbers is analogous to the spatial coordinates in three dimensions. You can always think of a vector with $n$ numbers as a point in $n$-dimensional “hyperspace”.&lt;/p&gt;

&lt;p&gt;For simplicity, we’ll generally use 2 and 3 dimensional vectors from now on, but everything we explain below applies to vectors of arbitrary sizes.&lt;/p&gt;

&lt;h3&gt; Vector Addition &lt;/h3&gt;
&lt;p&gt;Since we just defined a new mathematical concept, it’s natural to ask the question: can we add and multiply vectors?&lt;/p&gt;</content><author><name>seri</name></author><category term="linear algebra" /><category term="featured" /><summary type="html">Introduction Why Learn Linear Algebra? This write up is an overview of some of the linear algebra fundamentals. It focuses on providing an intuitive/geometric review of some of the main concepts. The coverage is by no means comprehensive or complete. It’s meant to serve as review material for those who haven’t touched it in a very long time, or as a brief intro for those who want an over-the-top view of the landscape. Linear algebra is the study of vectors and linear functions Functions are computations/transformations taht take a set of inputs and produce an output. Functions of several variables are often presented in one line such as: \[f(x,y) = 3x + 5y\] In mathematics, the concept of linearity plays a very important role. Mathematically, a linear function, or linear map, or linear operator, $f$ is a function that satisfies: $f(x,y) = f(x) + f(y)$ for any input $x$ and $y$ $f(cx) = cf(x)$ for any input $x$ and any scalar $c$ Put into words, the first condition means that the output of a function acting on a sum of inputs is just equal to the sum of the individual outputs. The second implies that the output of a scaled input is just the scaled output of the original input. Linear functions are those whose graph is a straight line. A linear function has the following form: \[y = f(x) = a+bx\] Here, $x$ is our independent variable and $y$ is our dependent variable (since the result $y$ depends on the transformation we perform on $x$). Notice that although we classify one variable as being dependent on the other, in reality, both variables are linked and dependent on each other. We simply choose to classify the variable which takes an input as the ‘independent’ one. In our example above, we can just as easily choose to exchange our variables and make $y$ the independent variable and $x$ the dependent one. Let’s take $y = f(x) = 3x+1$ as an example. We can choose to model the above function in terms of $y$ by taking the inverse, so we get: \(y = 3x+1 y-1 = 3x x = f(y) = y/3 - 1/3\) In either case, you get the point. Our linear function can be viewed as representing a linear relationship between two or more variables. The relationship between these variables are the relationships we model in linear algebra. To make things easier to grasp, we usually represent our variable relationship visually by projecting input variable $x$ on a horizontal axis (which we call the $x$-axis) and showing its mapping to variable $y$ on a vertical axis (which we call the $y$-axis). This lets us get a visual intuition of how our function works in transforming inputs to outputs, and lets us model our linear transformations geometrically. Some graphical examples of linear functions are provided below: Of course, linear functions aren’t the only types of functions. Non-linear functions don’t have a linear mapping and are not represented by straight lines. Some examples of non-linear functions are provided below: Vectors The key starting point of linear algebra is the concept of vectors. In high school physics, chances are you’ve already seen that concept as being nothing more than “a number and a direction”. This isn’t false, but it’s definitely not the whole story. It’s rather intuitive way of introducing vectors, hence why we’ll use this analogy extensively. From a mathematical perspective, vectors are just a way of stacking numbers together in a column or a row. For example, \(\begin{bmatrix} 2 \\ 3 \end{bmatrix} \begin{bmatrix} i \\ 3 \\ -3 \end{bmatrix} \begin{bmatrix} 5 &amp;amp; 2.1 \end{bmatrix} \begin{bmatrix} -3 &amp;amp; \frac{1}{2} &amp;amp; 4 \end{bmatrix}\) The first two vector examples are naturally referred to as column vectors and the last two as row vectors. The amount of numbers is referred to as the dimension of the vector. Even if you’ve never explicitly learned about vectors until now, you’ve already seen them. A 2-dimensional vector of real numbers is analogous to the Cartesian coordinates. For example, a 2-dimensional vector looks like: A 3-dimensional vector of real numbers is analogous to the spatial coordinates in three dimensions. You can always think of a vector with $n$ numbers as a point in $n$-dimensional “hyperspace”. For simplicity, we’ll generally use 2 and 3 dimensional vectors from now on, but everything we explain below applies to vectors of arbitrary sizes. Vector Addition Since we just defined a new mathematical concept, it’s natural to ask the question: can we add and multiply vectors?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/linear.png" /><media:content medium="image" url="http://localhost:4000/assets/images/linear.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>