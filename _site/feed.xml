<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-09-20T06:01:59+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Seri Lee Blog</title><subtitle>This is where I write posts about my research field.</subtitle><entry><title type="html">Sparse Convolutional Networks</title><link href="http://localhost:4000/sparse-cnn.html" rel="alternate" type="text/html" title="Sparse Convolutional Networks" /><published>2021-09-17T00:00:00+09:00</published><updated>2021-09-17T00:00:00+09:00</updated><id>http://localhost:4000/sparse-cnn</id><content type="html" xml:base="http://localhost:4000/sparse-cnn.html">&lt;!--more--&gt;

&lt;h2&gt; Introduction &lt;/h2&gt;
&lt;p&gt;Convolutional Neural Network (CNN) has been proved very effective for 2D image signal processing. However, for 3D point cloud signals, the extra dimension $Z$ increases the calculation significantly. On the other hand, unlike regular images, most of the voxels of the 3D point cloud are empty, which makes point cloud data in the 3D voxels often a sparse signals. The question is whether we can only calculate the convolution with the sparse data efficiently instead of scanning all the image pixels or spatial voxels.&lt;/p&gt;

&lt;p&gt;One intuitive thinking is, regular image signals are stored as matrix or tensor. The corresponding convolution is calculated as a dense matrix multiplication. The sparse signals are normally represented as data lists and index lists. We could develop a sparse convolution schema that uses the advantage of sparse signal representation.&lt;/p&gt;

&lt;h2&gt; Sparse Convolution Model &lt;/h2&gt;
&lt;p&gt;In a short&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&amp;lt;a=href=&quot;https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1&quot;&amp;gt; medium article &amp;lt;/a&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Neural Networks: Back to the Basics</title><link href="http://localhost:4000/know-nn.html" rel="alternate" type="text/html" title="Neural Networks: Back to the Basics" /><published>2021-09-17T00:00:00+09:00</published><updated>2021-09-17T00:00:00+09:00</updated><id>http://localhost:4000/know-nn</id><content type="html" xml:base="http://localhost:4000/know-nn.html">&lt;!--more--&gt;
&lt;h2&gt; Introduction &lt;/h2&gt;
&lt;h2&gt; Perceptron &lt;/h2&gt;
&lt;p&gt;What is a neural network? To get started, I’ll explain a type of artificial neuron called a &lt;span class=&quot;blue&quot;&gt; perceptron &lt;/span&gt;. Today, it’s more common to use other models of artificial neurons. The main neural model used is called the &lt;span class=&quot;red&quot;&gt; sigmoid neuron &lt;/span&gt;. We will get to sigmoid neurons shortly. But to understand why sigmoid neurons are defined the way they are, it’s worth taking the time to first understand perceptrons.&lt;/p&gt;

&lt;p&gt;So who do perceptrons work? A perceptron takes several binary inputs $x_1, x_2, \dots $ and produces a single binary output:&lt;/p&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/svg/perceptron.svg&quot; class=&quot;img-small-center&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;In the example shown, the perceptron has three inputs $x_1, x_2, x_3$. In general, it could have more or fewer inputs. Rosenblatt, the inventor of perceptron, introduced &lt;span class=&quot;highlight&quot;&gt; weights &lt;/span&gt;, $w_1, w_2, \dots$, real numbers expressing the importance of the respective inputs to the outputs.&lt;/p&gt;

&lt;p&gt;The neuron’s output, 0 or 1, is determined by whether the weighted sum $\sum_{j}w_j x_j$ is less or greater than some &lt;span class=&quot;shadow-blue&quot;&gt; threshold value &lt;/span&gt;. Just like the weights, the threshold is a real number which is a parameter of the neuron.&lt;/p&gt;

&lt;p&gt;To put it in more precise algebraic terms:&lt;/p&gt;

\[\text{output} = \begin{cases} 0 &amp;amp; \text{if $\sum_j w_j x_j \leq threshold$} \\ 1 &amp;amp; \text{if $\sum_j w_j x_j &amp;gt; threshold$}\end{cases}\]

&lt;p&gt;That’s all there is to how a perceptron works!&lt;/p&gt;

&lt;div class=&quot;textbox&quot;&gt; A way you can think about the perceptron is that it's a device that &lt;u&gt; makes decisions &lt;/u&gt; by &lt;u&gt; weighing up evidence &lt;/u&gt;. &lt;/div&gt;

&lt;p&gt;By varying the weights and the threshold, we can get different models of decision-making.&lt;/p&gt;

&lt;p&gt;&lt;small&gt; Obviously, the perceptron isn’t a complete model of human decision-making. &lt;/small&gt; But what the example illustrates is &lt;u&gt; how a perceptron can weigh up different kinds of evidence &lt;/u&gt; in order to make decisions.&lt;/p&gt;

&lt;p&gt;So it should seem plausible that &lt;span class=&quot;highlight-yellow&quot;&gt; a complex network of perceptrons could make quite subtle decisions &lt;/span&gt;.&lt;/p&gt;

&lt;picture&gt;&lt;img class=&quot;img-small-center&quot; src=&quot;/assets/images/mlp.png&quot; /&gt; &lt;/picture&gt;

&lt;p&gt;In this network, the first layer of perceptrons is making three very simple decisions by weighing the input evidence. The perceptrons in the second layer is making a decision by &lt;u&gt; weighing up the results from the first layer of decision-making &lt;/u&gt;. In this way, a perceptron in the second layer &lt;span highlight=&quot;highlight-skew&quot;&gt; can make a decision at a more complex and abstract level &lt;/span&gt; than perceptrons in the first layer.&lt;/p&gt;

&lt;p&gt;Even more complex decisions can be made by the perceptron in the third layer.  In this way, &lt;span class=&quot;underline-pink&quot;&gt; many layers of perceptrons can engage in sophisticated decision making. &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Let’s simplify the way we describe perceptrons. The condition $\sum_j w_j x_j &amp;gt; \text{threshold}$ is cumbersome, so we can make a change to simplify it. We will move the threshold to the other side of the inequality, and replace it by what is known as the perceptron’s &lt;span class=&quot;highlight-sketch&quot;&gt; bias &lt;/span&gt;, $ b \equiv - \text{threshold} $. Using this bias instead of the threshold, the perceptron rule can be written as:&lt;/p&gt;

\[\text{output} = \begin{cases} 0 &amp;amp; \text{if $w \cdot x + b \leq 0$} \\ 1 &amp;amp; \text{ if $w \cdot x + b &amp;gt; 0$} \end{cases}\]

&lt;p&gt;You can think of the bias as &lt;span class=&quot;underline-move&quot;&gt; as a measure of how easy it is to get the perceptron to output a $1$ &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;For a perceptron with &lt;u&gt; a really big bias &lt;/u&gt;, it’s extremely easy for the perceptron to &lt;b&gt; output a $1$ &lt;/b&gt;. But if the bias is negative, it’s difficult for the perceptron to output a $1$.&lt;/p&gt;

&lt;p&gt;I’ve described perceptrons as &lt;span class=&quot;highlight-gradient&quot;&gt; a method for weighing evidence to make decisions &lt;/span&gt;. Another way perceptrons can be used is &lt;span class=&quot;underline&quot;&gt; to compute the elementary logcal functions &lt;/span&gt; we usually think of as underlying computation, functions such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AND&lt;/code&gt;,&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OR&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NAND&lt;/code&gt;.&lt;/p&gt;

&lt;picture&gt;&lt;img class=&quot;img-small-center&quot; src=&quot;/assets/svg/example.svg&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;For example, suppose we have a perceptron with weight $-2$, and an overall bias of $3$. We can see that input $00$ produces the value $3$, so the output is $1$.&lt;/p&gt;

&lt;p&gt;Similar calculations show that $01$ or $10$ produces output $1$. But the input $11$ produces the value -1, hence the output $0$. So our perceptron is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NAND&lt;/code&gt; gate!&lt;/p&gt;

&lt;p&gt;In fact, we can &lt;span class=&quot;neon-green&quot;&gt; use networks of perceptrons to compute any logical function at all &lt;/span&gt;.&lt;/p&gt;

&lt;div class=&quot;textbox&quot;&gt; The reason is that the `NAND` gate is universal for computation. We can build any computation up out of `NAND` gates. &lt;/div&gt;

&lt;p&gt;Becaue &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NAND&lt;/code&gt; gates are universal for computation, perceptrons are also &lt;span class=&quot;gold&quot;&gt; universal for computation &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;It turns out that we can device &lt;span class=&quot;neon-pink&quot;&gt; learning algorithms &lt;/span&gt; which can &lt;u&gt; automatically tune the weights and biases of a network &lt;/u&gt; of artificial neurons.&lt;/p&gt;

&lt;p&gt;&lt;small&gt; This tuning happens in responsce to external stimuli, without direct intervention by a programmer. These learning algorithms enable us to use artifical neurons in a way which is radically different to conventional logic gates. &lt;/small&gt;&lt;/p&gt;

&lt;h2&gt; Sigmoid Neurons &lt;/h2&gt;

&lt;p&gt;How can we device such &lt;span class=&quot;neon-pink&quot;&gt; learning algorithms &lt;/span&gt; for a neural network?&lt;/p&gt;

&lt;p&gt;Well, what we’d like to see is for a small change in weight (or bias) to cause only a small corresponding change in the output from the network. This property makes learning possible.&lt;/p&gt;

\[w + \nabla w \rightarrow \text{output} + \nabla \text{output}\]

&lt;p&gt;The problem is that a small change in weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say 0 to 1.&lt;/p&gt;

&lt;p&gt;We can overcome this problem by introducing a new type of artificial neuron called a &lt;span class=&quot;shine&quot;&gt; sigmoid neuron &lt;/span&gt;. Sigmoid neurons are similar to perceptrons, but modified so that &lt;u&gt; small changes in their weights and biases casue only a small change in their output.&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;We can depict the sigmoid neurons in the same way we depicted perceptrons. Just like a perceptron, the sigmoid neuron has inputs $x_1, x_2, \dots$, but instead of being just $0$ or $1$, these inputs can take on any values between $0$ and $1$.&lt;/p&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/sigmoid.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;Just like a perceptron, the sigmoid neuron has weights for each input $w_1, w_2, \dots$ and an overall bias $b$. But the output is not $0$ or $1$ but instead $\sigma(w \cdot x + b)$, where $\sigma$ is called the &lt;span class=&quot;flow&quot;&gt; sigmoid function &lt;/span&gt; which is defined by:&lt;/p&gt;

\[\sigma(z) \equiv \frac{1}{1+e^{-z}}\]

&lt;p&gt;To put it more explicitly,&lt;/p&gt;

\[\frac{1}{1+ \exp(-\sum_j w_j x_j - b)}\]

&lt;p&gt;To understand the similarity to the perceptron model, suppose $z \equiv w \cdot x + b$ is a large positive number. Then $e^{-z} \approx 0$ and so $\sigma(z) \approx 1$.&lt;/p&gt;

&lt;p&gt;In other words, neuron is approximately $1$, if $z = w \cdot x + b$ is large and positive.  If $z = w \cdot + b$ is negative, $e^{-z} \rightarrow \infty$ and $\sigma(z) \approx 0$.&lt;/p&gt;

&lt;div class=&quot;textbox&quot; style=&quot;text-align: center !important;&quot;&gt; &lt;picture&gt;&lt;img src=&quot;/assets/images/euler.png&quot; style=&quot;width: 350px !important;&quot; /&gt;&lt;/picture&gt; &lt;p style=&quot;font-size: 70% !important; text-align: left !important;&quot;&gt; Remember our exponential function with Euler's number? $e = \lim_{n\to\infty} (1+1/n)^n = 1+ \frac{1}{1!}+ \frac{1}{2!} + \frac{1}{3!}+ \cdots \approx 2.718$. The gradient function of $y = e^x$ is equal to $e^x$. It is a natural choice as a logarithmic base.&lt;/p&gt; &lt;/div&gt;

&lt;p&gt;The exact algebraic form of $\sigma$ isn’t so important-what really matters is the shape of the function when plotted as in the above figure.&lt;/p&gt;

&lt;div class=&quot;textbox&quot; style=&quot;text-align: center !important;&quot;&gt;&lt;picture&gt;&lt;img src=&quot;/assets/images/sigmoid.png&quot; style=&quot;width:500px !important;&quot; /&gt;&lt;/picture&gt;&lt;p style=&quot;font-size: 90% !important; text-align: left !important;&quot;&gt;The left figure also show the graph of the derivative of sigmoid function in pink color. The sigmoid function is also called a squashing function as its domain is the set of all real numbers ranging $(0,1)$. When the activation function for a neuron is a sigmoid function, it is guaranteed that the output unit will always be  between 0 and 1.&lt;/p&gt; &lt;/div&gt;

&lt;div class=&quot;textbox&quot;&gt;&lt;p&gt;&lt;b&gt; Why Is the Sigmoid Function Important in Neural Networks?&lt;/b&gt; The sigmoid is a non-linear function, thus the ouput of this unit would be a non-linear function of the weighted sum of inputs. For a linearly separable problem in an $n$-dimensional space, the linear decision boundary is described by the equation of a hyperplane. The right figure shows a non-linearly separable problem, where a non-linear decision boundary is required. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;If we use a linear activation function in a neural network, then this model can only leran linearly separable problems. The addition of one hidden layer and a sigmoid activation function in the hidden layer, the neural network can eaily learn a non-linearly separable problem.&lt;/p&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/sigmoid-neuron.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;&lt;span class=&quot;underline-grad&quot;&gt; The only non-linear function that can be used as an activation function is one which is monotonically increasing. &lt;/span&gt; The function also requires to be &lt;span class=&quot;shine&quot;&gt; continuous &lt;/span&gt; and &lt;span class=&quot;reveal&quot;&gt; differentiable &lt;/span&gt; over the entire space of real numbers.&lt;/p&gt;

&lt;p&gt;The fact that sigmoid function is monotonic, continuous and differentiable everywhere, coupled with the property that its derivatives can be expressed in terms of itself, makes the learning process easier when using the back-propagation algorithm.&lt;/p&gt;

&lt;p&gt;Indeed, its the smoothness of the $\sigma$ function that is the crucial fact. The smoothness of $\sigma$ means that small changes $\nabla w_j$ in the weights and $\nabla b$ in the bias will produce a small change of $\nabla \text{output}$ from the neuron.&lt;/p&gt;

&lt;h2&gt; The architecture of Neural Networks &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/nn4.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;The following four-layer network has two hidden layers. Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called &lt;span class=&quot;rainbow&quot;&gt; multi-layer perceptrons &lt;/span&gt; or &lt;b&gt; MLPs &lt;/b&gt; despite being made up of sigmoid neurons, not perceptrons.&lt;/p&gt;

&lt;p&gt;While the design of the input and output layers of a neural network is often straighforward, there can be quite an art to &lt;u&gt; the design of the hidden layers &lt;/u&gt;. Researchers have developed many design heuristics for the hidden layers. Up to now, we’ve discussing neural networks where &lt;span class=&quot;underline-fancy&quot;&gt; the output from one layer is used as input to the next layer &lt;/span&gt;. Such netwokrs are called &lt;span class=&quot;blue&quot;&gt; feedforward neural networks&lt;/span&gt;. This means that &lt;span class=&quot;underline-fancy&quot;&gt; there are no loops in the network &lt;/span&gt;-&lt;span class=&quot;underline-move&quot;&gt; information is always fed forward, never fed back &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;small&gt; There are other models of artificial neural networks in which feedback loops are possible. These models are called &lt;mark class=&quot;gold&quot;&gt; recurrent neural networks &lt;/mark&gt;.&lt;/small&gt;&lt;/p&gt;
&lt;h2&gt; A Simple Network for Digit Recognition &lt;/h2&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/digit.png&quot; /&gt;&lt;/picture&gt;

&lt;p&gt;We are going to use a three-layer neural network to recognize individual digits.&lt;/p&gt;

&lt;p&gt;The input layer of the network contains neurons encoding the values of the input pixels. As discussed in the next section, our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains $784 = 28 \times 28$ neurons. For simplicity I’ve omitted most of the 784 input neurons in the diagram above. The input pixels are grayscale, with a value of $0.0$ representing white, a value of $1.0$ representing black, and in between values representing gradually darkening shades of gray.&lt;/p&gt;

&lt;p&gt;The second layer of the network is a hidden layer. We denote the number of neurons in the hidden layer by $n$, and we’ll experiment with different values for $n$. The example above illustrates a small hidden layer, containing just $n=15$ neurons.&lt;/p&gt;

&lt;p&gt;The output layer of the network contains $10$ neurons. If the first neuron fires, i.e. has an $\text{output} \approx 1$, then that will indicate that the network thinks the digit is a $0$. We number the output neurons from $0$ through $9$ and figure out which neuron has the highest activation value.&lt;/p&gt;

&lt;p&gt;You might wonder why we use $10$ output neurons. A seemingly natural way of doing that is to use just $4$ output neurons, treating each neuron as taking on a binary value. Why should our network use $10$ neurons instead? The ultimate justification is empirical: we can try out both network designs, and it turns out that, for this particular problem, &lt;span class=&quot;highlight-green&quot;&gt; the network with 10 output neurons learns to recognize digits better &lt;/span&gt; than the network with $4$ output neurons.&lt;/p&gt;

&lt;p&gt;We can give a plausible explanation for why it’s better to have $10$ outputs from the network, rather than $4$ because if we had 4 outputs, the first output neuron would be trying to decide what the most significant bit of the digit was. And there’s no easy way to relate that most significant bit to simple shapes.&lt;/p&gt;

&lt;h2&gt; Learning with Gradient Descent &lt;/h2&gt;

&lt;p&gt;The first thing we’ll need is a dataset to learn from. The MNIST data, which we are going to use, comes in two parts. The first part contains 60,000 images to be used as training data. The images are grayscale and $28 \times 28$ images.&lt;/p&gt;

&lt;p&gt;We’ll use the test data to evaluate how well our neural network has learned to recognize digits.&lt;/p&gt;

&lt;p&gt;We’ll use the notation $x$ to denote a training input. It’ll be convenient to regard each training input $x$ as a $28 \times 28 = 784$ -dimensional vector. Each entry in the vector represents the gray value for a single pixel in the image. We’ll denote the corresponding desired output by $y(x)$, where $y$ is a 10-dimensional vector.&lt;/p&gt;

&lt;h2&gt; Backpropagation Algorithm &lt;/h2&gt;

&lt;h3&gt; The Cost Function &lt;/h3&gt;

&lt;p&gt;The goal of back-propagation is &lt;mark class=&quot;gold&quot;&gt; to compute the partial derivatives $\frac{\partial C}{\partial w}$ and $\frac{\partial C}{\partial b}$ of the cost function $C$ with respect to any weight $w$ or bias $b$ in the network &lt;/mark&gt;.&lt;/p&gt;

&lt;p&gt;For back-propagation to work we need to make &lt;span class=&quot;three&quot;&gt; two main assumptions &lt;/span&gt; about the form of the cost function.&lt;/p&gt;

&lt;p&gt;It’s useful to have a cost function in mind, so we’ll set a quadratic cost function which has the form ($n$ is the number of training examples; $x$ is the sum over individual training examples; $y(x)$ is the corresponding desired output; $L$ denotes the number of layer in the network; $a^L(x)$ is the vector of activations output from the network when $x$ is the input).&lt;/p&gt;

\[C = \frac{1}{2n} \sum_x \lVert y(x) - a^L(x) \rVert^2\]

&lt;p&gt;The first assumption about the cost function $C$ is that the cost function can be written as &lt;span class=&quot;highlight-gradient&quot;&gt; an average $C = \frac{1}{n} \sum_x C_x$ over cost function $C_x$ for individual training examples $x$ &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The reason we need this assumption is because what back-propagation actually lets us do is &lt;mark class=&quot;coral&quot;&gt; is compute the partial derivatives $\partial C_x / \partial w$ and $\partial C_x / \partial b$ for a single training example. We then recover $\partial C / \partial w$ and $\partial C / \partial b$ by averaing over training examples.&lt;/mark&gt;&lt;/p&gt;

&lt;p&gt;For example, for the quadratic cost function’s cost for a single training example
The second assumption we make about the cost is that it can be written as a function of the outputs from the neural network:&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt; online book &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="deep learning" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/neuron.png" /><media:content medium="image" url="http://localhost:4000/assets/images/neuron.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Guide to Linear Algebra (Part 2)</title><link href="http://localhost:4000/bullshit-two.html" rel="alternate" type="text/html" title="Guide to Linear Algebra (Part 2)" /><published>2021-09-15T00:00:00+09:00</published><updated>2021-09-15T00:00:00+09:00</updated><id>http://localhost:4000/bullshit-two</id><content type="html" xml:base="http://localhost:4000/bullshit-two.html">&lt;!--more--&gt;

&lt;h2&gt; Hilbert Space &lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;blue&quot;&gt; The Euclidean space we consider in everyday life is mathematically called the Hilbert Space. &lt;/span&gt; Any general metric graph is a Hilbert Space. Any $n$ dimensional Hilbert space is sand to be in $\mathbb{R}^n$ which indicates that all the numbers represented in this space are real.&lt;/p&gt;

&lt;p&gt;Note that &lt;span class=&quot;red&quot;&gt; Cartesian space is different from Hilbert space &lt;/span&gt; and is a subset of it. Mathematically, a Hilbert space is an abstract vector space which preserves the structure of an inner product. In non-math terms, it just means that &lt;u&gt; one can measure length and angle in this space &lt;/u&gt;.&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&amp;lt;a=href=&quot;&quot;&amp;gt; TheAILearner &amp;lt;/a&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="linear algebra" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/linear.png" /><media:content medium="image" url="http://localhost:4000/assets/images/linear.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">All About Training GAN</title><link href="http://localhost:4000/gan.html" rel="alternate" type="text/html" title="All About Training GAN" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/gan</id><content type="html" xml:base="http://localhost:4000/gan.html">&lt;!--more--&gt;

&lt;h2&gt; 1.  Generative Adversarial Networks &lt;/h2&gt;

&lt;p&gt;Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. The discriminator should not be able to distinguish between real and fake images.&lt;/p&gt;

&lt;p&gt;Another way to look at the GAN setup is that the discriminator is trying to guide the generator by telling what real images look like. The two networks try to achieve what is called the Nash Equilibrium with respect to each other.&lt;/p&gt;

&lt;h2&gt; 2.  Training GANs &lt;/h2&gt;

&lt;p&gt;GAN networks are a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. There are no good objective metrics for evaluating whether a GAN is performing well during training, e.g. reviewing the loss is not sufficient. Instead the best approach is to visually inspect the generated examples and use subjective evaluation.&lt;/p&gt;

&lt;p&gt;Other quantitative measures, such as Inception Score (IS) or Frechet Inception Distance (FID) rely on pretrained models with a specific set of object classes. They lack an upper bound (which means hypothetically the highest possible score is infinity).&lt;/p&gt;

&lt;h3&gt; 2.1  Look at the Loss &lt;/h3&gt;

&lt;p&gt;In a discriminative model,the loss measures the accuracy of the prediction and we use it to monitor the progress of training. However, the loss in GAN measures how well we are doing compared with our opponent. Often, the generator cost increases but the image quality is actually improving.&lt;/p&gt;

&lt;p&gt;If you see the discriminator loss rapidly approaching, there is probably no chance of recovery and it is time to change something.&lt;/p&gt;

&lt;h3&gt; 2.1  Look at the Gradients &lt;/h3&gt;
&lt;p&gt;Monitor the gradients along with the losses in the networks. These can give you a good idea about the progress of training and can even help in debugging if things are not really working well.&lt;/p&gt;

&lt;p&gt;Ideally, the generator should receive large gradients early in the training because it needs to learn how to generate real-looking data. The discriminator on the other hand does not always get large gradients early on, because it can easily distinguish real and fake images.&lt;/p&gt;

&lt;p&gt;If the gradients at the layer of generator are too small, learning might be slow or not happening at all. The generator should get large gradients early on and the discriminator getting consistently high gradients at the top layer once the generator has been trained enough.&lt;/p&gt;

&lt;h2&gt; 3. Detecting GAN Failure Modes &lt;/h2&gt;
&lt;p&gt;The reason why GANs are difficult to train is that both generator and the discriminator are trained simultaneously in a zero-sum game. This means that improvements to one model come at the expense of the other model. 
The goal of training two models involves finding a point of equilibrium between the two competing concerns. It also means that everytime the parameters of one model are updated the nature of the optimization problem that is being solved is updated as well. The technical challenge of training two competing neural networks at the same time is that they can fail to converge.&lt;/p&gt;

&lt;ul&gt;&lt;li class=&quot;highlight&quot;&gt; &lt;b&gt; Convergence Failure &lt;/b&gt;&lt;/li&gt;
&lt;div class=&quot;indent&quot;&gt; The fact that GANs are composed by two networks, and each of them has its loss function leads to GANs unstability. In GAN architecture, the discriminator tries to minimize a cross-entropy while the generator tries to maximize it. When discriminator confidence is high and the discriminator starts to reject the samples that are produced by the generator, generator's gradient vanishes. 

This scenario happens when the generator score reaches near zero and the discriminator score reaches near one. The discriminator is overpowering the generator. If the score does not recover from these values for many iterations, it is better to stop training. &lt;/div&gt;

&lt;li class=&quot;highlight&quot;&gt;&lt;b&gt; Mode Collapse &lt;/b&gt; &lt;/li&gt;
&lt;div class=&quot;indent&quot;&gt; Mode collapse is when the GAN produces a small variety of images with many duplicates. This happens when the generator is unable to learn a rich feature representation because  it learns to associate similar outputs to multiple different inputs. The most promising way to check for mode collapse is to inspect the generated images. If there is little diversity in the output and some of them are almost identical, there is likely mode collapse. If you observe this happening, you should try to increase the ability of the generator to create more diverse outputs or impair the discriminator by randomly giving false labels to real images.

Another type of behavior you should look out for is when the generator oscillates between generating specific examples in the domain. They progress from generating one kind of sample to generating another kind of sample without eventually reaching equilibrium.
&lt;/div&gt;

&lt;li class=&quot;highlight&quot;&gt; &lt;b&gt; Diminisheed Gradient &lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;
&lt;div class=&quot;indent&quot;&gt; This situation happens when the discriminator gets too successful that the generator gradient vanishes and learns nothing.


&lt;h2&gt; Lessons I Learned &lt;/h2&gt;
&lt;div class=&quot;three&quot;&gt; Use a batch size smaller than or equal to 64.&lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; In my experience, using bigger batch sizes often hurt the performance. I suspect it fuels the problem of discriminator getting too good at discriminating the real and fake images, since large batch size means providing a lot of examples to train on. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Add noise to both real and synthetic data. &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; It is well known that making the training of discriminator more difficult is beneficial for the overall stability. Adding noise increases the complexity of the discriminator training and stabilizes the data distribution of the two competing networks. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Use Label Smoothing &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; If the label for real images is set to 1, change it to a lower value like 0.9. This solution discourages the discriminator from being overconfident. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Different learning rates for the generator and discriminator a.k.a. Two Time-Scale Update Rule &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; In my experience, choosing a higher learning rate for the discriminator(i.e. 0.0004) and a lower one(i.e. 0.0001) for the generator works well in practice. I guess the reason is that the generator has to make small steps to fool the discriminator so it does not choose fast but not precise solutions to win the adversarial game. &lt;/div&gt;

&lt;div class=&quot;three&quot;&gt; Use some kind of normalization method &lt;/div&gt;
&lt;div class=&quot;textbox&quot;&gt; For me, applying Spectral Normalization, a particular kind of normalization applied on the convolutional kernels, greatly helped the stability of training.&lt;/div&gt;

&lt;blockquote&gt; I learned that hyperparameter tuning takes a lot of time and patience especially for training GANs. &lt;/blockquote&gt;



&lt;/div&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">I wrote a short article about what I learned training GANs. GAN is well known for its instability in training and there are pitfalls worth knowing.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/gan.png" /><media:content medium="image" url="http://localhost:4000/assets/images/gan.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dissecting the Camera Matrix (Part 2)</title><link href="http://localhost:4000/extrinsic.html" rel="alternate" type="text/html" title="Dissecting the Camera Matrix (Part 2)" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/extrinsic</id><content type="html" xml:base="http://localhost:4000/extrinsic.html">&lt;!--more--&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/cali.png&quot; /&gt;&lt;/picture&gt;
&lt;div class=&quot;caption&quot;&gt; Overview of the Camera Calibration Parameters &lt;/div&gt;

&lt;h2&gt; The Extrinsic Camera Matrix &lt;/h2&gt;

&lt;p&gt;The extrinsic matrix takes the form of a rigid transformation matrix: a $3 \times 3$ rotation matrix in the left-block, and $3 \times 1$ translation column-vector in the right.&lt;/p&gt;

\[\begin{bmatrix}
\begin{array}{ccc|c}
  r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; t_1 \\
  r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; t_2 \\
  r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; t_3
\end{array}
\end{bmatrix}\]

&lt;p&gt;It is common to see a version of this matrix with extra row of $(0,0,0,1)$ added to the bottom. This makes the matrix square, which allows us to further decompose this matrix into a rotation followed by translation:&lt;/p&gt;

\[\begin{bmatrix}
\begin{array}{ccc|c}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; t_1 \\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; t_2 \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; t_3 \\
\hline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{array}
\end{bmatrix}

\times

\begin{bmatrix}
\begin{array}{ccc|c}
r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; 0 \\
r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; 0 \\
r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; 0 \\
\hline
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1
\end{array}
\end{bmatrix}\]

&lt;p&gt;The matrix describes how to transform points in world coordinates to camera coordinates. The important thing to remember about the extrinsic matrix is that it describes how the &lt;span class=&quot;blue&quot;&gt; world &lt;/span&gt; is transformed &lt;span class=&quot;blue&quot;&gt; relative to the camera &lt;/span&gt;. This if often counter-intuitive, because we usually want to specify how the &lt;span class=&quot;red&quot;&gt; camera &lt;/span&gt; is transformed &lt;span class=&quot;red&quot;&gt; relative to the world &lt;/span&gt;.&lt;/p&gt;

&lt;h2&gt; Building the Extrinsic Matrix from Camera Pose &lt;/h2&gt;

&lt;p&gt;Like I said before, it is often more natural to &lt;span class=&quot;highlight-yellow&quot;&gt; specify the camera’s pose directly &lt;/span&gt; rather than specifying &lt;span class=&quot;highlight-pink&quot;&gt; how world points should transform to camera coordinates &lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Luckily, building an extrinsic camera matrix this way is easy: just &lt;span class=&quot;highlight-green&quot;&gt; build a rigid transformation matrix that describes the camera’s pose &lt;/span&gt; and then &lt;span class=&quot;rainbow&quot;&gt; take its inverse &lt;/span&gt;.&lt;/p&gt;

\[\begin{bmatrix}
\begin{array}{c|c}
R &amp;amp; t \\
0 &amp;amp; 1 
\end{array}
\end{bmatrix}

= 

\begin{bmatrix}
\begin{array}{c|c}
R_c &amp;amp; C \\
0 &amp;amp; 1 
\end{array}
\end{bmatrix}^{-1}\]

&lt;p&gt;Let $C$ be a column vector describing the location of the camera-center in world coordinates, and let $R_c$ be the rotation matrix describing the camera’s orientation with respect to the world coordinate axes. Then extrinsic matrix is obtained by inverting the camera’s pose matrix.&lt;/p&gt;

&lt;blockquote&gt; Algebraically a rotation matrix in $n$-dimensions is a $n \times n$ special orthogonal matrix, i.e. an orthogonal matrix whose determinant is 1. &lt;/blockquote&gt;

&lt;div class=&quot;sidenote&quot;&gt; We can define matrix $R$ that rotates in the $xy$-Cartesian plane counterclock-wise through an angle $\theta$ about the origin of the Cartesian system as follows:

$$
R = \begin{bmatrix}
\cos\theta &amp;amp; -\sin\theta \\
\sin\theta &amp;amp; \cos\theta
\end{bmatrix}
$$

&lt;/div&gt;

&lt;div class=&quot;sidenote&quot;&gt; The set of all rotation matrices form a group, known as the special orthogonal group. The inverse of a rotation matrix is its transpose, which is also a rotation matrix. 

$$
\displaylines{
R^T = R^{-1} \\
det(R) = 1
}
$$
&lt;/div&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/rotation.png&quot; /&gt;&lt;/picture&gt;
&lt;div class=&quot;caption&quot;&gt; the extrinsic matrix is obtained by inverting the camera's pose matrix &lt;/div&gt;

&lt;p&gt;We here use the fact that the inverse of a rotation matrix is its transpose, and inverting a translation matrix simply negates the translation vector. Relationship between the extrinsic matrix parameters and the camera’s pose is straightforward:&lt;/p&gt;

\[\displaylines{
R = R^T_c \\
t = -RC
}\]

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://ksimek.github.io/2012/08/14/decompose/&quot;&gt; ksimek blog &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prateekvjoshi.com/2014/05/31/understanding-camera-calibration/&quot;&gt; prateekvjoshi blog &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">This is the second part of our journey to master the camera matrix. In this blog post, we will study the extrinsic camera parameters. Extrinsic matrix describes the camera's location in the world.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/extrinsic.png" /><media:content medium="image" url="http://localhost:4000/assets/images/extrinsic.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Image Processing: the Basics</title><link href="http://localhost:4000/template.html" rel="alternate" type="text/html" title="Image Processing: the Basics" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/template</id><content type="html" xml:base="http://localhost:4000/template.html">&lt;!--more--&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/disparity.png&quot; /&gt;&lt;/picture&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&amp;lt;a=href=&quot;&quot;&amp;gt; TheAILearner &amp;lt;/a&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introduction to Visual SLAM: From Theory to Practice</title><link href="http://localhost:4000/slam.html" rel="alternate" type="text/html" title="Introduction to Visual SLAM: From Theory to Practice" /><published>2021-09-14T00:00:00+09:00</published><updated>2021-09-14T00:00:00+09:00</updated><id>http://localhost:4000/slam</id><content type="html" xml:base="http://localhost:4000/slam.html">&lt;!--more--&gt;

&lt;h2&gt; Monocular Dense Reconstruction &lt;/h2&gt;
&lt;h3&gt; Stereo Vision &lt;/h3&gt;

&lt;picture&gt;&lt;img src=&quot;/assets/images/disparity.png&quot; /&gt;&lt;/picture&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&amp;lt;a=href=&quot;&quot;&amp;gt; TheAILearner &amp;lt;/a&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/depth/1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/images/depth/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Advanced PyTorch: Things You Didn’t Know</title><link href="http://localhost:4000/pytorch.html" rel="alternate" type="text/html" title="Advanced PyTorch: Things You Didn’t Know" /><published>2021-09-12T00:00:00+09:00</published><updated>2021-09-12T00:00:00+09:00</updated><id>http://localhost:4000/pytorch</id><content type="html" xml:base="http://localhost:4000/pytorch.html">&lt;!--more--&gt;

&lt;h2&gt; Flatten Operation for a Batch of Image Inputs to a CNN 
&lt;/h2&gt;
&lt;p&gt;Flattening specific tensor axis is often required with CNNs because we work with batches of inputs opposed to single inputs. A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flattened out so that the fully connected layer can accept them as the input. A flatten operation is a specific type of reshaping operation where by all of the axes are smooshed or squashed together.&lt;/p&gt;

&lt;p&gt;To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something that is not already flat. For example, in the MNIST dataset, we will look at an handwritten image of eight. This image has 2 distinct dimensions, height and width.&lt;/p&gt;

&lt;p&gt;The height and width are $18 \times 18$ respectively. These dimensions tell use that this is a cropped image becaue the MNIST dataset contains $28 \times 28$ images. Let’s see how these two axes of height and width are flattened out into a single axis of length 324 (c.f. 324 what we get when multiplying 18 with 18).&lt;/p&gt;

&lt;h3&gt; Flattening Specific Axes of a Tensor &lt;/h3&gt;

&lt;p&gt;Tensor inputs to a convolutional neural network typically have 4 axes, one for batch size, one for color channels, and one each for height and width.&lt;/p&gt;

\[[B,C,H,W]\]

&lt;p&gt;Suppose we have the following three tensors:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Each of these has a shape of $4 \times 4$, so we have three rank-2 tensors. For our purpose, we’ll consider these to be three $4 \times 4$ images that we will use to create a batch that can be passed to a CNN. Batches are represented using a single tensor, so we’ll need to combine these three tensors into a single larger tensor that has 3 axes instead of 2.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here, we used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stack()&lt;/code&gt; method to concatenate our sequence of tensors along a new axis. Since we have three tensors along a new axis, we know that the length of this axis should be 3. At this point, we have a rank-3 tensor that contains a batch of three $4 \times 4$ images. All we need to do now to get this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit single color channel for each of these image tensors, so in practice, these would be grayscale images.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Notice how the additional axis of length 1 doesn’t change the number of elements in the tensor. This is because the product of the components values doesn’t change when we multiply by one.&lt;/p&gt;

&lt;p&gt;The first axis has 3 elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain 4 arrays that contain 4 numbers or scalar components.&lt;/p&gt;

&lt;h3&gt; Flattening the Tensor Batch &lt;/h3&gt;

&lt;p&gt;Let’s see how to flatten images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, &lt;span class=&quot;underline&quot;&gt; we don’t want to flatten the whole thing &lt;/span&gt; We only want to &lt;span class=&quot;glow&quot;&gt; flatten the image tensors &lt;/span&gt; within the batch tensor.&lt;/p&gt;

&lt;p&gt;For example, if we do the following operations on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# this is the same operation as t.flatten()
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the batches together into a single axis. The flattened batch won’t work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess.&lt;/p&gt;

&lt;p&gt;The solution here, is to flatten each image while &lt;span color=&quot;blink&quot;&gt; still maintaining the batch axis &lt;/span&gt;. This means we want to &lt;span class=&quot;underline&quot;&gt; flatten only part of the tensor &lt;/span&gt;. We want to flatten the color channel axis with the height and width axes.&lt;/p&gt;

&lt;blockquote&gt; The Axes that Need to be Flattened: $[C,H,W]$ &lt;/blockquote&gt;
&lt;p&gt;This can be done with PyTorch’s built in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flatten()&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice how we specified the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start_dim&lt;/code&gt; parameter.This tells the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flatten()&lt;/code&gt; method which axis it should start the flatten operation. Now we have a rank-2 tensor with three single color channel images that have been flattened out into 16 pixels.&lt;/p&gt;

&lt;h3&gt; Flattening an RGB Image &lt;/h3&gt;
&lt;p&gt;If we flatten an RGB image, what happens to the color? Each color channel will be flattened first, then the flattened channels will be lined up side by side on a single axis of the tensor.&lt;/p&gt;

&lt;p&gt;For example, we build an RGB image tensor like the following code:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;By flattening the image tensor, this is how it is going to look like.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2&gt; Broadcasting and Element-Wise Operations with PyTorch &lt;/h2&gt;

&lt;blockquote&gt; Remember, all these rules apply to PyTorch Tensors! Python built-in types such as list will not behave this way.&lt;/blockquote&gt;
&lt;h3&gt; Element-Wise Operations &lt;/h3&gt;
&lt;p&gt;An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element. Therefore, we can deduce that tensors must have the same shape in order to perform an element-wise operation.&lt;/p&gt;

&lt;h3&gt; Broadcasting Tensors &lt;/h3&gt;
&lt;p&gt;Broadcasting describes how tensors with different shapes are treated during element-wise operations. For example, suppose we have the following two tensors:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;What will be the result of this two tensors’ element-wise addition operation? Even though these two tensors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t2&lt;/code&gt; will be transformed via broadcasting to match the shape of the higher rank tensor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t1&lt;/code&gt;, and the element-wise operation will be performed as usual.&lt;/p&gt;

&lt;p&gt;The concept of broadcasting is the key to understanding how this operation will be carried out. We can check the broadcast transformation using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcast_to()&lt;/code&gt; numpy function.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcast_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing and especially during normalization routines.&lt;/p&gt;

&lt;h3&gt; Element-Wise Operation Applies to Comparision and Functions &lt;/h3&gt;
&lt;p&gt;Comparison operations are also element-wise operations. For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.bool&lt;/code&gt; value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt;. It is also fine to assume that the function is applied to each element of the tensor.&lt;/p&gt;

&lt;div class=&quot;sidenote&quot;&gt; there are other ways to refer to element-wise operations, such as component-wise or point-wise &lt;/div&gt;

&lt;h2&gt; Argmax and Reduction Operations for Tensors &lt;/h2&gt;
&lt;p&gt;Now, we will focus in on the frequently used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;argmax()&lt;/code&gt; function, and we’ll see how to access the data inside our tensors.&lt;/p&gt;

&lt;h3&gt; Tensor Reduction Operation &lt;/h3&gt;

&lt;blockquote&gt; A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.&lt;/blockquote&gt;

&lt;p&gt;Reduction operations allow us to perform operations on element within a single tensor. Let’s look at an example. Suppose we have the following rank-2 tensor:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;8.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The sum of our tensor’s scalar components is calculated using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum()&lt;/code&gt; tensor method. The result of this call is a &lt;span class=&quot;rainbow&quot;&gt; scalar-valued tensor &lt;/span&gt;. Since the number of elements have been reduced by the operation, we can conclude that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum()&lt;/code&gt; method is a reduction operation.&lt;/p&gt;

&lt;p&gt;Other common reduction functions include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.sum()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.prod()&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.mean()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t.std()&lt;/code&gt;. All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor’s elements.&lt;/p&gt;

&lt;p&gt;Reduction operations in general allow us to compute aggregate values across data structures. But do reduction operations always reduce to a tensor with a single element? The answer is no. In fact, we often reduce specific axes at a time. This process is important.&lt;/p&gt;

&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://deeplizard.com/learn/video/K3lX3Cltt4c&quot;&gt; deeplizard &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content><author><name>seri</name></author><category term="PyTorch" /><category term="featured" /><summary type="html">This blog post is for those who know the basics of PyTorch but want to go a step further. We will be diving into principles and applications of deep learning via PyTorch.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/pytorch.png" /><media:content medium="image" url="http://localhost:4000/assets/images/pytorch.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Depth Estimation - An Introduction</title><link href="http://localhost:4000/depth-intro.html" rel="alternate" type="text/html" title="Depth Estimation - An Introduction" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>http://localhost:4000/depth-intro</id><content type="html" xml:base="http://localhost:4000/depth-intro.html">&lt;h3&gt; Paradigms for 3D Images Representation over a Plane &lt;/h3&gt;
&lt;p&gt;As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the depth information should be able to be represented in a plane, for printing purposes, for example.&lt;/p&gt;

&lt;p&gt;There are three widely used modes for depth representation:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt; Gray scale 2.5D representation. This paradigm uses the gray scale intensity to represent the depth of each pixel in the image. Thus, the color, texture and luminosity of the original image are lost in this representation. The name &quot;2.5D&quot; refers to the fact that this kind of images has the depth information directly in each pixel, while it is represented over a 2D space. In this paradigm, the gray level represents the inverse of the distance. Thus, more a pixel is bright, closer is the point represented. Vice versa, the darker is a pixel, further is the represented point. This is most commonly used way for depth representation. &lt;/li&gt;
&lt;li&gt; Color 2.5D representation. This representation is similar to the previous one. The difference is the use of colors to represent depth. In the following image, red-black colors represent closer points, and blue-dark colors the further points. However, other color representations are available in the literature. &lt;/li&gt;
&lt;li&gt; Pseudo-3D representation. This representation provides different points of view of the reconstructed space. &lt;/li&gt;

The main advantage of the first two methods is the possibility of implementing objective comparison among algorithms, as it is done in the Middlebury database and test system. 

&lt;h3&gt; Important terms and issues in depth estimation &lt;/h3&gt;
The depth estimation world is quite complex research field, where many techniques and setups have been proposed. The set of algorithms which solve the depth map estimation problem deals with many different mathematical concepts which should be briefly explained for a minimum overall comprehension of the matter. In this section, we will review some important points about image processing applied to depth estimation.

&lt;h4&gt; Standard Test Beds &lt;/h4&gt;
The availability of common tests and comparable results is a mandatory constraint in active and widely explored fields. Likewise, the possibility of objective comparisons make easier to classify different proposals.

In depth estimation, and more specifically in stereo vision, one of the most important test bed is the Middlebury database and test bed. The test beds provides both eyes images of a 3D scene, as well as the ground truth map. The same test allow, as said, algorithms classification.

&lt;h4&gt; Color or Grayscale Images? &lt;/h4&gt;
The first point when we want to process an image, whichever is the goal, is to decide what to process. In this case, color or grayscale images. As it can be seen in the following figure, color images have much more information than gray scale images. Color images should, hence, be more appropriate for data extraction, among them, depth information. 

However, the color images have an important disadvantage: for a 256 level definition, they are represented by 3 bytes (24-bit representation), while grayscale images with the same level only require one single byte. The consequence is obvious: color image processing requires much more time and operations.

&lt;h4&gt; The Epipolar Geometry &lt;/h4&gt;
When dealing with stereo vision setups, we have to face the epipolar geometry problem. Let $C_l$ and $C_r$ be the focal centers of the left and right sensors (or eyes), and $L$ and $R$ the left and right image planes. Finally, $P$ will be a physical point of the scene and $p_l$ and $p_r$ the projections of $P$ over $L$ and $R$, respectively. 

&lt;picture&gt;
&lt;img src=&quot;/assets/images/epipolar.png&quot; /&gt;
&lt;/picture&gt;

In this figure, we can also see both &quot;epipoles&quot;, i.e., the points where the line connecting both focal centers intersect the image planes. They are noted as $e_l$ and $e_r$.

The geometrical properties of this setup force that every point of the line $Pp_l$ lies on the line $p_re_r$ which is called &quot;epipole line&quot;. The correspondence ofa poitn seen in one image must be searched in the corresponding epipolar line in the other one. A simplified version of the geometry arise when the image planes are parallel. This is the base of the so-called fronto-parallel hypothesis.

&lt;h4&gt; The Fronto-Parallel Hypothesis &lt;/h4&gt;
The epipolar geometry of two sensors can be simplified, as said, positioning both planes parallel, arriving to the following setup:

&lt;picture&gt;
&lt;img src=&quot;/assets/images/fronto.png&quot; /&gt;
&lt;/picture&gt;

The epipoles are placed in the infinite, and the epipolar (and search) lines become horizontal. The point (except the occluded ones) are only decaled horizontally. 

This geometrical setup can be implemented by properly orienting the sensors, or by means of mathematical transformation fo the original images. If the last option is the case, the result is called &quot;rectified image&quot;. 

The most important consequences of this geometry, regarding the Cartesian plane can be written as follows:
&lt;ul&gt;&lt;li&gt; $y_l = y_r$. The height of a physical point is the same in both images. &lt;/li&gt;
&lt;li&gt; $x_l = x_r + \nabla d$. The abcissa of a physical point is decaled by the so-called &lt;span class=&quot;blue&quot;&gt; parallax &lt;/span&gt; or &lt;span class=&quot;blue&quot;&gt; disparity &lt;/span&gt;, which is inversely related to the depth. &lt;/li&gt;
&lt;li&gt; A point in the infinite has identical abscissa coordinates in both image planes. &lt;/li&gt;&lt;/ul&gt;

&lt;h4&gt; Matching &lt;/h4&gt;
When different viewpoints from the same scene are compared, a further problem arises that is associated with the mutual identification of images. The solution to this problem is commonly referrred to as matching. The matching process consists of identifying each physical points within different images. However, matching techniques are not only used in stereo or multivision procedures but also widely used for image retrieval or fingerprint identification where it is important to allow rotational and scalar distortions.

There are also various constraints that are generally satisfied by true matches thus simplifying the depth estimation algorithm, such as similarity, smoothness, ordering and uniqueness. 

As we will see, the matching process is a conceptual approach to identify similar characteristics in different images. It is, then, subjected to errors. The matching is, hence, implemented by means of comparators allowing different identification strategies such as minimum square errors (MSE), sum of absolute differences (SAD) or sum of squared differences (SSD). The characteristic compared through the matching process can be anything quantifiable. Thus, we will see algorithms matching points, edges, regions or other image cues. 








&lt;h2&gt; References &lt;/h2&gt;
&lt;ul&gt; 
&lt;li&gt;&lt;a href=&quot;https://ksimek.github.io/2012/08/14/decompose/&quot;&gt; ksimek blog &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://prateekvjoshi.com/2014/05/31/understanding-camera-calibration/&quot;&gt; prateekvjoshi blog &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;/ul&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">Paradigms for 3D Images Representation over a Plane As we saw in the previous section, the projection onto a plane forces the loss of the depth dimension of the scene. However, the depth information should be able to be represented in a plane, for printing purposes, for example. There are three widely used modes for depth representation: Gray scale 2.5D representation. This paradigm uses the gray scale intensity to represent the depth of each pixel in the image. Thus, the color, texture and luminosity of the original image are lost in this representation. The name &quot;2.5D&quot; refers to the fact that this kind of images has the depth information directly in each pixel, while it is represented over a 2D space. In this paradigm, the gray level represents the inverse of the distance. Thus, more a pixel is bright, closer is the point represented. Vice versa, the darker is a pixel, further is the represented point. This is most commonly used way for depth representation. Color 2.5D representation. This representation is similar to the previous one. The difference is the use of colors to represent depth. In the following image, red-black colors represent closer points, and blue-dark colors the further points. However, other color representations are available in the literature. Pseudo-3D representation. This representation provides different points of view of the reconstructed space. The main advantage of the first two methods is the possibility of implementing objective comparison among algorithms, as it is done in the Middlebury database and test system. Important terms and issues in depth estimation The depth estimation world is quite complex research field, where many techniques and setups have been proposed. The set of algorithms which solve the depth map estimation problem deals with many different mathematical concepts which should be briefly explained for a minimum overall comprehension of the matter. In this section, we will review some important points about image processing applied to depth estimation. Standard Test Beds The availability of common tests and comparable results is a mandatory constraint in active and widely explored fields. Likewise, the possibility of objective comparisons make easier to classify different proposals. In depth estimation, and more specifically in stereo vision, one of the most important test bed is the Middlebury database and test bed. The test beds provides both eyes images of a 3D scene, as well as the ground truth map. The same test allow, as said, algorithms classification. Color or Grayscale Images? The first point when we want to process an image, whichever is the goal, is to decide what to process. In this case, color or grayscale images. As it can be seen in the following figure, color images have much more information than gray scale images. Color images should, hence, be more appropriate for data extraction, among them, depth information. However, the color images have an important disadvantage: for a 256 level definition, they are represented by 3 bytes (24-bit representation), while grayscale images with the same level only require one single byte. The consequence is obvious: color image processing requires much more time and operations. The Epipolar Geometry When dealing with stereo vision setups, we have to face the epipolar geometry problem. Let $C_l$ and $C_r$ be the focal centers of the left and right sensors (or eyes), and $L$ and $R$ the left and right image planes. Finally, $P$ will be a physical point of the scene and $p_l$ and $p_r$ the projections of $P$ over $L$ and $R$, respectively. In this figure, we can also see both &quot;epipoles&quot;, i.e., the points where the line connecting both focal centers intersect the image planes. They are noted as $e_l$ and $e_r$. The geometrical properties of this setup force that every point of the line $Pp_l$ lies on the line $p_re_r$ which is called &quot;epipole line&quot;. The correspondence ofa poitn seen in one image must be searched in the corresponding epipolar line in the other one. A simplified version of the geometry arise when the image planes are parallel. This is the base of the so-called fronto-parallel hypothesis. The Fronto-Parallel Hypothesis The epipolar geometry of two sensors can be simplified, as said, positioning both planes parallel, arriving to the following setup: The epipoles are placed in the infinite, and the epipolar (and search) lines become horizontal. The point (except the occluded ones) are only decaled horizontally. This geometrical setup can be implemented by properly orienting the sensors, or by means of mathematical transformation fo the original images. If the last option is the case, the result is called &quot;rectified image&quot;. The most important consequences of this geometry, regarding the Cartesian plane can be written as follows: $y_l = y_r$. The height of a physical point is the same in both images. $x_l = x_r + \nabla d$. The abcissa of a physical point is decaled by the so-called parallax or disparity , which is inversely related to the depth. A point in the infinite has identical abscissa coordinates in both image planes. Matching When different viewpoints from the same scene are compared, a further problem arises that is associated with the mutual identification of images. The solution to this problem is commonly referrred to as matching. The matching process consists of identifying each physical points within different images. However, matching techniques are not only used in stereo or multivision procedures but also widely used for image retrieval or fingerprint identification where it is important to allow rotational and scalar distortions. There are also various constraints that are generally satisfied by true matches thus simplifying the depth estimation algorithm, such as similarity, smoothness, ordering and uniqueness. As we will see, the matching process is a conceptual approach to identify similar characteristics in different images. It is, then, subjected to errors. The matching is, hence, implemented by means of comparators allowing different identification strategies such as minimum square errors (MSE), sum of absolute differences (SAD) or sum of squared differences (SSD). The characteristic compared through the matching process can be anything quantifiable. Thus, we will see algorithms matching points, edges, regions or other image cues. References ksimek blog prateekvjoshi blog</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/camera/camera.jpeg" /><media:content medium="image" url="http://localhost:4000/assets/images/camera/camera.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision</title><link href="http://localhost:4000/point-cloud_.html" rel="alternate" type="text/html" title="Dense 3D Point Cloud Representation of a Scene Using Uncalibrated Monocular Vision" /><published>2021-09-09T00:00:00+09:00</published><updated>2021-09-09T00:00:00+09:00</updated><id>http://localhost:4000/point-cloud_</id><content type="html" xml:base="http://localhost:4000/point-cloud_.html">&lt;h2&gt; Introduction &lt;/h2&gt;
&lt;p&gt;In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is affected by a variety of components. From the electro-optic sensors to the image resolution, contrast, exposure and blurriness variables, all add to the complexity of analyzing a scene and processing the imagery.&lt;/p&gt;

&lt;h2&gt; Monocular Vision &lt;/h2&gt;
&lt;p&gt;Traditional stereoscopy systems, such as human visual system, utilize multiple viewing angles of the same object in order to do triangulation and get a depth perception. Monocular vision, better described as vision through a single camera source, presents new challenges when compared to stereo vision or a multi-camera system. In a stereo system, similar to human vision, distances between the cameras (the baseline) and their orientation is known and in most circumstances remains constant. In order to generate various viewing angles with a monocular system, the camera must continuously be moving. With a moving camera, the system obtains two different viewing angles form two points in time. The challenge becomes to accurately compute the distance the camera has traveled or the exact location of the camera at each frame of video. In addition, the orientation of the camera at each point in time must be computed from the scene.&lt;/p&gt;

&lt;h2&gt; Visual Structure from Motion &lt;/h2&gt;
&lt;p&gt;In his work, Wu et al. describes the methodology of Visual Structure from Motion as following. Using a set of image feature locations and correspondences, the goal of bundle adjustment is to find 3D point positions and camera parameters that minimize the re-projection error. This optimization problem is constructed as a non-linear least squares problem, where the error is the squared &lt;img src=&quot;https://latex.codecogs.com/gif.latex?L_2&quot; /&gt; norm of the difference between the observed feature location and the projection of the corresponding 3D point on the image plane of the camera.&lt;/p&gt;

&lt;p&gt;Wu explains by letting &lt;img src=&quot;https://latex.codecogs.com/gif.latex?x&quot; /&gt; be a vector of parameters and &lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x)%20=%20[f_1(x),%20&amp;#x5C;dots,%20f_k(x)]&quot; /&gt; be the vector of residual errors for 3D reconstruction. Then the optimization problem he wishes to solve is the non-linear least squares problem shown in the below equation.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?x*%20=%20&amp;#x5C;argmin_{x}%20&amp;#x5C;sum_{i=1}^{k}%20&amp;#x5C;lVert%20f_i(x)%20&amp;#x5C;rVert^2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Levenberg-Marquardt (LM) algorithm is an extremely popular algorithm for solving non-linear least squares problems and is the algorithm choice for bundle adjustment. LM operates by computing a series of regularized linear approximations to the original nonlinear problem. Let &lt;img src=&quot;https://latex.codecogs.com/gif.latex?J(x)&quot; /&gt; be the Jacobian of &lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x)&quot; /&gt;, then in each iteration LM solves a linear least squares problem of the form&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?&amp;#x5C;delta*%20=%20&amp;#x5C;argmin_{&amp;#x5C;delta}%20&amp;#x5C;lVert%20J(x)&amp;#x5C;delta%20+%20f(x)%20&amp;#x5C;rVert^2%20+%20&amp;#x5C;lambda%20&amp;#x5C;lVert%20D(x)%20&amp;#x5C;delta%20&amp;#x5C;rVert^2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and updates &lt;img src=&quot;https://latex.codecogs.com/gif.latex?x%20&amp;#x5C;leftarrow%20x%20+%20&amp;#x5C;delta*&quot; /&gt; if &lt;img src=&quot;https://latex.codecogs.com/gif.latex?&amp;#x5C;lVert%20f(x+&amp;#x5C;delta*)&amp;#x5C;rVert%20&amp;lt;%20&amp;#x5C;lVert%20f(x)%20&amp;#x5C;rVert&quot; /&gt;. Here &lt;img src=&quot;https://latex.codecogs.com/gif.latex?D(x)&quot; /&gt; is a non-negative diagonal matrix, typically the square root of the diagonal of the matrix &lt;img src=&quot;https://latex.codecogs.com/gif.latex?J(x)^TJ(x)&quot; /&gt; and &lt;img src=&quot;https://latex.codecogs.com/gif.latex?&amp;#x5C;lambda&quot; /&gt; is a nonnegative parameter that controls the strength of regularization. Wu explains that the regularization is needed to ensure a convergent algorithm. LM updates the value of &lt;img src=&quot;https://latex.codecogs.com/gif.latex?&amp;#x5C;lambda&quot; /&gt; at each step based on how well the Jacobian &lt;img src=&quot;https://latex.codecogs.com/gif.latex?J(x)&quot; /&gt; approximates &lt;img src=&quot;https://latex.codecogs.com/gif.latex?f(x)&quot; /&gt;.&lt;/p&gt;

&lt;h2&gt; SIFT Features &lt;/h2&gt;

&lt;p&gt;The SIFT algorithm can be broken down into four main stages:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;scale-space peak selection&lt;/li&gt;
  &lt;li&gt;point localization&lt;/li&gt;
  &lt;li&gt;orientation assignment&lt;/li&gt;
  &lt;li&gt;point descriptor
The first stage is to search for interest points over location and scale. The image is constructed in a Gaussian Pyramid, where the image is downsampled and blurred at each level. These blurred images at each level are used to compute the Difference of Gaussians (DoG), which locate edges and corners within an image. Interesting points are then extracted in stage 2 by locating the maxima/minimal pixels within different scales of DoG at sub-pixel accuracy. Once interest points have been located, an orientation is assigned based on the gradient orientation of the pixels around the interest point.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once orientation and scale have been addressed, the final stage is the generation of point descriptors.&lt;/p&gt;

&lt;p&gt;d&lt;/p&gt;</content><author><name>seri</name></author><category term="computer vision" /><category term="featured" /><summary type="html">Introduction In this blog post, we describe the challenges and existing solutions within the research community regarding reconstructing of a scene using a single camera. Imagery is affected by a variety of components. From the electro-optic sensors to the image resolution, contrast, exposure and blurriness variables, all add to the complexity of analyzing a scene and processing the imagery. Monocular Vision Traditional stereoscopy systems, such as human visual system, utilize multiple viewing angles of the same object in order to do triangulation and get a depth perception. Monocular vision, better described as vision through a single camera source, presents new challenges when compared to stereo vision or a multi-camera system. In a stereo system, similar to human vision, distances between the cameras (the baseline) and their orientation is known and in most circumstances remains constant. In order to generate various viewing angles with a monocular system, the camera must continuously be moving. With a moving camera, the system obtains two different viewing angles form two points in time. The challenge becomes to accurately compute the distance the camera has traveled or the exact location of the camera at each frame of video. In addition, the orientation of the camera at each point in time must be computed from the scene. Visual Structure from Motion In his work, Wu et al. describes the methodology of Visual Structure from Motion as following. Using a set of image feature locations and correspondences, the goal of bundle adjustment is to find 3D point positions and camera parameters that minimize the re-projection error. This optimization problem is constructed as a non-linear least squares problem, where the error is the squared norm of the difference between the observed feature location and the projection of the corresponding 3D point on the image plane of the camera. Wu explains by letting be a vector of parameters and be the vector of residual errors for 3D reconstruction. Then the optimization problem he wishes to solve is the non-linear least squares problem shown in the below equation. The Levenberg-Marquardt (LM) algorithm is an extremely popular algorithm for solving non-linear least squares problems and is the algorithm choice for bundle adjustment. LM operates by computing a series of regularized linear approximations to the original nonlinear problem. Let be the Jacobian of , then in each iteration LM solves a linear least squares problem of the form and updates if . Here is a non-negative diagonal matrix, typically the square root of the diagonal of the matrix and is a nonnegative parameter that controls the strength of regularization. Wu explains that the regularization is needed to ensure a convergent algorithm. LM updates the value of at each step based on how well the Jacobian approximates . SIFT Features The SIFT algorithm can be broken down into four main stages: scale-space peak selection point localization orientation assignment point descriptor The first stage is to search for interest points over location and scale. The image is constructed in a Gaussian Pyramid, where the image is downsampled and blurred at each level. These blurred images at each level are used to compute the Difference of Gaussians (DoG), which locate edges and corners within an image. Interesting points are then extracted in stage 2 by locating the maxima/minimal pixels within different scales of DoG at sub-pixel accuracy. Once interest points have been located, an orientation is assigned based on the gradient orientation of the pixels around the interest point. Once orientation and scale have been addressed, the final stage is the generation of point descriptors. d</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/cnn.png" /><media:content medium="image" url="http://localhost:4000/assets/images/cnn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>