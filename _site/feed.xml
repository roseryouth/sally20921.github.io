<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://github.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://github.com/" rel="alternate" type="text/html" /><updated>2021-08-31T00:35:45+09:00</updated><id>https://github.com/feed.xml</id><title type="html">Seri Lee Blog</title><subtitle>This is where I write posts about my research field.</subtitle><entry><title type="html">Pri3D: Can 3D Priors Help 2D Representation Learning?</title><link href="https://github.com/pri3d/" rel="alternate" type="text/html" title="Pri3D: Can 3D Priors Help 2D Representation Learning?" /><published>2021-08-29T00:00:00+09:00</published><updated>2021-08-29T00:00:00+09:00</updated><id>https://github.com/pri3d</id><content type="html" xml:base="https://github.com/pri3d/">&lt;p&gt;&lt;img src=&quot;/assets/images/pri3d.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pri3D leverages 3D priors for downstream 2D image understanding tasks: during pre-training, we incorporate view-invariant and geometric priors from color-geometry information given by RGB-D datasets, imbuing geometric priors into learned features. We show that these 3D-imbued learned features can effectively transfer to improved performance on 2D tasks such as semantic segmentation, object detection, and instance segmentation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3D shapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints. We introduce an approach to learn view-invariant, geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view image constraints and image-geometry constraints to encode 3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learning on the image-based tasks of semantic segmentation, instance segmentation and object detection on real-world indoor datasets, but moreover, provides significant improvement in the low data regime. We show significant improvement of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against baselines on ScanNet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In recent years, we have seen rapid progress in learning-based approaches for semantic understanding of 3D scenes, particularly in the tasks of 3D semantic segmentation, 3D object detection, and 3D semantic instance segmentation. Such approaches leverage geometric observations, exploiting the representation of points, voxels, or meshes to obtain accurate 3D semantics. These have shown significant promise towards realizing applications such as depth-based scene understanding for robotics, as well as augmented or virtual reality. In parallel to the development of such methods, the availability of large-scale RGB-D datasets has further accelerated the research area.&lt;/p&gt;

&lt;p&gt;One advantage of learning directly in 3D in contrast to learning solely from 2D images is that methods operate in metric 3D space; hence, it is not necessary to learn view-dependent effects and/or projective mappings. This allows training 3D neural networks from scratch in a relatively short time frame and typically requires a (relatively) small number of training samples; e.g. state-of-the-art 3D neural networks can be trained with around 1000 scenes from ScanNet. Our main idea is to leverage these advantages in the form of 3D priors for image-based scene understanding.&lt;/p&gt;

&lt;p&gt;Simultaneously, we have seen tremendous progress on representation learning in the image domain, mostly powered by the success of recent contrastive learning based methods. The exploration in 2D representation learning heavily relies on the paradigm of instance discrimination, where different augmented copies of the same instance are drawn closer. Different invariances can be encoded from those low-level augmentations such as random cropping, flipping and scaling, as well as color jittering. However, despite the common belief that 3D view invariance is an essential property for a capable visual system, there remains little study linking the 3D priors and 2D representation learning. The goal of our work is to explore the combination of contrastive representation learning with 3D priors, and offer some preliminary evidence towards answering an important question: can 3D priors help 2D representation learning?&lt;/p&gt;

&lt;p&gt;To this end, we introduce Pri3D, which aims to learn with 3D priors in a pre-training stage and subsequently use them as initialization for fine-tuning on image-based downstream tasks such as semantic segmentation, detection, and instance segmentation. More specifically, we introduce geometric constraints to a contrastive learning scheme, which are enabled by multi-view RGB-D data that is readily available. We propose to exploit geometric correlations implicit multi-view constraints between different images through the correspondence of pixels which correspond to the same geometry, as well as explicit correspondence of geometric patches which correspond to image regions. This imbues geometric knowledge into the learned representations of the image inputs which can then be leveraged as pre-trained features for various image-based vision tasks, particularly in the low training data regime.&lt;/p&gt;

&lt;p&gt;We demonstrate our approach by pre-training on ScanNet under these geometric constraints for representation learning, and show that such self-supervised pre-training (i.e., no semantic labels are used) results in improved performance of 2D semantic segmentation, instance segmentation and detection tasks. We demonstrate this not only on ScanNet data, but also generalizing to improved performance on NYUv2 semantic segmentation, instance segmentation and detection tasks. Moreover, leveraging such geometric priors for pre-training provides robust features which can consistently improve performance under a wide range of amount of training data available. While we focus on indoor scene understanding in this paper, we believe our results can shed light on the paradigm of representation learning with 3D priors and open new opportunities towards more general 3D-aware image understanding.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pri3d-overview.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Method Overview. During pre-training, we use geometric constraints from RGB-D reconstructions to learn 3D priors for image-based representations. Specifically, we propose a contrastive learning formulation that models multi-view correspondences (View-Invariant Contrastive Loss) as well as geometry-to-image alignments (Geometric Prior Contrastive Loss). Our Pri3D pre-training strategy embeds geometric priors into the learned representations (in a form of pre-trained 2D convolutional network weights) that can be further leveraged for downstream 2D-only image understanding tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;learning-representations-from-3d-priors&quot;&gt;Learning Representations from 3D Priors&lt;/h2&gt;
&lt;p&gt;In this section, we introduce Pri3D: our key idea is to leverage constraints from RGB-D reconstructions, now readily available in various datasets, to embed 3D priors in image-based representations. From a dataset of RGB-D sequences, each sequence consists of depth and color frames, ${D_i}$ and ${C_i}$, respectively, as well as automatically-computed 6-DoF camera pose alignments ${T_i}$ (mapping from each camera space to world space) from state-of-the-art SLAM, all resulting in a reconstructed 3D surface geometry $S$. Specifically, we observe that multi-view constraints can be exploited in order to learn view-invariance without the need of costly semantic labels. In addition, we learn features through geometric representations given by the obtained geometry in RGB-D scans, again, without the need of human annotations. For both, we use state-of-the-art contrastive learning in order to constrain the multi-modal input for training. We show that these priors can be embedded in the image-based representations such that the learned features can be used as pre-trained features for purely image-based perception tasks; i.e., we can perform tasks such as image segmentation or instance segmentation on a single RGB image. An overview approach is shown in Figure 2.&lt;/p&gt;

&lt;h3 id=&quot;view-invariant-learning&quot;&gt;View-Invariant Learning&lt;/h3&gt;
&lt;p&gt;In 2D contrastive pre-training algorithms, a variety of data augmentations are used for finding positive matching pairs, such as MoCo and SimCLR. For instance, they use the random crops as self-supervised constraints within the same image for positive pairs, and correspondences to crops from other images as negative pairs. Our key idea is that with the availability of 3D data for training, we can leverage geometric knowledge to provide matching constraints between multiple images that see the same points. To this end, we use the ScanNet RGB-D dataset which provides a sequence of RGB-D images with camera poses computed by a state-of-the-art SLAM method, and reconstructed surface geometry $S$. Note that both the pose alignments and the 3D reconstructions were obtained in a fully-automated fashion without any user input.&lt;/p&gt;

&lt;p&gt;For a given RGB-D sequence in the train set, our method then leverages the 3D data to finding pixel-level correspondences between 2D frames. We consider all pairs of frames $(i,j)$ from the RGB-D sequence. We then back-project frame $i$’s depth map $D_i$ to camera space, and transform the points into the world space by $T_i$. The depth values of frame $j$ are similarly transformed into world space. Pixel correspondences between the two frames are then determined as those whose 3D world locations like within 2cm of each other. We use the pairs of frames which have at least 30% overlap, with overlap computed as number of corresponding pixels in both frames divided by total number of points in the two frames. In total, we sample around 840k pairs of images from the ScanNet training data.&lt;/p&gt;

&lt;p&gt;In the training phase, a pair of sampled images is input to a shared 2D network backbone. In our experiments, we use a UNet-style backbone with ResNet architecture as an encoder, but note that our method is agnostic to the underlying encoder backbone. We then consider the feature map from decoder of the 2D backbone, where its size is half of the input resolution. For each image in the pair, we use the aforementioned pixel-to-pixel correspondences which refer to the same physical 3D point. Note that these correspondences may have different color values due to view-dependent lighting effects but represent the same 3D world location; additionally, the regions surrounding the correspondences appear different due to different viewing angles. In this fashion, we treat these pairs of correspondences as positive samples in contrastive learning; we use all non-matching pixels as negatives. Non-matching pixels are also defined within the set of correspondences. For a pair of frames with $n$ pairs of correspondences as positive samples, we use all $n(n-1)$ negative pairs (each of $n$ pixels from the first frame with each $n-1$ non-matching pixel from the second). Non-matching pixel-voxels are defined similarly but from a pair of frame and 3D chunk.&lt;/p&gt;

&lt;p&gt;Between the features of matching and non-matching pixel locations, we then compute a PointInfoNCE loss, which is defined as: $$L_p = -\sum_{(a,b) \in M}\log{\frac{\exp(f_a \cdot f_b / \tau)}{\sum_{(\cdot,k) \in M}\exp(f_a \cdot f_k /\tau)}}$$, where $M$ is the set of pairs of pixel correspondences, ahd $f$ represents the associated feature vector of a pixel in the feature map. By leveraging multi-view correspondences, e apply implicit 3D priors-without any explicit 3D learning, we imbue view-invariance in the learned image-based features.&lt;/p&gt;

&lt;h3 id=&quot;geometric-prior&quot;&gt;Geometric Prior&lt;/h3&gt;
&lt;p&gt;In addition to multi-view constraints, we also leverage explicit geometry-color correspondences inherent to the RGB-D data during training. For an RGB-D train sequence, the geometry-color correspondences are given by associating the surface reconstruction $S$ with the RGB frames of the sequence. For each frame $i$, we compute its view frustum in the world space. A volumetric chuck $V_i$ of $S$ is then cropped from the axis-aligned bounding box of the view frustum. We represent $V_i$ as a 2cm resolution volumetric occupancy grid from the surface. We thus consider pairs of color frames and geometric chuncks $(C_i, V_i)$.&lt;/p&gt;

&lt;p&gt;From the color-geometry pairs $(C_i, V_i)$, we compute pixel-voxel correspondences by projecting the depth values for each pixel in the corresponding frame $D_i$ into world space to find an associated occupied voxel in $V_i$ that lies within 2cm of the 3D location of the pixel.&lt;/p&gt;

&lt;p&gt;During training, we leverage the color-geometry correspondences with a 2D network backbone and a 3D network backbone. We use a UNet-style architecture with ResNet encoder for the 2D network backbone, and a UNet-sytle sparse convolutional 3D network backbone. Similarly to view-invariant training, we also take the output from the decoder of the 2D network backbone where its output size is half the input resolution. We then use the pixel-voxel correspondences in $(C_i, V_i)$ for contrastive learning, with positives as all matching pixel-voxel pairs and negatives as all non-matching pixel-voxel pairs. We apply the PointInfoNCE loss with $f_i$ as the 2D feature of a pixel, and $f_j$ is the feature vector from its 3D correspondence, and $M$ the set of 2D-3D pixel-voxel correspondences pairs.&lt;/p&gt;

&lt;h3 id=&quot;joint-learning&quot;&gt;Joint Learning&lt;/h3&gt;
&lt;p&gt;We can leverage not only the view-invariant constraints and geometric priors during training, but also learn jointly from the combination of both constraints. We can thus employ a shared 2D network backbone and a 3D network backbone, with the 2D network backbone constrained by both view-invariant constraints and as the 2D part of geometric prior constraint.&lt;/p&gt;

&lt;p&gt;During training, we consider $(C_i, C_j, V_i, V_j)$ of overlapping color frames $C_i$ and $C_j$ as well as $V_i$ and $V_j$ which have geometric correspondence with $C_i$, $C_j$ respectively. The shared 2D network backbone then processes $C_i$, $C_j$ and computes the view-invariant loss from Section 3.1. At the same time, $V_i$ and $V_j$ are processed by the 3D sparse convolutional backbone, with the loss relative to the features of $C_i$ and $C_j$ respectively. This embeds both constraints into the learned 2D representation.&lt;/p&gt;</content><author><name>seri</name></author><category term="paper" /><summary type="html">Pri3D leverages 3D priors for downstream 2D image understanding tasks: during pre-training, we incorporate view-invariant and geometric priors from color-geometry information given by RGB-D datasets, imbuing geometric priors into learned features. We show that these 3D-imbued learned features can effectively transfer to improved performance on 2D tasks such as semantic segmentation, object detection, and instance segmentation. Abstract Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3D shapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints. We introduce an approach to learn view-invariant, geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view image constraints and image-geometry constraints to encode 3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learning on the image-based tasks of semantic segmentation, instance segmentation and object detection on real-world indoor datasets, but moreover, provides significant improvement in the low data regime. We show significant improvement of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against baselines on ScanNet. Introduction In recent years, we have seen rapid progress in learning-based approaches for semantic understanding of 3D scenes, particularly in the tasks of 3D semantic segmentation, 3D object detection, and 3D semantic instance segmentation. Such approaches leverage geometric observations, exploiting the representation of points, voxels, or meshes to obtain accurate 3D semantics. These have shown significant promise towards realizing applications such as depth-based scene understanding for robotics, as well as augmented or virtual reality. In parallel to the development of such methods, the availability of large-scale RGB-D datasets has further accelerated the research area. One advantage of learning directly in 3D in contrast to learning solely from 2D images is that methods operate in metric 3D space; hence, it is not necessary to learn view-dependent effects and/or projective mappings. This allows training 3D neural networks from scratch in a relatively short time frame and typically requires a (relatively) small number of training samples; e.g. state-of-the-art 3D neural networks can be trained with around 1000 scenes from ScanNet. Our main idea is to leverage these advantages in the form of 3D priors for image-based scene understanding. Simultaneously, we have seen tremendous progress on representation learning in the image domain, mostly powered by the success of recent contrastive learning based methods. The exploration in 2D representation learning heavily relies on the paradigm of instance discrimination, where different augmented copies of the same instance are drawn closer. Different invariances can be encoded from those low-level augmentations such as random cropping, flipping and scaling, as well as color jittering. However, despite the common belief that 3D view invariance is an essential property for a capable visual system, there remains little study linking the 3D priors and 2D representation learning. The goal of our work is to explore the combination of contrastive representation learning with 3D priors, and offer some preliminary evidence towards answering an important question: can 3D priors help 2D representation learning? To this end, we introduce Pri3D, which aims to learn with 3D priors in a pre-training stage and subsequently use them as initialization for fine-tuning on image-based downstream tasks such as semantic segmentation, detection, and instance segmentation. More specifically, we introduce geometric constraints to a contrastive learning scheme, which are enabled by multi-view RGB-D data that is readily available. We propose to exploit geometric correlations implicit multi-view constraints between different images through the correspondence of pixels which correspond to the same geometry, as well as explicit correspondence of geometric patches which correspond to image regions. This imbues geometric knowledge into the learned representations of the image inputs which can then be leveraged as pre-trained features for various image-based vision tasks, particularly in the low training data regime. We demonstrate our approach by pre-training on ScanNet under these geometric constraints for representation learning, and show that such self-supervised pre-training (i.e., no semantic labels are used) results in improved performance of 2D semantic segmentation, instance segmentation and detection tasks. We demonstrate this not only on ScanNet data, but also generalizing to improved performance on NYUv2 semantic segmentation, instance segmentation and detection tasks. Moreover, leveraging such geometric priors for pre-training provides robust features which can consistently improve performance under a wide range of amount of training data available. While we focus on indoor scene understanding in this paper, we believe our results can shed light on the paradigm of representation learning with 3D priors and open new opportunities towards more general 3D-aware image understanding. Method Overview. During pre-training, we use geometric constraints from RGB-D reconstructions to learn 3D priors for image-based representations. Specifically, we propose a contrastive learning formulation that models multi-view correspondences (View-Invariant Contrastive Loss) as well as geometry-to-image alignments (Geometric Prior Contrastive Loss). Our Pri3D pre-training strategy embeds geometric priors into the learned representations (in a form of pre-trained 2D convolutional network weights) that can be further leveraged for downstream 2D-only image understanding tasks. Learning Representations from 3D Priors In this section, we introduce Pri3D: our key idea is to leverage constraints from RGB-D reconstructions, now readily available in various datasets, to embed 3D priors in image-based representations. From a dataset of RGB-D sequences, each sequence consists of depth and color frames, ${D_i}$ and ${C_i}$, respectively, as well as automatically-computed 6-DoF camera pose alignments ${T_i}$ (mapping from each camera space to world space) from state-of-the-art SLAM, all resulting in a reconstructed 3D surface geometry $S$. Specifically, we observe that multi-view constraints can be exploited in order to learn view-invariance without the need of costly semantic labels. In addition, we learn features through geometric representations given by the obtained geometry in RGB-D scans, again, without the need of human annotations. For both, we use state-of-the-art contrastive learning in order to constrain the multi-modal input for training. We show that these priors can be embedded in the image-based representations such that the learned features can be used as pre-trained features for purely image-based perception tasks; i.e., we can perform tasks such as image segmentation or instance segmentation on a single RGB image. An overview approach is shown in Figure 2. View-Invariant Learning In 2D contrastive pre-training algorithms, a variety of data augmentations are used for finding positive matching pairs, such as MoCo and SimCLR. For instance, they use the random crops as self-supervised constraints within the same image for positive pairs, and correspondences to crops from other images as negative pairs. Our key idea is that with the availability of 3D data for training, we can leverage geometric knowledge to provide matching constraints between multiple images that see the same points. To this end, we use the ScanNet RGB-D dataset which provides a sequence of RGB-D images with camera poses computed by a state-of-the-art SLAM method, and reconstructed surface geometry $S$. Note that both the pose alignments and the 3D reconstructions were obtained in a fully-automated fashion without any user input. For a given RGB-D sequence in the train set, our method then leverages the 3D data to finding pixel-level correspondences between 2D frames. We consider all pairs of frames $(i,j)$ from the RGB-D sequence. We then back-project frame $i$’s depth map $D_i$ to camera space, and transform the points into the world space by $T_i$. The depth values of frame $j$ are similarly transformed into world space. Pixel correspondences between the two frames are then determined as those whose 3D world locations like within 2cm of each other. We use the pairs of frames which have at least 30% overlap, with overlap computed as number of corresponding pixels in both frames divided by total number of points in the two frames. In total, we sample around 840k pairs of images from the ScanNet training data. In the training phase, a pair of sampled images is input to a shared 2D network backbone. In our experiments, we use a UNet-style backbone with ResNet architecture as an encoder, but note that our method is agnostic to the underlying encoder backbone. We then consider the feature map from decoder of the 2D backbone, where its size is half of the input resolution. For each image in the pair, we use the aforementioned pixel-to-pixel correspondences which refer to the same physical 3D point. Note that these correspondences may have different color values due to view-dependent lighting effects but represent the same 3D world location; additionally, the regions surrounding the correspondences appear different due to different viewing angles. In this fashion, we treat these pairs of correspondences as positive samples in contrastive learning; we use all non-matching pixels as negatives. Non-matching pixels are also defined within the set of correspondences. For a pair of frames with $n$ pairs of correspondences as positive samples, we use all $n(n-1)$ negative pairs (each of $n$ pixels from the first frame with each $n-1$ non-matching pixel from the second). Non-matching pixel-voxels are defined similarly but from a pair of frame and 3D chunk. Between the features of matching and non-matching pixel locations, we then compute a PointInfoNCE loss, which is defined as: \(L_p = -\sum_{(a,b) \in M}\log{\frac{\exp(f_a \cdot f_b / \tau)}{\sum_{(\cdot,k) \in M}\exp(f_a \cdot f_k /\tau)}}\), where $M$ is the set of pairs of pixel correspondences, ahd $f$ represents the associated feature vector of a pixel in the feature map. By leveraging multi-view correspondences, e apply implicit 3D priors-without any explicit 3D learning, we imbue view-invariance in the learned image-based features. Geometric Prior In addition to multi-view constraints, we also leverage explicit geometry-color correspondences inherent to the RGB-D data during training. For an RGB-D train sequence, the geometry-color correspondences are given by associating the surface reconstruction $S$ with the RGB frames of the sequence. For each frame $i$, we compute its view frustum in the world space. A volumetric chuck $V_i$ of $S$ is then cropped from the axis-aligned bounding box of the view frustum. We represent $V_i$ as a 2cm resolution volumetric occupancy grid from the surface. We thus consider pairs of color frames and geometric chuncks $(C_i, V_i)$. From the color-geometry pairs $(C_i, V_i)$, we compute pixel-voxel correspondences by projecting the depth values for each pixel in the corresponding frame $D_i$ into world space to find an associated occupied voxel in $V_i$ that lies within 2cm of the 3D location of the pixel. During training, we leverage the color-geometry correspondences with a 2D network backbone and a 3D network backbone. We use a UNet-style architecture with ResNet encoder for the 2D network backbone, and a UNet-sytle sparse convolutional 3D network backbone. Similarly to view-invariant training, we also take the output from the decoder of the 2D network backbone where its output size is half the input resolution. We then use the pixel-voxel correspondences in $(C_i, V_i)$ for contrastive learning, with positives as all matching pixel-voxel pairs and negatives as all non-matching pixel-voxel pairs. We apply the PointInfoNCE loss with $f_i$ as the 2D feature of a pixel, and $f_j$ is the feature vector from its 3D correspondence, and $M$ the set of 2D-3D pixel-voxel correspondences pairs. Joint Learning We can leverage not only the view-invariant constraints and geometric priors during training, but also learn jointly from the combination of both constraints. We can thus employ a shared 2D network backbone and a 3D network backbone, with the 2D network backbone constrained by both view-invariant constraints and as the 2D part of geometric prior constraint. During training, we consider $(C_i, C_j, V_i, V_j)$ of overlapping color frames $C_i$ and $C_j$ as well as $V_i$ and $V_j$ which have geometric correspondence with $C_i$, $C_j$ respectively. The shared 2D network backbone then processes $C_i$, $C_j$ and computes the view-invariant loss from Section 3.1. At the same time, $V_i$ and $V_j$ are processed by the 3D sparse convolutional backbone, with the loss relative to the features of $C_i$ and $C_j$ respectively. This embeds both constraints into the learned 2D representation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/pri3d.png" /><media:content medium="image" url="https://github.com/assets/images/pri3d.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">RandomRooms: Unsupervised Pre-training from Synthetic Shapes and</title><link href="https://github.com/random-rooms/" rel="alternate" type="text/html" title="RandomRooms: Unsupervised Pre-training from Synthetic Shapes and" /><published>2021-08-27T00:00:00+09:00</published><updated>2021-08-27T00:00:00+09:00</updated><id>https://github.com/random-rooms</id><content type="html" xml:base="https://github.com/random-rooms/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;3D point cloud understanding has made great progress in recent years. However, one major bottleneck is the scarcity of annotated real datasets, especially compared to 2D object detection tasks, since a large amount of labor is involved in annotating the real scans of a scene. A promising solution to this problem is to make better use of the synthetic dataset, which consists of CAD object models, to boost the learning on real datasets. This can be achieved by the pre-training and fine-tuning procedure. However, recent work on 3D pre-training exhibits failure when transfer features learned on synthetic objects to other real-world applications. In this work, we put forward a new method called RandomRooms to accomplish this objective. In particular, we propose to generate random layouts of a scene by making use of the objects in the synthetic CAD dataset and learn the 3D scene representation by applying object-level contrastive learning on two random scenes generated from the same set of synthetic objects. The model pre-trained in this way can serve as a better initialization when later fine-tuning on the 3D object detection task. Empirically, we show consistent improvement in downstream 3D detection tasks on several base models, especially when less training data are used, which strongly demonstrates the effectiveness and generalization of our method. Benefiting from the rich semantic knowledge and diverse objects from synthetic data, our method establishes the new state-of-the-art on widely-used 3D detection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide a new perspective for bridging object and scene-level 3D understanding.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rr-title.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The main idea of &lt;em&gt;RandomRooms&lt;/em&gt;. To generate two different layouts, we randomly place the same set of objects sampled from synthetic datasts in rectangular rooms. With the proposed object-level contrastive learning, models pretrained on these pseudo scenes can serve as a better initialization for downstream 3D object detection task.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recent years have witnessed great progress in 3D deep learning, especially on 3D point clouds. With the emergence of powerful models, we are now able to make significant breakthroughs on many point cloud tasks, ranging from object-level understanding ones to scene-level understanding ones, such as 3D object detection and 3D semantic segmentation. These scene-level tasks are considered to be more complicated and more important as they often require higher level understanding compared to object level tasks like shape classification. One of the most important tasks for 3D point cloud scene understanding is the 3D object detection, which aims at localizing the objects of interest in the point cloud of the scene and telling the category they belong to. However, one major bottleneck that hinders the researchers from moving forward is the lack of large-scale real datasets, considering the difficulty in collecting and labeling high-quality 3D scene data. Compared to 2D object detection task where we have large annotated real datasets COCO, the real datasets here we use for 3D object detection task are much smaller in scales, and generating a synthesized scene dataset also involves a heavy workload in modeling and rendering.&lt;/p&gt;

&lt;p&gt;A preferred solution is to utilize synthetic CAD object models to help the learning of 3D object detector since it is much easier to access such type of data. Considering we have no annotation of bounding box for synthetic CAD data, this idea can be achieved in a similar way as the unsupervised pre-training for 2D vision takss where we first pretrain on a large-scale dataset in an unsupervised manner and then fine-tune on a smaller annotated dataset. Yet, most previous works focus on the pretraining for single object level tasks, such as reconstruction, shape classification or part segmentation, or on some low-level tasks like registration. A recent work, namely PointContrast, first explores the possibility of pre-training in the context of 3D representation learning for higher level scene understanding tasks, i.e. 3D detection and segmentation. Nevertheless, they conduct the pre-training on the real scene dataset and provide a failure case when pre-training the backbone model on ShapeNet, which consists of synthetic CAD object models. They attribute this unsuccessful attempt to two reasons, that is, the domain gap between real and synthetic data as well as the insufficiency of capturing point-level representation by directly training on single objects. Despite these difficulties, it is still desirable to make the ShapeNet play the role of ImageNet in 2D vision since it is easy to obtain a large number of synthetic CAD models.&lt;/p&gt;

&lt;p&gt;In this work, we put forward a new framework to show the possibility of using a synthetic CAD model dataset, i.e. ShapeNet, for the 3D pre-training before fine-tuning on downstream 3D object detection task. To this end, we propose a method named RandomRoom. In particular, we propose to generate two different layouts using one set of objects which are randomly sampled out of the ShapeNet dataset. Having these two scenes that are made up of the same set of objects, we can then perform the contrastive learning at the object level to learn the 3D scene representation.&lt;/p&gt;

&lt;p&gt;Different from PointContrast where the contrastive learning is performed at the point level, our approach has two advantages. One is to remove the requirement of point correspondence between two views, which is indispensable in PointContrast given that it is necessary to exploit such information to obtain positive and negative pairs for the contrastive learning. This requirement limits the applications of PointContrast, since the CAD model datasets like ShapeNet and many other real-world datasets like SUN RGB-D cannot provide such information. The other advantage is that our method can support more diverse backbone models. Most state-of-the-art models on tasks like 3D object detection apply PointNet++ style models as their backbone, and replacing it with Sparse Res-UNet may lead to the drop of accuracy, according to the PointContrast. However, PointContrast cannot well support the pre-training of PointNet++ style model as the UNet-like models, since the point correspondence may be missing after each abstraction level in PointNet++. With the proposed RandomRoom, we are enabled to perform contrastive learning at the level of objects and thus better support the pretraining of PointNet++ like models as we no longer need to keep the point correspondence for contrastive learning like PointContrast.&lt;/p&gt;

&lt;p&gt;Our method is straightforward yet effective. We conduct the experiments on the 3D object detection task where only the geometric information is available for input as the models in CAD datasets do not carry color information. The results of empirical study strongly demonstrate the effectiveness of our method. In particular, we achieve the state-of-the-art of 3D object detection on two widely used benchmarks, ScanNetV2 and SUN-RGBD. Furthermore, our method can achieve even more improvements when much less training samples are used, demonstrating that our model can learn a better initialization for 3D object detection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;randomrooms&quot;&gt;RandomRooms&lt;/h2&gt;
&lt;p&gt;In this section, we describe the details of the proposed RandomRooms method. We first briefly review existing contrastive representation learning methods and illustrate the intuition of our method in Section 3.1. Then, we describe how to use synthetic objects to construct random rooms in 3.2. In Section 3.3, we show our pretrain task for learning scene level representation from the pseudo scenes. The overview of our framework is represented in Figure 2.&lt;/p&gt;

&lt;h3 id=&quot;overview-of-contrastive-learning&quot;&gt;Overview of Contrastive Learning&lt;/h3&gt;
&lt;p&gt;We begin by reviewing the existing contrastive representation learning methods for 2D and 3D understanding to illustrate the motivation of our method.&lt;/p&gt;

&lt;p&gt;Contrastive learning is at the core of several recent methods on unsupervised learning, which exhibits promising performance on both 2D and 3D tasks and shows impressive generalization ability as a new type of pre-training method for various downstream tasks. The key ingredient of contrastive learning is constructing positive and negative pairs to learn discriminative representation, which inherits the idea of conventional contrastive learning in metric learning literature. Given an input $x$ and its positive pair $x_{+}$ and a set of negative examples ${x_i}$, a commonly used training objective for contrastive representation learning is based on InfoNCE: $$L_{contrastive} = -log{\frac{\exp(\phi(x) \cdot \phi(x_{+})/\tau)}{\sum_i \exp(\phi(x) \cdot \phi(x_i)/\tau)}}$$ where $\phi$ is the encoder network that maps the input to a feature vector and $\tau$ is a temperature hyper-parameter. Intuitively, the contrastive learning methods supervise models by encouraging the features of the different views of the same sample to be close to each other and distinguishable from other samples. Hence the quality of positive pairs and negative examples is a critical factor is a critical factor to learn the encoder.&lt;/p&gt;

&lt;p&gt;Since category annotations are not available in the unsupervised learning scenario, a common practice is using different augmentations of an input as the positive pairs and treating all other samples as negative samples. Although this design has proven to be effective in image representation learning, we argue there is a better solution to construct positive pairs for 3D understanding. One fundamental difference between 2D and 3D data is that the spatial structures of pixels do not reflect the actual geometric structures of the objects, but the spatial structures in 3D data always faithfully illustrate the layouts in the real world. This property suggests that it may be easier to manipulate or &lt;em&gt;augment&lt;/em&gt; 3D data compared to 2D images. Inspired by the rendering techniques in computer graphics, we propose to generate positive pairs of 3D scenes by randomly manipulating the layouts of 3D objects in a scene. Since we only need 3D objects instead of the whole scene in this process, our method makes it possible to use 3D object models to promote scene level representation learning.&lt;/p&gt;

&lt;p&gt;It is worth noting that a recent work, namely PointContrast, explores 3D contrastive representation learnign by using 3D point clouds from different views as the positive pair, where a point level contrastive loss is designed. This method is baed on the multi-view point cloud sequences provided in ScanNetV2. Instead, our method focuses on leveraging object level 3D data, which are easier to collect and have more diverse categories.&lt;/p&gt;

&lt;h3 id=&quot;random-rooms-from-synthetic-objects&quot;&gt;Random Rooms from Synthetic Objects&lt;/h3&gt;
&lt;p&gt;Compared to ScanNetV2, which contains ~15k objects from 17 categories, synthetic shape datasets like ShapeNet provide a more plentiful source for 3D understanding. For example, ShapeNetCore contains ~52k objects from 55 categories. Therefore, the primary goal of this paper is to study how to use synthetic CAD models collected by ShapeNet to improve downstream tasks like 3D detection and segmentation on real-world datasets.&lt;/p&gt;

&lt;p&gt;Previous work shows that directly pre-training on ShapeNet will not yield performance improvement on downstream detection and segmentation task. We suspect the main reason is the domain gap between the single object classification taks on ShapeNet and the multiple objects localization task on real-world datsets. In order to bridge the gap, we propose to generate pseudo scenes (we name them &lt;em&gt;random rooms&lt;/em&gt;) from synthetic objects to construct the training data that are helpful ofr scene level understanding.&lt;/p&gt;

&lt;p&gt;Given a set of randomly sampled objects, we generate a random room following the three steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Object Augmentation&lt;/strong&gt;: We first resize the object to a random size in $[0.5m, 2.0m]$ to ensure the objects have similar sizes as objects in ScanNetV2. Then, we apply commonly used object point cloud augmentation techniques including rotation, point dropping, and jittering.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Layout Generation&lt;/strong&gt;: For the ease of implementation, we place objects in a rectangular room. The size of the room is adaptively adjusted according to the overall area of the augmented objects. The layout is generated based on two simple principles: 1) non-overlapping: any two objects should not occupy the same space in the room; 2) gravity: objects should not float in the air, and larger objects should not be placed over the smaller ones. In turn, we place objects in the descending order of the area. Insired by &lt;em&gt;Tetris&lt;/em&gt;, for each object, we first randomly choose a position in the X-Y plane that satisfies the above principles, then determine the location (the Z value) based on the current maximum height of the position. The object will not be placed in a position if the current maximum height of the position exceeds 2m.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scene Augmentation&lt;/strong&gt;: Lastly, we apply data augmentation like rotation along the $Z$ axis, point dropping, jittering to the whole scene. To make the generated scenes more similar to the real scenes, we also add the floor and walls as confounders.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some examples of the random rooms are illustrated in Figure 6.&lt;/p&gt;

&lt;h3 id=&quot;representation-learning-from-random-rooms&quot;&gt;Representation Learning from Random Rooms&lt;/h3&gt;
&lt;p&gt;To utilize the generated random rooms, we devise an object-level contrastive learning (OCL) method, which learns discriminative representation without category annotations.&lt;/p&gt;

&lt;p&gt;Givene $n$ randomly sampled objects $\{ x_1, x_2, \dots, x_n \}$, we first generate two random rooms $R_A = \{x^A_1, x^A_2, \dots, x^A_n\}$ and $R_B = \{x^B_1, x^B_2, \dots, x^B_n\}$ by conducting the above mentioned steps individually. Then we employ the point cloud encoder-decoder network $\mathcal{M}$ (e.g. PointNet++ with feature propagation layers) to extract per-point features of the two scenes $F_A = \mathcal{M}(R_A)$ and $F_B = \mathcal{M}(R_B)$. Since the random room is constructed by several individual objects, the instance labels can be naturally defined. The goal of object-level contrastive learning is to exploit instance labels as a source of free and plentiful supervisory signals for training a rich representation for point cloud understanding. To obtain the feature of each object, we apply the average pooling operation $\mathcal{A}$ on per-point features belonging to this object: $$ \begin{split} \{h^A_1, h^A_2, \dots, h^A_n\} = \mathcal{A}(F_A), \\ \{h^B_1, h^B_2, \dots, h^B_n\} = \mathcal{A}(F_B) \end{split}$$. Similar to the common practice in contrastive learning, the object features are projected onto a unit hypersphere using a multi-layer perceptron network (MLP) followed by L2 normalization. The object-level contrastive learning objective can be written as $$ L_{OCL} = - \frac{1}{n}\sum_{i=1}^n \log{\frac{\exp(f^A_i \cdot f^B_i /\tau)}{\sum_{f \in \mathcal{F} \exp(f^A_i \cdot f/\tau)}}} - \frac{1}{n}\sum_{i=1}^n \log{\frac{\exp(f^B_i \cdot f^A_i /\tau)}{\sum_{f \in \mathcal{F} \exp(f^B_i \cdot f/\tau)}}} $$, where $f_i^A = \phi(h_i^A)$ and $f_i^B = \phi(h_i^B)$ are the projected features of the $i$-th object in $R_A$ and $R_B$ respectively, $\phi$ is the projection head and $\mathcal{F}$ is the set of all projected features in the batch. Note that compared to point-level contrastive learning task in PointContrast, our method further utilizes the instance-level knowledge thanks to the generation mechanism of RandomRooms. We argue that objec-level contrastive learning introduces more semantic knowledge and can be more helpful for downstream localization tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rr-00.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;One primary goal of representation learning is to learn the representation that can transfer to downstream tasks. To apply our RandomRooms method to scene level understanding taks like 3D object detection, we adopt the &lt;em&gt;unsupervised pre-training&lt;/em&gt; + &lt;em&gt;supervised fine-tuning&lt;/em&gt; pipeline. Specifically, we first pre-train the backbone model on ShapeNet using our method, then we use the pre-trained weights as the initialization and further fine-tune the model on the downstream 3D object detection task.&lt;/p&gt;

&lt;h3 id=&quot;pre-training-setups&quot;&gt;Pre-training Setups&lt;/h3&gt;
&lt;p&gt;We perform the pre-training on ShapeNet, a dataset composed of richly-annotated shapes represented by 3D CAD models of objects from 55 common categories. To generate the random room, we first need to randomly sample multiple objects from the dataset. The number of objects we sample is a random integer from 12 to 18, which is similar to the average number of objects in ScanNetV2 scenes. Then for each sampled object, we perform the random room generation algorithm mentioned in Section 3.2. The object-level contrastive learning loss is used to train the model in an unsupervised manner.&lt;/p&gt;

&lt;p&gt;For the downstream 3D object detection task, we use the backbone model which take as input 40,000 points. Following the network configurations in these two works, we use the 1024-point feature as the output of the backbone models and perform contrastive learning one this feature. During pre-training, we use the Adam optimizer with initial learning 0.001. We train the model for 300 epochs and the learning rate is multiplied by 0.1 at the 100-th and 200-th epoch. The batch size is set to 16 such that roughly 200~300 unique objects are involved in the contrastive learning at every iteration.&lt;/p&gt;</content><author><name>sal</name></author><category term="Paper" /><summary type="html">Abstract 3D point cloud understanding has made great progress in recent years. However, one major bottleneck is the scarcity of annotated real datasets, especially compared to 2D object detection tasks, since a large amount of labor is involved in annotating the real scans of a scene. A promising solution to this problem is to make better use of the synthetic dataset, which consists of CAD object models, to boost the learning on real datasets. This can be achieved by the pre-training and fine-tuning procedure. However, recent work on 3D pre-training exhibits failure when transfer features learned on synthetic objects to other real-world applications. In this work, we put forward a new method called RandomRooms to accomplish this objective. In particular, we propose to generate random layouts of a scene by making use of the objects in the synthetic CAD dataset and learn the 3D scene representation by applying object-level contrastive learning on two random scenes generated from the same set of synthetic objects. The model pre-trained in this way can serve as a better initialization when later fine-tuning on the 3D object detection task. Empirically, we show consistent improvement in downstream 3D detection tasks on several base models, especially when less training data are used, which strongly demonstrates the effectiveness and generalization of our method. Benefiting from the rich semantic knowledge and diverse objects from synthetic data, our method establishes the new state-of-the-art on widely-used 3D detection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide a new perspective for bridging object and scene-level 3D understanding. The main idea of RandomRooms. To generate two different layouts, we randomly place the same set of objects sampled from synthetic datasts in rectangular rooms. With the proposed object-level contrastive learning, models pretrained on these pseudo scenes can serve as a better initialization for downstream 3D object detection task. Introduction Recent years have witnessed great progress in 3D deep learning, especially on 3D point clouds. With the emergence of powerful models, we are now able to make significant breakthroughs on many point cloud tasks, ranging from object-level understanding ones to scene-level understanding ones, such as 3D object detection and 3D semantic segmentation. These scene-level tasks are considered to be more complicated and more important as they often require higher level understanding compared to object level tasks like shape classification. One of the most important tasks for 3D point cloud scene understanding is the 3D object detection, which aims at localizing the objects of interest in the point cloud of the scene and telling the category they belong to. However, one major bottleneck that hinders the researchers from moving forward is the lack of large-scale real datasets, considering the difficulty in collecting and labeling high-quality 3D scene data. Compared to 2D object detection task where we have large annotated real datasets COCO, the real datasets here we use for 3D object detection task are much smaller in scales, and generating a synthesized scene dataset also involves a heavy workload in modeling and rendering. A preferred solution is to utilize synthetic CAD object models to help the learning of 3D object detector since it is much easier to access such type of data. Considering we have no annotation of bounding box for synthetic CAD data, this idea can be achieved in a similar way as the unsupervised pre-training for 2D vision takss where we first pretrain on a large-scale dataset in an unsupervised manner and then fine-tune on a smaller annotated dataset. Yet, most previous works focus on the pretraining for single object level tasks, such as reconstruction, shape classification or part segmentation, or on some low-level tasks like registration. A recent work, namely PointContrast, first explores the possibility of pre-training in the context of 3D representation learning for higher level scene understanding tasks, i.e. 3D detection and segmentation. Nevertheless, they conduct the pre-training on the real scene dataset and provide a failure case when pre-training the backbone model on ShapeNet, which consists of synthetic CAD object models. They attribute this unsuccessful attempt to two reasons, that is, the domain gap between real and synthetic data as well as the insufficiency of capturing point-level representation by directly training on single objects. Despite these difficulties, it is still desirable to make the ShapeNet play the role of ImageNet in 2D vision since it is easy to obtain a large number of synthetic CAD models. In this work, we put forward a new framework to show the possibility of using a synthetic CAD model dataset, i.e. ShapeNet, for the 3D pre-training before fine-tuning on downstream 3D object detection task. To this end, we propose a method named RandomRoom. In particular, we propose to generate two different layouts using one set of objects which are randomly sampled out of the ShapeNet dataset. Having these two scenes that are made up of the same set of objects, we can then perform the contrastive learning at the object level to learn the 3D scene representation. Different from PointContrast where the contrastive learning is performed at the point level, our approach has two advantages. One is to remove the requirement of point correspondence between two views, which is indispensable in PointContrast given that it is necessary to exploit such information to obtain positive and negative pairs for the contrastive learning. This requirement limits the applications of PointContrast, since the CAD model datasets like ShapeNet and many other real-world datasets like SUN RGB-D cannot provide such information. The other advantage is that our method can support more diverse backbone models. Most state-of-the-art models on tasks like 3D object detection apply PointNet++ style models as their backbone, and replacing it with Sparse Res-UNet may lead to the drop of accuracy, according to the PointContrast. However, PointContrast cannot well support the pre-training of PointNet++ style model as the UNet-like models, since the point correspondence may be missing after each abstraction level in PointNet++. With the proposed RandomRoom, we are enabled to perform contrastive learning at the level of objects and thus better support the pretraining of PointNet++ like models as we no longer need to keep the point correspondence for contrastive learning like PointContrast. Our method is straightforward yet effective. We conduct the experiments on the 3D object detection task where only the geometric information is available for input as the models in CAD datasets do not carry color information. The results of empirical study strongly demonstrate the effectiveness of our method. In particular, we achieve the state-of-the-art of 3D object detection on two widely used benchmarks, ScanNetV2 and SUN-RGBD. Furthermore, our method can achieve even more improvements when much less training samples are used, demonstrating that our model can learn a better initialization for 3D object detection. RandomRooms In this section, we describe the details of the proposed RandomRooms method. We first briefly review existing contrastive representation learning methods and illustrate the intuition of our method in Section 3.1. Then, we describe how to use synthetic objects to construct random rooms in 3.2. In Section 3.3, we show our pretrain task for learning scene level representation from the pseudo scenes. The overview of our framework is represented in Figure 2. Overview of Contrastive Learning We begin by reviewing the existing contrastive representation learning methods for 2D and 3D understanding to illustrate the motivation of our method. Contrastive learning is at the core of several recent methods on unsupervised learning, which exhibits promising performance on both 2D and 3D tasks and shows impressive generalization ability as a new type of pre-training method for various downstream tasks. The key ingredient of contrastive learning is constructing positive and negative pairs to learn discriminative representation, which inherits the idea of conventional contrastive learning in metric learning literature. Given an input $x$ and its positive pair $x_{+}$ and a set of negative examples ${x_i}$, a commonly used training objective for contrastive representation learning is based on InfoNCE: \(L_{contrastive} = -log{\frac{\exp(\phi(x) \cdot \phi(x_{+})/\tau)}{\sum_i \exp(\phi(x) \cdot \phi(x_i)/\tau)}}\) where $\phi$ is the encoder network that maps the input to a feature vector and $\tau$ is a temperature hyper-parameter. Intuitively, the contrastive learning methods supervise models by encouraging the features of the different views of the same sample to be close to each other and distinguishable from other samples. Hence the quality of positive pairs and negative examples is a critical factor is a critical factor to learn the encoder. Since category annotations are not available in the unsupervised learning scenario, a common practice is using different augmentations of an input as the positive pairs and treating all other samples as negative samples. Although this design has proven to be effective in image representation learning, we argue there is a better solution to construct positive pairs for 3D understanding. One fundamental difference between 2D and 3D data is that the spatial structures of pixels do not reflect the actual geometric structures of the objects, but the spatial structures in 3D data always faithfully illustrate the layouts in the real world. This property suggests that it may be easier to manipulate or augment 3D data compared to 2D images. Inspired by the rendering techniques in computer graphics, we propose to generate positive pairs of 3D scenes by randomly manipulating the layouts of 3D objects in a scene. Since we only need 3D objects instead of the whole scene in this process, our method makes it possible to use 3D object models to promote scene level representation learning. It is worth noting that a recent work, namely PointContrast, explores 3D contrastive representation learnign by using 3D point clouds from different views as the positive pair, where a point level contrastive loss is designed. This method is baed on the multi-view point cloud sequences provided in ScanNetV2. Instead, our method focuses on leveraging object level 3D data, which are easier to collect and have more diverse categories. Random Rooms from Synthetic Objects Compared to ScanNetV2, which contains ~15k objects from 17 categories, synthetic shape datasets like ShapeNet provide a more plentiful source for 3D understanding. For example, ShapeNetCore contains ~52k objects from 55 categories. Therefore, the primary goal of this paper is to study how to use synthetic CAD models collected by ShapeNet to improve downstream tasks like 3D detection and segmentation on real-world datasets. Previous work shows that directly pre-training on ShapeNet will not yield performance improvement on downstream detection and segmentation task. We suspect the main reason is the domain gap between the single object classification taks on ShapeNet and the multiple objects localization task on real-world datsets. In order to bridge the gap, we propose to generate pseudo scenes (we name them random rooms) from synthetic objects to construct the training data that are helpful ofr scene level understanding. Given a set of randomly sampled objects, we generate a random room following the three steps: Object Augmentation: We first resize the object to a random size in $[0.5m, 2.0m]$ to ensure the objects have similar sizes as objects in ScanNetV2. Then, we apply commonly used object point cloud augmentation techniques including rotation, point dropping, and jittering. Layout Generation: For the ease of implementation, we place objects in a rectangular room. The size of the room is adaptively adjusted according to the overall area of the augmented objects. The layout is generated based on two simple principles: 1) non-overlapping: any two objects should not occupy the same space in the room; 2) gravity: objects should not float in the air, and larger objects should not be placed over the smaller ones. In turn, we place objects in the descending order of the area. Insired by Tetris, for each object, we first randomly choose a position in the X-Y plane that satisfies the above principles, then determine the location (the Z value) based on the current maximum height of the position. The object will not be placed in a position if the current maximum height of the position exceeds 2m. Scene Augmentation: Lastly, we apply data augmentation like rotation along the $Z$ axis, point dropping, jittering to the whole scene. To make the generated scenes more similar to the real scenes, we also add the floor and walls as confounders. Some examples of the random rooms are illustrated in Figure 6. Representation Learning from Random Rooms To utilize the generated random rooms, we devise an object-level contrastive learning (OCL) method, which learns discriminative representation without category annotations. Givene $n$ randomly sampled objects $\{ x_1, x_2, \dots, x_n \}$, we first generate two random rooms $R_A = \{x^A_1, x^A_2, \dots, x^A_n\}$ and $R_B = \{x^B_1, x^B_2, \dots, x^B_n\}$ by conducting the above mentioned steps individually. Then we employ the point cloud encoder-decoder network $\mathcal{M}$ (e.g. PointNet++ with feature propagation layers) to extract per-point features of the two scenes $F_A = \mathcal{M}(R_A)$ and $F_B = \mathcal{M}(R_B)$. Since the random room is constructed by several individual objects, the instance labels can be naturally defined. The goal of object-level contrastive learning is to exploit instance labels as a source of free and plentiful supervisory signals for training a rich representation for point cloud understanding. To obtain the feature of each object, we apply the average pooling operation $\mathcal{A}$ on per-point features belonging to this object: \(\begin{split} \\{h^A_1, h^A_2, \dots, h^A_n\\} = \mathcal{A}(F_A), \\ \\{h^B_1, h^B_2, \dots, h^B_n\\} = \mathcal{A}(F_B) \end{split}\). Similar to the common practice in contrastive learning, the object features are projected onto a unit hypersphere using a multi-layer perceptron network (MLP) followed by L2 normalization. The object-level contrastive learning objective can be written as \(L_{OCL} = - \frac{1}{n}\sum_{i=1}^n \log{\frac{\exp(f^A_i \cdot f^B_i /\tau)}{\sum_{f \in \mathcal{F} \exp(f^A_i \cdot f/\tau)}}} - \frac{1}{n}\sum_{i=1}^n \log{\frac{\exp(f^B_i \cdot f^A_i /\tau)}{\sum_{f \in \mathcal{F} \exp(f^B_i \cdot f/\tau)}}}\), where $f_i^A = \phi(h_i^A)$ and $f_i^B = \phi(h_i^B)$ are the projected features of the $i$-th object in $R_A$ and $R_B$ respectively, $\phi$ is the projection head and $\mathcal{F}$ is the set of all projected features in the batch. Note that compared to point-level contrastive learning task in PointContrast, our method further utilizes the instance-level knowledge thanks to the generation mechanism of RandomRooms. We argue that objec-level contrastive learning introduces more semantic knowledge and can be more helpful for downstream localization tasks. Experiments One primary goal of representation learning is to learn the representation that can transfer to downstream tasks. To apply our RandomRooms method to scene level understanding taks like 3D object detection, we adopt the unsupervised pre-training + supervised fine-tuning pipeline. Specifically, we first pre-train the backbone model on ShapeNet using our method, then we use the pre-trained weights as the initialization and further fine-tune the model on the downstream 3D object detection task. Pre-training Setups We perform the pre-training on ShapeNet, a dataset composed of richly-annotated shapes represented by 3D CAD models of objects from 55 common categories. To generate the random room, we first need to randomly sample multiple objects from the dataset. The number of objects we sample is a random integer from 12 to 18, which is similar to the average number of objects in ScanNetV2 scenes. Then for each sampled object, we perform the random room generation algorithm mentioned in Section 3.2. The object-level contrastive learning loss is used to train the model in an unsupervised manner. For the downstream 3D object detection task, we use the backbone model which take as input 40,000 points. Following the network configurations in these two works, we use the 1024-point feature as the output of the backbone models and perform contrastive learning one this feature. During pre-training, we use the Adam optimizer with initial learning 0.001. We train the model for 300 epochs and the learning rate is multiplied by 0.1 at the 100-th and 200-th epoch. The batch size is set to 16 such that roughly 200~300 unique objects are involved in the contrastive learning at every iteration.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/demo1.jpg" /><media:content medium="image" url="https://github.com/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Welcome to Jekyll!</title><link href="https://github.com/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2019-02-04T00:00:00+09:00</published><updated>2019-02-04T00:00:00+09:00</updated><id>https://github.com/welcome-to-jekyll</id><content type="html" xml:base="https://github.com/welcome-to-jekyll/">&lt;p&gt;$$x^2 + y^2 = 2$$&lt;/p&gt;

&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.
$$x^2 + y^2 = 2$$
Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name>sal</name></author><category term="Jekyll" /><summary type="html">\[x^2 + y^2 = 2\] You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated. To add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works. \(x^2 + y^2 = 2\) Jekyll also offers powerful support for code snippets: def print_hi(name) puts &quot;Hi, #{name}&quot; end print_hi('Tom') #=&amp;gt; prints 'Hi, Tom' to STDOUT. Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/demo1.jpg" /><media:content medium="image" url="https://github.com/assets/images/demo1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Powerful things you can do with the Markdown editor</title><link href="https://github.com/powerful-things-markdown-editor/" rel="alternate" type="text/html" title="Powerful things you can do with the Markdown editor" /><published>2019-02-03T00:00:00+09:00</published><updated>2019-02-03T00:00:00+09:00</updated><id>https://github.com/powerful-things-markdown-editor</id><content type="html" xml:base="https://github.com/powerful-things-markdown-editor/">&lt;p&gt;There are lots of powerful things you can do with the Markdown editor. If you’ve gotten pretty comfortable with writing in Markdown, then you may enjoy some more advanced tips about the types of things you can do with Markdown!&lt;/p&gt;

&lt;p&gt;As with the last post about the editor, you’ll want to be actually editing this post as you read it so that you can see all the Markdown code we’re using.&lt;/p&gt;

&lt;h2 id=&quot;special-formatting&quot;&gt;Special formatting&lt;/h2&gt;

&lt;p&gt;As well as bold and italics, you can also use some other special formatting in Markdown when the need arises, for example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;del&gt;strike through&lt;/del&gt;&lt;/li&gt;
  &lt;li&gt;==highlight==&lt;/li&gt;
  &lt;li&gt;*escaped characters*&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;writing-code-blocks&quot;&gt;Writing code blocks&lt;/h2&gt;

&lt;p&gt;There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;like this&lt;/code&gt;. Larger snippets of code can be displayed across multiple lines using triple back ticks:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.my-link {
    text-decoration: underline;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you want to get really fancy, you can even add syntax highlighting using Rouge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference-lists&quot;&gt;Reference lists&lt;/h2&gt;

&lt;p&gt;The quick brown jumped over the lazy.&lt;/p&gt;

&lt;p&gt;Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference.&lt;/p&gt;

&lt;h2 id=&quot;full-html&quot;&gt;Full HTML&lt;/h2&gt;

&lt;p&gt;Perhaps the best part of Markdown is that you’re never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here’s a standard YouTube embed code as an example:&lt;/p&gt;

&lt;p&gt;&lt;iframe style=&quot;width:100%;&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Cniqsc9QfDo?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;</content><author><name>jane</name></author><category term="Jekyll" /><category term="tutorial" /><category term="summer" /><summary type="html">There are lots of powerful things you can do with the Markdown editor. If you’ve gotten pretty comfortable with writing in Markdown, then you may enjoy some more advanced tips about the types of things you can do with Markdown! As with the last post about the editor, you’ll want to be actually editing this post as you read it so that you can see all the Markdown code we’re using. Special formatting As well as bold and italics, you can also use some other special formatting in Markdown when the need arises, for example: strike through ==highlight== *escaped characters* Writing code blocks There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, like this. Larger snippets of code can be displayed across multiple lines using triple back ticks: .my-link { text-decoration: underline; } If you want to get really fancy, you can even add syntax highlighting using Rouge. Reference lists The quick brown jumped over the lazy. Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference. Full HTML Perhaps the best part of Markdown is that you’re never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here’s a standard YouTube embed code as an example:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://images.unsplash.com/photo-1528784351875-d797d86873a1?ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=750&amp;q=80" /><media:content medium="image" url="https://images.unsplash.com/photo-1528784351875-d797d86873a1?ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=750&amp;q=80" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The first mass-produced book to deviate from a rectilinear format</title><link href="https://github.com/first-mass-produced/" rel="alternate" type="text/html" title="The first mass-produced book to deviate from a rectilinear format" /><published>2019-02-02T00:00:00+09:00</published><updated>2019-02-02T00:00:00+09:00</updated><id>https://github.com/first-mass-produced</id><content type="html" xml:base="https://github.com/first-mass-produced/">&lt;p&gt;The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter.&lt;/p&gt;

&lt;p&gt;An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes.&lt;/p&gt;

&lt;p&gt;As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother.&lt;/p&gt;

&lt;p&gt;To deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter.&lt;/p&gt;

&lt;p&gt;An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes.&lt;/p&gt;

&lt;p&gt;As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother.&lt;/p&gt;</content><author><name>sal</name></author><category term="tutorial" /><category term="featured" /><summary type="html">The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter. An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”. It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story. The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes. As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother. To deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter. An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”. The claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story. The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes. As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/17.jpg" /><media:content medium="image" url="https://github.com/assets/images/17.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Education must also train one for quick, resolute and effective thinking.</title><link href="https://github.com/education/" rel="alternate" type="text/html" title="Education must also train one for quick, resolute and effective thinking." /><published>2019-02-01T00:00:00+09:00</published><updated>2019-02-01T00:00:00+09:00</updated><id>https://github.com/education</id><content type="html" xml:base="https://github.com/education/">&lt;p&gt;There are lots of powerful things you can do with the Markdown editor&lt;/p&gt;

&lt;p&gt;If you’ve gotten pretty comfortable with writing in Markdown, then you may enjoy some more advanced tips about the types of things you can do with Markdown!&lt;/p&gt;

&lt;p&gt;As with the last post about the editor, you’ll want to be actually editing this post as you read it so that you can see all the Markdown code we’re using.&lt;/p&gt;

&lt;h2 id=&quot;special-formatting&quot;&gt;Special formatting&lt;/h2&gt;

&lt;p&gt;As well as bold and italics, you can also use some other special formatting in Markdown when the need arises, for example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;del&gt;strike through&lt;/del&gt;&lt;/li&gt;
  &lt;li&gt;==highlight==&lt;/li&gt;
  &lt;li&gt;*escaped characters*&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;writing-code-blocks&quot;&gt;Writing code blocks&lt;/h2&gt;

&lt;p&gt;There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;like this&lt;/code&gt;. Larger snippets of code can be displayed across multiple lines using triple back ticks:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.my-link {
    text-decoration: underline;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you want to get really fancy, you can even add syntax highlighting using Rouge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference-lists&quot;&gt;Reference lists&lt;/h2&gt;

&lt;p&gt;The quick brown jumped over the lazy.&lt;/p&gt;

&lt;p&gt;Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference.&lt;/p&gt;

&lt;h2 id=&quot;full-html&quot;&gt;Full HTML&lt;/h2&gt;

&lt;p&gt;Perhaps the best part of Markdown is that you’re never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here’s a standard YouTube embed code as an example:&lt;/p&gt;

&lt;p&gt;&lt;iframe style=&quot;width:100%;&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Cniqsc9QfDo?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;</content><author><name>sal</name></author><category term="Jekyll" /><category term="tutorial" /><summary type="html">There are lots of powerful things you can do with the Markdown editor If you’ve gotten pretty comfortable with writing in Markdown, then you may enjoy some more advanced tips about the types of things you can do with Markdown! As with the last post about the editor, you’ll want to be actually editing this post as you read it so that you can see all the Markdown code we’re using. Special formatting As well as bold and italics, you can also use some other special formatting in Markdown when the need arises, for example: strike through ==highlight== *escaped characters* Writing code blocks There are two types of code elements which can be inserted in Markdown, the first is inline, and the other is block. Inline code is formatted by wrapping any word or words in back-ticks, like this. Larger snippets of code can be displayed across multiple lines using triple back ticks: .my-link { text-decoration: underline; } If you want to get really fancy, you can even add syntax highlighting using Rouge. Reference lists The quick brown jumped over the lazy. Another way to insert links in markdown is using reference lists. You might want to use this style of linking to cite reference material in a Wikipedia-style. All of the links are listed at the end of the document, so you can maintain full separation between content and its source or reference. Full HTML Perhaps the best part of Markdown is that you’re never limited to just Markdown. You can write HTML directly in the Markdown editor and it will just work as HTML usually does. No limits! Here’s a standard YouTube embed code as an example:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/3.jpg" /><media:content medium="image" url="https://github.com/assets/images/3.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Accumulated experience of social living</title><link href="https://github.com/acumulated-experience/" rel="alternate" type="text/html" title="Accumulated experience of social living" /><published>2019-01-30T00:00:00+09:00</published><updated>2019-01-30T00:00:00+09:00</updated><id>https://github.com/acumulated-experience</id><content type="html" xml:base="https://github.com/acumulated-experience/">&lt;p&gt;The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes.&lt;/p&gt;

&lt;p&gt;As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother.&lt;/p&gt;

&lt;p&gt;The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter.&lt;/p&gt;

&lt;p&gt;An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>sal</name></author><category term="Jekyll" /><category term="tutorial" /><summary type="html">The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes. As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original - which sees Little Red Riding Hood being gobbled up as well as her grandmother - is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother. The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter. An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”. It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/15.jpg" /><media:content medium="image" url="https://github.com/assets/images/15.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">About Bundler</title><link href="https://github.com/about-bundler/" rel="alternate" type="text/html" title="About Bundler" /><published>2019-01-29T00:00:00+09:00</published><updated>2019-01-29T00:00:00+09:00</updated><id>https://github.com/about-bundler</id><content type="html" xml:base="https://github.com/about-bundler/">&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gem install bundler&lt;/code&gt; installs the bundler gem through RubyGems. You only need to install it once - not every time you create a new Jekyll project. Here are some additional details:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundler&lt;/code&gt; is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gemfile&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gemfile.lock&lt;/code&gt; files inform &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bundler&lt;/code&gt; about the gem requirements in your site. If your site doesn’t have these Gemfiles, you can omit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec&lt;/code&gt; and just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run jekyll serve&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When you run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bundler&lt;/code&gt; uses the gems and versions as specified in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Gemfile.lock&lt;/code&gt; to ensure your Jekyll site builds with no compatibility or dependency conflicts.&lt;/p&gt;

&lt;p&gt;For more information about how to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bundler&lt;/code&gt; in your Jekyll project, this tutorial should provide answers to the most common questions and explain how to get up and running quickly.&lt;/p&gt;</content><author><name>sal</name></author><category term="Jekyll" /><summary type="html">gem install bundler installs the bundler gem through RubyGems. You only need to install it once - not every time you create a new Jekyll project. Here are some additional details: bundler is a gem that manages other Ruby gems. It makes sure your gems and gem versions are compatible, and that you have all necessary dependencies each gem requires. The Gemfile and Gemfile.lock files inform Bundler about the gem requirements in your site. If your site doesn’t have these Gemfiles, you can omit bundle exec and just run jekyll serve. When you run bundle exec jekyll serve, Bundler uses the gems and versions as specified in Gemfile.lock to ensure your Jekyll site builds with no compatibility or dependency conflicts. For more information about how to use Bundler in your Jekyll project, this tutorial should provide answers to the most common questions and explain how to get up and running quickly.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/2.jpg" /><media:content medium="image" url="https://github.com/assets/images/2.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">We all wait for summer</title><link href="https://github.com/we-all-wait-for-summer/" rel="alternate" type="text/html" title="We all wait for summer" /><published>2019-01-28T00:00:00+09:00</published><updated>2019-01-28T00:00:00+09:00</updated><id>https://github.com/we-all-wait-for-summer</id><content type="html" xml:base="https://github.com/we-all-wait-for-summer/">&lt;p&gt;This is changed. As I engage in the so-called “bull sessions” around and about the school, I too often find that most college men have a misconception of the purpose of education. Most of the “brethren” think that &lt;a href=&quot;#&quot;&gt;education should equip&lt;/a&gt; them with the proper instruments of exploitation so that they can forever trample over the masses. Still others think that education should furnish them with noble ends rather than means to an end.&lt;/p&gt;

&lt;p&gt;It seems to me that education has a two-fold function to perform in the life of man and in society: the one is utility and the other is culture. Education must enable a man to become more efficient, to achieve with increasing facility the ligitimate goals of his life.&lt;/p&gt;</content><author><name>jane</name></author><category term="Jekyll" /><category term="tutorial" /><category term="featured" /><summary type="html">This is changed. As I engage in the so-called “bull sessions” around and about the school, I too often find that most college men have a misconception of the purpose of education. Most of the “brethren” think that education should equip them with the proper instruments of exploitation so that they can forever trample over the masses. Still others think that education should furnish them with noble ends rather than means to an end. It seems to me that education has a two-fold function to perform in the life of man and in society: the one is utility and the other is culture. Education must enable a man to become more efficient, to achieve with increasing facility the ligitimate goals of his life.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/1.jpg" /><media:content medium="image" url="https://github.com/assets/images/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Tree of Codes</title><link href="https://github.com/tree-of-codes/" rel="alternate" type="text/html" title="Tree of Codes" /><published>2019-01-27T00:00:00+09:00</published><updated>2019-01-27T00:00:00+09:00</updated><id>https://github.com/tree-of-codes</id><content type="html" xml:base="https://github.com/tree-of-codes/">&lt;p&gt;The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter.&lt;/p&gt;

&lt;p&gt;As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original — which sees Little Red Riding Hood being gobbled up as well as her grandmother — is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”.&lt;/p&gt;

&lt;p&gt;The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes.&lt;/p&gt;</content><author><name>sal</name></author><category term="Jekyll" /><category term="tutorial" /><summary type="html">The first mass-produced book to deviate from a rectilinear format, at least in the United States, is thought to be this 1863 edition of Red Riding Hood, cut into the shape of the protagonist herself with the troublesome wolf curled at her feet. Produced by the Boston-based publisher Louis Prang, this is the first in their “Doll Series”, a set of five “die-cut” books, known also as shape books — the other titles being Robinson Crusoe, Goody Two-Shoes (also written by Red Riding Hood author Lydia Very), Cinderella, and King Winter. As for this particular rendition of Charles Perrault’s classic tale, the text and design is by Lydia Very (1823-1901), sister of Transcendentalist poet Jones Very. The gruesome ending of the original — which sees Little Red Riding Hood being gobbled up as well as her grandmother — is avoided here, the gore giving way to the less bloody aims of the morality tale, and the lesson that one should not disobey one’s mother. It would seem the claim could also extend to die cut books in general, as we can’t find anything sooner, but do let us know in the comments if you have further light to shed on this! Such books are, of course, still popular in children’s publishing today, though the die cutting is not now limited to mere outlines, as evidenced in a beautiful 2014 version of the same Little Red Riding Hood story. An 1868 Prang catalogue would later claim that such “books in the shape of a regular paper Doll… originated with us”. The die cut has also been employed in the non-juvenile sphere as well, a recent example being Jonathan Safran Foer’s ambitious Tree of Codes.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://github.com/assets/images/2.jpg" /><media:content medium="image" url="https://github.com/assets/images/2.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>