<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Mundana Free Jekyll Theme | Seri Lee Blog</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Mundana Free Jekyll Theme | Seri Lee Blog</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Mundana Free Jekyll Theme">
<meta property="og:locale" content="en_US">
<meta name="description" content="A great Jekyll theme developed by Sal @wowthemesnet.">
<meta property="og:description" content="A great Jekyll theme developed by Sal @wowthemesnet.">
<link rel="canonical" href="http://localhost:4000/">
<meta property="og:url" content="http://localhost:4000/">
<meta property="og:site_name" content="Seri Lee Blog">
<link rel="next" href="http://localhost:4000/page2">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Mundana Free Jekyll Theme">
<script type="application/ld+json">
{"description":"A great Jekyll theme developed by Sal @wowthemesnet.","headline":"Mundana Free Jekyll Theme","url":"http://localhost:4000/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"@type":"WebSite","name":"Seri Lee Blog","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <link rel="stylesheet" href="/assets/css/custom.css">
    
    <!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">-->
    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="/assets/js/lib.js" defer></script>
    <script> 
	    MathJax = {
		    loader: {load: ['input/tex', 'ui/menu']},
		    tex: {
			    inlineMath: {'[+]': [['$','$']]},
			    displayMath: {'[+]':[['$$', '$$']]}
			}
	    }
   </script>
    <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
 <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>
  <script>
    function convert() {
      //
      //  Get the MathML input string, and clear any previous output
      //
      var input = document.getElementById("input").value.trim();
      output = document.getElementById('output');
      output.innerHTML = '';
      //
      //  Convert the MathMl to an HTML node and append it to the output
      //
      output.appendChild(MathJax.mathml2chtml(input));
      //
      //  Then update the document to include the adjusted CSS for the
      //    content of the new equation.
      //
      MathJax.startup.document.clear();
      MathJax.startup.document.updateDocument();
    }
  </script> 
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body class=" homefirstpage ">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Seri Lee Blog</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Author</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>




            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/_posts/2021-08-31-d4.html",
    "title": "Depth from Disparity",
    "body": "We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 1,
    "url": "http://localhost:4000/_posts/2021-09-04-d3.html",
    "title": "Depth from Disparity",
    "body": "We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 2,
    "url": "http://localhost:4000/_posts/2021-09-05-d2.html",
    "title": "Depth from Disparity",
    "body": "We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 3,
    "url": "http://localhost:4000/_posts/2021-09-05-disparity.html",
    "title": "Depth from Disparity",
    "body": "We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 4,
    "url": "http://localhost:4000/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 5,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": "Made by Seri @porfolio. "
    }, {
    "id": 6,
    "url": "http://localhost:4000/author-seri.html",
    "title": "Sal",
    "body": "                        Sal /span&gt;&lt;/h2&gt;        https://sites. google. com/snu. ac. kr/sally20921porfolio         Hi, I am Seri, the author of this blog.       &lt;/div&gt;                        &lt;/div&gt;    Posts by Sal:                   		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Self-Supervised Depth Estimation	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Dissecting the Camera Matrix (Part 2)	: 		  In this blog post, we will study the intrinsic camera matrix. We will examine two equivalent interpretations: as a description of the virtual camera’s geometry and a sequence of simpl. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Dense Depth Using Stereo Vision	: 		  What exactly is this all-important dense depth that stereo vision makes possible? When we talk about dense depth, we mean we can take the video we capture from our car's cameras and t. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Dissecting the Camera Matrix (Part 1)	: 		  This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices, and and I will try to untangle the issues that can crop-up with different coord. . . 	 			In 				computer vision, 								Sep 05, 2021						            		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 04, 2021						            		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						            		Depth Estimation: Basics and Intuition	: 		  IntroductionIn computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting. . . 	 			In 				computer vision, 								Sep 02, 2021						            		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						            		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Aug 31, 2021						            		3D Rigid Body Motion (Part 1)	: 		   The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle. IntroductionIn this arti. . . 	 			In 				computer vision, 								Aug 31, 2021						            		Homogeneous Coordinates and Projective Geometry	: 		  IntroductionIn this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry. Termin. . . 	 			In 				computer vision, 								Aug 31, 2021						            		3D CNN	: 		  def print_hi(name) puts  Hi, #{name} endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT. 	 			In 				deep learning, 								Aug 31, 2021						        &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;: "
    }, {
    "id": 7,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                             Seri :       (View Posts)      Hi, I am Seri, the author of this blog.                           &nbsp;       &nbsp;                                      "
    }, {
    "id": 8,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories               deep learning:                                  		3D CNN	: 		  def print_hi(name) puts  Hi, #{name} endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT. 	 			In 				deep learning, 								Aug 31, 2021						                              computer vision:                                  		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Self-Supervised Depth Estimation	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Dissecting the Camera Matrix (Part 2)	: 		  In this blog post, we will study the intrinsic camera matrix. We will examine two equivalent interpretations: as a description of the virtual camera’s geometry and a sequence of simpl. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Dense Depth Using Stereo Vision	: 		  What exactly is this all-important dense depth that stereo vision makes possible? When we talk about dense depth, we mean we can take the video we capture from our car's cameras and t. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Dissecting the Camera Matrix (Part 1)	: 		  This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices, and and I will try to untangle the issues that can crop-up with different coord. . . 	 			In 				computer vision, 								Sep 05, 2021						                                 		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 04, 2021						                                 		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						                                 		Depth Estimation: Basics and Intuition	: 		  IntroductionIn computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting. . . 	 			In 				computer vision, 								Sep 02, 2021						                                 		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						                                 		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Aug 31, 2021						                                 		3D Rigid Body Motion (Part 1)	: 		   The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle. IntroductionIn this arti. . . 	 			In 				computer vision, 								Aug 31, 2021						                                 		Homogeneous Coordinates and Projective Geometry	: 		  IntroductionIn this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry. Termin. . . 	 			In 				computer vision, 								Aug 31, 2021						                                             Featured:    				                                          Depth from Disparity                          In                     computer vision,                                                                                           Depth from Disparity                          In                     computer vision,                                                                                           Self-Supervised Depth Estimation                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 2)                          In                     computer vision,                                                                                           Dense Depth Using Stereo Vision                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 1)                          In                     computer vision,                                                                                           Depth from Disparity                          In                     computer vision,                                                                                           Inverse Projection Transformation                          In                     computer vision,                                                                                           Depth Estimation: Basics and Intuition                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 2)                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 1)                          In                     computer vision,                                                                                           3D CNN                          In                     deep learning,                                                                   "
    }, {
    "id": 9,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to Seri Lee Blog. We will reply as soon as possible!   "
    }, {
    "id": 10,
    "url": "http://localhost:4000/",
    "title": "Mundana Free Jekyll Theme",
    "body": "                                  Depth from Disparity  :       We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include st. . .               In                 computer vision,                                        Sep 05, 2021                                                                                                                             Depth from Disparity          :                       In                         computer vision,                                                                  Sep 05, 2021                                                                                                                                     Self-Supervised Depth Estimation          :                       In                         computer vision,                                                                  Sep 05, 2021                                                                                                                                    Dissecting the Camera Matrix (Part 2)          :                       In                         computer vision,                                                                  Sep 05, 2021                                                            Depth from Disparity                  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include st. . .                 Read More            	                                Homogeneous Coordinates and Projective Geometry                  IntroductionIn this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time t. . .                 Read More            	            &lt;/div&gt;             All Stories:                   		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                  		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                  		Self-Supervised Depth Estimation	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 05, 2021						                  		Dissecting the Camera Matrix (Part 2)	: 		  In this blog post, we will study the intrinsic camera matrix. We will examine two equivalent interpretations: as a description of the virtual camera’s geometry and a sequence of simpl. . . 	 			In 				computer vision, 								Sep 05, 2021						                  		Dense Depth Using Stereo Vision	: 		  What exactly is this all-important dense depth that stereo vision makes possible? When we talk about dense depth, we mean we can take the video we capture from our car's cameras and t. . . 	 			In 				computer vision, 								Sep 05, 2021						                  		Dissecting the Camera Matrix (Part 1)	: 		  This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices, and and I will try to untangle the issues that can crop-up with different coord. . . 	 			In 				computer vision, 								Sep 05, 2021						                  		Depth from Disparity	: 		  We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e. . . 	 			In 				computer vision, 								Sep 04, 2021						                  		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						                  		Depth Estimation: Basics and Intuition	: 		  IntroductionIn computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting. . . 	 			In 				computer vision, 								Sep 02, 2021						                  		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						                                                &laquo;                              1                               2                              Next &raquo;                                          Featured:    				                                          Depth from Disparity                          In                     computer vision,                                                                                           Depth from Disparity                          In                     computer vision,                                                                                           Self-Supervised Depth Estimation                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 2)                          In                     computer vision,                                                                                           Dense Depth Using Stereo Vision                          In                     computer vision,                                                                                           Dissecting the Camera Matrix (Part 1)                          In                     computer vision,                                                                                           Depth from Disparity                          In                     computer vision,                                                                                           Inverse Projection Transformation                          In                     computer vision,                                                                                           Depth Estimation: Basics and Intuition                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 2)                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 1)                          In                     computer vision,                                                                                           3D CNN                          In                     deep learning,                                                               &lt;/div&gt; "
    }, {
    "id": 11,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 12,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 13,
    "url": "http://localhost:4000/page2/",
    "title": "Mundana Free Jekyll Theme",
    "body": "  {% if page. url ==  /  %}            {% assign latest_post = site. posts[0] %}          &lt;div class= topfirstimage  style= background-image: url({% if latest_post. image contains  ://  %}{{ latest_post. image }}{% else %} {{site. baseurl}}/{{ latest_post. image}}{% endif %}); height: 200px;  background-size: cover;  background-repeat: no-repeat; &gt;&lt;/div&gt;           {{ latest_post. title }}  :       {{ latest_post. excerpt | strip_html | strip_newlines | truncate: 136 }}               In         {% for category in latest_post. categories %}        {{ category }},         {% endfor %}                                {{ latest_post. date | date: '%b %d, %Y' }}                            {%- assign second_post = site. posts[1] -%}                        {% if second_post. image %}                         &lt;img class= w-100  src= {% if second_post. image contains  ://  %}{{ second_post. image }}{% else %}{{ second_post. image | absolute_url }}{% endif %}  alt= {{ second_post. title }} &gt;                        {% endif %}                                    {{ second_post. title }}          :                       In             {% for category in second_post. categories %}            {{ category }},             {% endfor %}                                                      {{ second_post. date | date: '%b %d, %Y' }}                                    {%- assign third_post = site. posts[2] -%}                        {% if third_post. image %}                         &lt;img class= w-100  src= {% if third_post. image contains  ://  %}{{ third_post. image }}{% else %}{{site. baseurl}}/{{ third_post. image }}{% endif %}  alt= {{ third_post. title }} &gt;                        {% endif %}                                    {{ third_post. title }}          :                       In             {% for category in third_post. categories %}            {{ category }},             {% endfor %}                                                      {{ third_post. date | date: '%b %d, %Y' }}                                    {%- assign fourth_post = site. posts[3] -%}                        {% if fourth_post. image %}                        &lt;img class= w-100  src= {% if fourth_post. image contains  ://  %}{{ fourth_post. image }}{% else %}{{site. baseurl}}/{{ fourth_post. image }}{% endif %}  alt= {{ fourth_post. title }} &gt;                        {% endif %}                                    {{ fourth_post. title }}          :                       In             {% for category in fourth_post. categories %}            {{ category }},             {% endfor %}                                                      {{ fourth_post. date | date: '%b %d, %Y' }}                                  {% for post in site. posts %} {% if post. tags contains  sticky  %}                    {{post. title}}                  {{ post. excerpt | strip_html | strip_newlines | truncate: 136 }}                 Read More            	            {% endif %}{% endfor %} {% endif %}             All Stories:         {% for post in paginator. posts %}          {% include main-loop-card. html %}        {% endfor %}                   {% if paginator. total_pages &gt; 1 %}              {% if paginator. previous_page %}        &laquo; Prev       {% else %}        &laquo;       {% endif %}       {% for page in (1. . paginator. total_pages) %}        {% if page == paginator. page %}        {{ page }}        {% elsif page == 1 %}        {{ page }}        {% else %}        {{ page }}        {% endif %}       {% endfor %}       {% if paginator. next_page %}        Next &raquo;       {% else %}        &raquo;       {% endif %}            {% endif %}                     {% include sidebar-featured. html %}      &lt;/div&gt; "
    }, {
    "id": 14,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 15,
    "url": "http://localhost:4000/disparity.html",
    "title": "Depth from Disparity",
    "body": "2021/09/05 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 16,
    "url": "http://localhost:4000/d2.html",
    "title": "Depth from Disparity",
    "body": "2021/09/05 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 17,
    "url": "http://localhost:4000/ssl.html",
    "title": "Self-Supervised Depth Estimation",
    "body": "2021/09/05 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 18,
    "url": "http://localhost:4000/intrinsic.html",
    "title": "Dissecting the Camera Matrix (Part 2)",
    "body": "2021/09/05 - In this blog post, we will study the intrinsic camera matrix. We will examine two equivalent interpretations: as a description of the virtual camera’s geometry and a sequence of simple 2D transformations.   The Intrinsic Matrix : The intrinsic matrix is paramterized as \(K = \begin{bmatrix} f_x &amp; s&amp; x_o \\ 0 &amp; f_y &amp; y_o \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) Each intrinsic parameter describes a geometric property of the camera. Let’s examine each of these properties in defail.  Focal Length : The focal length is the distance between the pinhole and the film (a. k. a the image plane). For reasons that we will discuss later, the focal length is measured in pixels. In a true pinhole camera, both $f_x$ and $f_y$ have the same value, which is illustrated as $f$ in the above figure. In practice, $f_x$ and $f_y$ can differ for a number of reasons: the camera's lens introduces unintentional distortion the camera uses an anamorphic format, where the lens compresses a widescreen scene into a standard-sized sensor.  flaws in the digital camera sensor. In all of these cases, the resulting image has non-square pixels. Having two different focal lengths sadly isn’t intuitive, so some texts use a single focal length and an aspect ratio that describes the amount of deviation from a perfectly squared pixel. Such a parameterization nicely separates the camera geometry (i. e. focal length) from distortion (aspect ratio).  Principal Point Offset a. k. a $x_0$ and $y_0$ : The camera’s principal axis is the line perpendicular to the image plane that passes through the pinhole. Its intersection with the image plane is referred to as the principal point . The principal point offset is the location of the principal point relative to film’s origin. The exact definition dependson which convention is used for the location of the origin; the illustration above assumes it’s at the bottom-left off the film. Notice that the box surrounding the camera is irrelevant, only the pinholes position relative to the film matters.  Axis Skew $s$ and Other Geometric Properties : Axis skew causes shear distortion in the projected image. Apparently some digitalization processees can cause nonzero skew. The focal length and principal point offset amount to simple translation of the film relative to the pinhole. What about rotating or scaling the film? Rotating the film around the pinhole is equivalent to rotating the camera itself, which is handled by the extrinsic matrix. Rotating the film around any other fixed point $x$ is equivalent to rotating around the pinhole $P$, then translating by $(x-P)$. What about scaling? It should be obvious that doubling all camera dimensions (film size and focal length) has no effect on the captured scene. If, instead, you double the film size and not the focal length, it is equivalent to doubling both and then halving the focal length. Thus, representing the film’s scale explicitly would be redundant. It is captured by focal length.  The Camera Frustom : Focal Length: From Pixels to World Units : The above discussion of camera-scaling shows that there are infinite number of pinhole cameras that produce the same image. The intrinsic matrix is only concerened with the relationship between camera coordinates and image coordinates, so the absolute camera dimensions are irrelevant.  Using pixel units for focal length and principal point allows us to represent the relative dimensions fo the camera, namely, the film’s position relative to its size in pixels. Another way to say this is that the intrinsic camera transformation is invariant to uniform scaling of the camera geometry . By representing dimensions in pixel units, we naturally capture this invariance. You can use similar triangles to convert pixel units to world units if you know at least one camera dimension in world units. For example, if you know the camera’s film (or digital sensor) has a width of $W$ in millimiters, the image width in pixels $w$, you can convert the focal length $f_x$ to world units using [F_x:f_x = W: w  F_x = f_x \frac{W}{w}] Other parameters $f_y$, $x_0$ and $y_0$ can be converted to their world-unit counterpart using similar equations: [F_y = f_y \frac{H}{h} X_O = x_0 \frac{W}{w} Y_0 = y_0 \frac{H}{h}] The Camera Frustum: A Pinhole Camera Made Simple : References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 19,
    "url": "http://localhost:4000/dense.html",
    "title": "Dense Depth Using Stereo Vision",
    "body": "2021/09/05 -  Stereo Vision Unlocks Dense Depth :  Sparse Depth Provided by LiDAR (left) and Dense Depth which Stereo Vision Gives Us (right) What exactly is this all-important dense depth that stereo vision makes possible? When we talk about dense depth, we mean we can take the video we capture from our car’s cameras and tell you exactly how far away everything (by everything, every pixel) is. This contrasts with the sparse depth you get from LiDAR, where highly accurate depth calculations are achieved but only for a small fraction of the visible points in a video.  Dense Depth Matters, Because Safety Matters : Dense depth is a must. Why? First up, seeing in 3D is essential for accurately classifying what we see in a scene. Secondly, seeing with stereo vision’s dense depth allows us to better classify the entirety of a complex scene and its unpredictable road layouts. In complex city environments, the sparse quality of other methods can lead to missing or indeterminate details. There can be areas in the scene whre it’s hard, or impossible to determine the depth due to something blocking or partially blocking the sensors’ view of that area. For example, one car may be partially obscured behind another. These partial occlussions make it harder to understand the scene semantically. Dense depth gives us answers. Third, dense depth also helps us combat wet, grey European weather. Rain and snow produce noise effects that obscure the scene. Dense depth allows to better overcome this inference.  How Does It Work? : The cameras on our car are set up in pairs, left and right, with both cameras in the pair pointing in the same direction. The difference in position of the cameras gives them slightly different views of the world. We use this difference to estimate the distance to objects in the scene. Let’s see how this works. For example, find a scene around you that’s going to stay fairly stationary for the next couple of minutes-one with some close objects, and others further away. Look at one object that’s further away, and as you do so, cover one eye then the other. Try to notice how the closer objects move from left to right as you switch eyes. We calculate how far away an object is by measuring those changes in the horizontal position of close objects in relation to far objects. The larger the change in horizontal position, the close the object must be. If an object hardly moves at all, it must be very far away. We calibrate our cameras so that, if an object is infinitely far away it will appear in teh same pixel in both left and right cameras. By matching the pixels in the image from left hand camera corresponding to the same object in the right hand camera, we can measure the difference in the position of those two pixels in teh two frames. We can then take this disparity and calculate the distance to the object. References:  article from medium "
    }, {
    "id": 20,
    "url": "http://localhost:4000/camera.html",
    "title": "Dissecting the Camera Matrix (Part 1)",
    "body": "2021/09/05 - This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices, and and I will try to untangle the issues that can crop-up with different coordinate conventions.   Camera Calibration and Decomposition : Primarily, camera calibration is about finding the quantities internal to the camera that affect the imaging process. Here are some of the factors that will be taken care of:  image center : we need to find the position of the image center in the image. Wait, isn't the image center located at $(width/2, height/2)$? Well, no. Unless we calibrate the camera, the image will almost always appear to be off-center.  focal length : this is a very important parameter. Remember how people using DSLR cameras tend to focus on things before capturing the image? this parameter is directly related to the focus of the camera and it is very critical.  scaling factors : the scaling factors for row pixels and column pixels might be different. If we don't take care of this thing, the image will look stretched (either horizontally or vertically).  skew factor: this refers to shearing. the image will look like a parallelogram otherwise. &lt;/li&gt; lens distortion: this refers to the pseudo zoom effect that we see near the center of any image. &lt;/li&gt; shearing refers to a transformation in which all points along a given line $L$ remain fixed while other points are shifted parallel to $L$ by a distance proportional to their perpendicular distance from $L$.  Pinhole Camera Model : &lt;/picutre&gt;Before we jump into anything, let's see where this all began. When we capture an image, we are basically mapping the 3D scene to a 2D scene. It means that every point in the 3D world gets mapped to the 2D plane of our image. This is called the pinhole camera model . It basically describes the relationship between the coordinates of the 3D point and its projection on the 2D image. This, of course, is the ideal case where there is absolutely no distortion of any kind. Every camera is modeled based on this, and every camera aspires to simulate this as close as possible. But in the real world, we have to deal with things like geometric distortions, blurring, finite sized apertures, etc. The figure shown here depicts a pinhole camera model. The camera is placed at the origin $O$. The point $P$ represents a point in the real world. We are trying to capture that onto a 2D plane. The image plane represents the 2D plane that you get after capturing the image. The image plane actually contains the image that you see after capturing a picture. So basically, we are trying to map every 3D point to a point on the image plane. In this case, the point $P$ gets mapped to $P_c$. The distance between the origin $O$ and this image plane is called the focal length of the camera. This is the parameter you modify when you adjst the focus of the camera.  Intrinsic and Extrinsic Parameters : In the above figure, we want to estimate $(u,v)$ from $(X,Y,Z)$. Let's say the focal length is denoted by $f$. If you look at the triangle formed using the origin-$P_c$-and the $Z$-axis with the origin-$P$ and $Z$-axis, you will notice that they are similar triangles. This means that $u$ depends on the $f$, $X$ and $Z$. Similarly, $v$ depends on $f$, $Y$ and $Z$. $$u = fX/Zv = fY/Z$$Next, if the origin of the 2D image coordinate system does not coincide with where the $Z$-axis intersects the image plane, we need to translate $P_c$ into the desired origin. Let this translation be defined by $(t_u, t_v)$. So now, $u$ and $v$ are given by:$$u = fX/Z + t_uv = fY/Z + t_v$$So up until now, we have something that can translate $(X,Y,Z)$ to $(u,v)$. Let's denote this matrix $M$. So we can write:$$ P_c = MP$$Since this is a camera image, we need to express it in inches. For this, we will need to know the resolution of the camera in pixels/inch. If the pixels are square the resolution will be identical in both $u$ and $v$ directions of the camera image coordinates. However, for a more general case, we assume rectangular pixels with resolution $m_u$ and $m_v$ pixels/inch in $u$ and $v$ directions respectively. Therefore, to measure $P_c$ in pixels, its $u$ and $v$ coordinates should be multiplied by $m_u$ and $m_v$ respectively. So now, this new transformation matrix depends on $f, X, Y, Z, t_u, t_v, m_u, m_v$. Let's denote this by:$$P_c = KP$$Here, $K$ is called the intrinsic parameter matrix for the camera. Now, if the camera does not have its center of projection at $(0,0,0)$ and is oriented in an arbitrary fashion (not necessarily $z$-perpendicular to the image plane), then we need roation and translation to make the camera coordinate system coincide with the configuration in that pinhole camera figure. Let the rotation applied to coincide the principal axis with $Z$-axis given by a $3 \times 3$ rotation matrix $R$. Then the matrix is formed by first applying the translation followed by the rotation is given by the $3 \times 4$ matrix. $$E = \(R|RT)$$This is called the extrinsic parameter matrix for the camera . So, the complete camera transformation can now be represented as: $$ K\(R|RT) = KR\(I|T)$$Hence, $P_c$ the projection of $P$ is given by:$$P\_c = KR\(I|T)P = CP$$$C$ is a $3 \times 4$ matrix usually called the complete camera calibration matrix. So basically, camera calibration matrix is used to transform a 3D point in the real world to a 2D point on the image plane considering all the things like focal length of the camera, distortion, resolution, shifting of origin, etc.  References :  ksimek blog prateekvjoshi blog "
    }, {
    "id": 21,
    "url": "http://localhost:4000/d3.html",
    "title": "Depth from Disparity",
    "body": "2021/09/04 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 22,
    "url": "http://localhost:4000/inverse.html",
    "title": "Inverse Projection Transformation",
    "body": "2021/09/03 -  Depth and Inverse Projection : RGB + Depth = 3D Back-Projected Points When an image of a scene is captured by a camera, we lose depth information as objects and points in 3D space are mapped onto a 2D image plane. This is also known as projective transformation , in which points in the world are converted to pixels on a 2D plane. &lt;/p&gt;However, what if we want to do the inverse ? That is, we want to recover and reconstruct the scene given only 2D image. To do that, we would need to know the depth or $Z$-component of each corresponding pixels. Depth can be represented as an image as shown in the figure above, with brigther intensity denoting points further away.  In this blog post, we will take a tour and understand the mathematics and concepts of performing back-projection from 2D pixel coordinates to 3D points. We will assume that a depth map is provided to perform the 3D reconstruction. The concept that we will go through are camera calibration parameters, projective transformation using intrinsic and its inverse, and coordinate transformation between frames.  Central Projection of Pinhole Camera Model : First and foremost, understanding the geometrical model of the camera projection serves as the core idea. What we are ultimately interested in is the depth, parameter $Z$. Here, we consider the simplest pinhole camera model with no skew or distortion factor. 3D points are mapped to the image plane $(u,v) = f(X,Y,Z)$. The complete mathematical model that describes this transformation can be written as $p = K[R|t]*P$. $$s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \begin{bmatrix} f_x &amp; 0 &amp; u_0 \\ 0 &amp; f_y &amp; v_0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} r_{11} &amp; r_{12} &amp; r_{13} &amp; t{1} \\ r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2 \\ r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3\end{bmatrix} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}$$, where $p$ is the projected point on the image plane $K$ is the camera intrinsics matrix $[R|t]$ is the extrinsic parameters describing the relative transformation of the point in the world frame to the camera frame $P$ or $[X, Y, Z, 1]$ represents the 3D point expressed in a predefined world coordinate system in Euclidean space Aspect ratio scaling $s$ controls how pixels are scaled in the $x$ and $y$ direction as focal length chnanges Intrinsic Parameter Matrix :  Camera Projective Geometry The matrix $K$ is responsible for projecting 3D points to the image plane. To do that, the following quantities must be defined as:* Focal length $(f_x, f_y)$: measure the position of the image plane with respect to the camera center. * Principal point $(u_0, v_0)$; the optical center of the image plane* Skew factor: the misalignment from a square pixel if the image plane axes are not perpendicular. In our example, this is set to zero. The most common way of solving all the paramters is using the checkerboard method, where several 2D-3D correspondences obtained through matching and solving the unknown parameters by means of PnP, Direct Linear Transform or RANSAC to improve robustness. With all the unknowns determined, we can finally proceed to recover the 3D points $(X,Y,Z)$ by applying the inverse.  Back-Projection : Suppose $(X,Y,Z,1)$ is in the camera coordinate frame, i. e. we do not need to consider the extrinsic matrix $[R|t]$. ## References- MEDIUM "
    }, {
    "id": 23,
    "url": "http://localhost:4000/depth.html",
    "title": "Depth Estimation: Basics and Intuition",
    "body": "2021/09/02 - Introduction: In computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting epipolar geometry. This article will focus on giving readers a background into depth estimation and the problems associated with it. How We View the World: Let’s start with how humans perceive depth in general. This will give us some valuable insights on depth estimation since many of the methods were derived from our human vision system. Both machine and human vision share similarities in the way image is formed. Theoretically, when light rays from a source hit surfaces, it reflects off and directs towards the back of our retina, projecting them and our eye processes them as 2D just like how an image is formed on an image plane. So how do we actually measure distance and understand our environment in 3D when the projected scene is in 2D? The mechanism at work here is that our brain starts to reason about the incoming visual signals by recognizing patterns such as the size, texture and motion about the scene known as depth cues. There is no distance information about the image but somehow we can interpret and recover depth information effortlessly. These cues allow us to view objects and surfaces which are supposedly on flat images as 3D. How to Destroy Depth: Interpreting these depth cues begins with how scenes are projected to perspective view in humans and camera vision. On the other hand, an orthographic projection to front view or side view is one which destroys all depth information. Consider the above image. An observer could disentangle which aspect of the house is nearer to him/her as seen in the left image. However, it is totally impossible to distinguish relative distances from the right image. Even the background might be lying on the same plane as the house. Judging Depth Using Cues: There are basically 4 categories of depth cues: static monocular depth from motion, binocular and physiological cues. We subconsciously take advantage of these signals to perceive depth remarkably well. Pictorial Depth Cues: Our ability to perceive depth from a single image depends on the spatial arrangement of things in the scene. Below, I have summarized some of the hints that enable us to reason about the distance of different objects. It may already feel natural to you from your daily interaction.  Size of objects Texture Linear PerspectiveAn interesting study was conducted at UC, Berkeley and they show experimentally that when the horizon is viewable, there is an overwhelming tendency for us to exploit this property to quickly perceive depth. Depth Cues from Motion (Motion Parallax): This should not be surprising to you as well. When you, as an observer, is in motion, things around you pass by faster than the one that is farther away. The farther something appears, the slower it seems to pass away from the observer. Depth Cues from Stereo Vision (Binocular Parallax): The difference in view observed by your left and right eye is known as retina disparity. This phenomenon is also known as stereopsis: ability to perceive depth due to 2 different perspectives of the world. By comparing iamges from the retinas in the two eyes, the brain computes distance. The greater the disparity, the closer the things are around you. Depth Estimation in Computer Vision: The goal of depth estimation is to obtain a representation of the spatial structure of a scene, recovering the three-dimensional shape and appearance of objects in imagery. This is also known as the inverse problem, where we seek to recover some unknowns given insufficient information to fully specify the solution. So how do machines actually perceive depth? Can we somehow transfer some of the ideas discussed above? The earliest algorithm with impressive results begin with depth estimation using stereo vision back in the 90s. A lot of progress was made on dense stereo correspondence algorithm. Researchers were able to utilize geometry to constrain and replicate the idea of stereopsis mathemtically and at the same time running at real-time. As for monocular depth estimation, it recently started to gain popularity by using neural networks to learn representation that distills depth directly. Through neural networks, depth cues are implicitly learned through gradient-based methods. Besides this, there has been great advancement in self-supervised depth estimation. In this method, a model is trained to predict depth by means of optimizing a proxy signal. No ground truth label is needed in the training process. Most research either exploits geometrical cues such as multi-view geometry or epipolar geometry to learn depth. Depth Estimation from Stereo Vision: The main idea of solving depth using a stereo camera involves the concept of triangulation and stereo matching. The former depends on good calibration and rectification to constrain the problem so that it can be modelled on a 2D plane known as epipolar plane, which greatly reduces the latter (stereo matching) to a line search along the epipolar line.  Analogous to binocular parallax, once we are able to match pixel correspondences between the 2 views, the next task is to obtain a representation that encodes the difference. This representation is known as disparity. The formula to obtain depth from disparity can be worked out from similar triangles. To summarize the steps:  identify similar points from feature descriptors Match feature correspondence using a matching cost function Using epipolar geometry, find and match correspondence in one picture frame to the other. A matching cost function is used to measure the pixel dissimilarity.  Compute disparity from known correspondences $d = x1 - x2$ as shown in the above image.  Compute depth from the known disparity $ z = (f*b)/d$. Age of Deep Learning: Deep learning excels in high-level perceptual and cognitive task such as recognition, detection and scene understanding. Depth perception falls into this category and likewise should be a natural way forward. The seminal work of estimating depth directly from a monocular image started from Saxena. They learned to regress depth directly from monocular cues in 2D images via supervised learning, by minimizing a regression loss. Since then, many varieties of approaches have been proposed to improve the representation learning by proposing new architectures or loss functions. Self-Supervised Monocular Depth Estimation using Stereo: In this framework, the model will predict the disparities $d_l$ and $d_r$ only from the left RGB, $I_l$. Similar to the above method, a spatial transformer network warps the RGB image pair $I_l$, $I_r$ using the disparity. So the paired view can be synthesized and a reconstruction loss between the reconstructed views $I_{pred_l}$ and $I_{pred_r}$ and the target views $I_l$ and $Ir$ is used to supervise the training. Self-Supervised Depth Estimation using SfM Framework: This method frames the problem as learning to generate a novel view from a video sequence. The task of the neural network is to generate the target view $I_t$ from source view by taking image at different time step $I, I_{t-1}, I_{t+1}$ and applying a learnt transformation from a pose network to perform the image warping. Training was made possible by treating the warped view synthesis as supervision in a differentiable manner using a spatial transformer network. At inference time, the depth CNN would predict depth from a single RGB image. Do note that these methods do have some shortcomings such as unable to determine scale and modelling moving objects as described in the next section. CNN Depth Cues and Bias Learnt: Understanding and deciphering the black box has been an ongoing research in interpretable machine learning. In the context of depth estimation, a few works have started investigating what depth cues do neural network relies on or the inductive biased learnt from a particular dataset. They found out that:  Position of objects relative to ground contact point provides strong contextual information Shape does not matter but shadow does In an experiment, by placing an arbitrary object with artificial casted shadow, the model would estimate depth reasonably even if it is not available during training. Why is Measuring Depth So Difficult?: Let’s try to understand some of the fundamental problems of depth estimation. The main culprit lies in the projection of 3D views to 2D images where depth information is lost. Another problem is deeply seeded when there are motion and moving objects. Depth Estimation is Ill-Posed: On many depth estimation paper, many authors mention that the problem of estimating depth from a single RGB image is ill-posed inverse problem. What this means is that many 3D scenes observed in the world can indeed correspond to the same 2D plane. Scale-Ambiguity for Monocular Depth Estimatio: Recall that adjusting the focal length will proportionally scale the points on the image plane. Now, suppose we scale the entire scene, $X$ by some factor $k$ and at the same time, scale the camera matrices $P$ by the factor of $1/k$, the projections of the scene points in the image remains exactly the same. $$ x = PX = (1/k)P * kX $$That is to say, we can never recover the exact scale of the actual scene from the image alone! Note that this issue exists for monocular base techniques, as the scale can be recovered for a stero rig with a known baseline. Projection Ambiguity: Suppose we perform a geometric transformation of the scene. It is possible that after the transformation, these points will map to the same location on the plane. There exists not only projective ambiguity, but also affine and similarity ambiguity as well. Properties that Degrade Matching: For stereo based or multi-view depth estimation that requires triangulation, it usually involves the pipeline of Detect-Describe-Match. Matching becomes very difficult when the scene is taken from an extremely different viewpoint or varying changes in illumination between images. An extreme case is given in the above picture. These problematic cases include textureless regions (where many pixels will ahve the same pixel intensity), reflective surfaces, repetitive patterns or occlusions. Also. violating the Lambertian property (Lambertian surfaces refer to surfaces that appear to have teh same brightness no matter where it is viewed from). When images that show the same scene from 2 different view, the corresponding brightness intensity may not be equivalent due to non-ideal diffuse reflection. Moving Objects Violate the Static Assumption for SfM Methods: Dynamic objects in the scene further complicates the estimation process. Depth estimation via Structure from Motion involves a moving camera and consecutive static scenes. This assumption must hold for matching and aligning pxiels. This assumption breaks when there are moving objects in the scene. To this end, many researchers have looked into several methods to model moving objects in the scene by incorporating velocity information using optical flow or by using instance segmentation mask to model the object’s motion from one frame to another. References:  medium article"
    }, {
    "id": 24,
    "url": "http://localhost:4000/rigid2.html",
    "title": "3D Rigid Body Motion (Part 2)",
    "body": "2021/09/01 - Rotation Vectors and Euler AnglesRotation Vectors: With a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-freedom 3D rigid body motion? Obviously, the matrix representation has at least the following disadvantages:  $SO(3)$ has a rotation matrix of 9 qunatities, but a 3D rotation only has 3 degrees of freedom. Therefore, the matrix expression is redundant.  Similarly, the transformation matrix expresses 6 degree-of-freedom transformation with 16 quantities. So, is there a more compact representation? The rotation matrix itself has constraints: it must be an orthogonal matrix with a determinant of 1. The same is true for the transformation matrix. These constraints make the solution more difficult when you want to estimate or optimize a rotation matrix/transform matrix. Therefore, we hope that there is a way to describe rotation and translation more compactly. For example, is it feasible to express the rotation with a three-dimensional vector and express transformation with a six-dimensional vector?Obviously, a rotation can be described by a rotation axis and a rotation angle. Thus, we can use a vector whose direction is parallel with the axis of rotation, and the length is equal to the angle of rotation, which is called the rotation vector (or angle-axis/axis-angle). Only a three-dimensional vector is needed here to describe the rotation. Similarly, we may also use a rotation vector and a translation vector and a to express a transformation for a transformation matrix. The variable at this time is exactly six dimensions. Consider a rotation represented by $R$. If described by a rotation vector, assuming that the rotation axis is a unit-length vector $n$ and the angle is $\theta$, then the vector $\theta n$ can also describe this rotation. So, we have to ask, what is the connection between the two expressions? In fact, it is not difficult to derive their conversion relationship. The conversion from the rotation vector to the rotation matrix is shown by the Rodrigues’ formula. Since the derivation process is a little complicated, it is not described here. Only the result of the conversion is given. $$ R = \cos \theta I + (1-\cos \theta) n n^T + \sin \theta n^{\wedge}$$. The symbol $\wedge$ is a vector to skew-symmetric conversion. Conversely, we can also calculate the conversion from a rotation matrix to a rotation vector. For the corner $\theta$, taking the trace of both sides, we have:$$\begin{split} &amp; tr(R) = \cos \theta tr(I) + (1-\cos \theta) tr(nn^T) + \sin \theta tr(n^{\wedge}) \\ &amp; = 3 \cos \theta + (1 - \cos \theta) \\ &amp; = 1+ 2 \cos \theta \end{split}$$. Therefore, $$\theta = \arccos (\frac{tr(R) - 1}{2})$$. Regarding the axis $n$, since the rotation axis does not change after the rotation, we have: $$Rn=n$$. So, the axis $n$ is the eigenvector corresponding to the amtrix $R$’s eigenvalue $1$. Solving this equation and normalizing it gives the axis of rotation. References:  Introduction to Visual SLAM"
    }, {
    "id": 25,
    "url": "http://localhost:4000/d4.html",
    "title": "Depth from Disparity",
    "body": "2021/08/31 - We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow etc. In the first part of the series, we will focus on stereo vision.  🌟 3D Reconstruction from 2D Signals : We are going to look at depth estimation as a multi-view problem. Multi-view problems can span into different domains including stereo vision, Structure from Motion and optical flow. Although each carries great importance in the problem of Virtual Reality (VR) and robotics, we will focus on stereo vision in this multi-part series. Specifically, the progress of deep learning in stereo vision technology. How can we automatically compute 3D geometry from images? What cues in the image provide 3D information? Before looking at binocular, let’s consider single view characteristics. Well, we humans do so naturally. Here are several cues we use to infer depth information: shading texture  focus motion  perspective occlusion  symmetry Nonetheless, structure and depth are inherently ambiguous from a single view. As seen from the optical center (i. e. , camera location), $P1$ and $P2$ are projected onto the image plane as equivalent.  The Stereo Problem : Stereo vision systems reference the knowledge of two images captured simultaneously from a pair of cameras (left and right) with camera parameters (extrinsic and intrinsic). Stereo is heavily motivated by biology. Classic stereo problem include disparity, depth, occlusion, Structure from Motion, motion parallax etc. Provided left and right images, the ambiguity problem of the aforementioned is satisfied via triangulation from the two image planes by matching correspondences of a point in the world coordinate to the ones that can be seen in both images. The basic principle of triangulation is shown above: the reconstruction via the intersection of two rays. Requirements? Calibration and correspondences. Furthermore,  the Epipolar Constraint reduces the correspondence problem to $1D$ search along conjugate epipolar lines shown in the above figure. Thus, Epipolar Constraint assumes that stereo pairs are rectified images, meaning the same epipolar line aligns across rows such that it is orthogonal to both and aligned with one another. Rectification is achieved by learning a transformation based on intrinsic and extrinsic parameters: a process that dates back several decades. From the rectified image pair, the depth $Z$ can be determined by its inversely proportionate relationhip with disparity $d$, where the disparity is defined as the pixel difference along the horizontal when comparing correspondences of left and right. This can be written as: [I(x,y) = D(x+d, y)] This relationship is easiest to grasp visually. Provided a point in the left image plane $P_L(x,y)$ along with the correspondence in the right image plane and the world coordinates $(X,Y,Z)$ projected as a point in a 3D scene, our goal is to reconstruct the missing $Z$ (depth) provided the stereo pair. We set the relative coordinates (red axis) along with having known baseline $b$ between the two camera centers. Then $/frac{x_L}{f}$ can be expressed as $\frac{X+b/2}{Z}$ and $\frac{x_R}{f} = \frac{X-b/2}{Z}$. Therefore, [\frac{y_L}{f} = \frac{y_R}{f} = \frac{Y}{Z}] and the world coordinate can be expressed as [X = \frac{b(x_L+x_R}{2(x_L - x_R} Y = \frac{b(y_L + y_R}{2(x_L-x_R} Z = \frac{bf}{(x_L - x_R)}] $d = x_L - x_R$ is the disparity between corresponding left and right image points References :  article from medium "
    }, {
    "id": 26,
    "url": "http://localhost:4000/rigid.html",
    "title": "3D Rigid Body Motion (Part 1)",
    "body": "2021/08/31 -  The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle. Introduction: In this article, I will introduce one of the fundamental problems of visual SLAM: How to describe a rigid body’s motion in 3-dimensional space? Intuitively, we certainly know that this consists of one rotation plus one translation. The translation part does not really have any problems, but the rotation part is questionable. I will introduce the meaning of rotation matrices, quaternions, Euler angles and how they are computed and transformed. Rotation Matrix: Points, Vectors, and Coordinate Systems: The space of our daily life is 3-dimensional, so we are born to be used to 3D movements. The 3D space consists of three axes, so the position of one spatial point can be specified by three coordinates. However, we should now consider a rigid body, which has its position and orientation. The camera can also be viewed as a rigid body in three dimensions, so what we care about in Visual SLAM are the problem of the camera’s position and orientation. Combined, we can say, “the camera is at the $(0,0,0)$ point, facing the front”. Let’s describe this in a mathematical term. We start from the basic content: points and vectors. Points are the basic element in space, no length, no volume. Connecting the two points forms a vector. A vector can be thought of as an arrow pointing from one point to another. Here we need to warn you not to confuse the vector with its coordinates. A vector is one thing in space, such as $a$. Here, $a$ does not need to be associated with several real numbers. We can naturally talk about the plus or minus operation of two vectors, without relating to any real numbers. Only when we specify a coordinate system in this 3D space can we talk about the vector’s coordinates in this system, finding several real numbers corresponding to this vector. With the knowledge of linear algebra, the coordinates of a point in 3D space can be described as $\mathbb{R}^3$. How to do we describe this? Suppose that in this linear space, we fined a set of base $(e_1, e_2, e_3)$, then, an arbitrary vector $a$ has a coordinate under this base: $$ a = \begin{bmatrix} e_1 &amp; e_2 &amp; e_3 \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = a_1 e_1 + a_2 e_2 + a_3 e_3 $$. Here, $(a_1, a_2, a_3)^T$ is called $a$’s coordinates. The coordinates’ specific values are related to the vector itself and the selection of the bases. In $\mathbb{R}^3$, the coordinate system usually consists of $3$ orthogonal coordinate axes (it can also be non-orthogonal, but it is rare in practice). For example, given $x$ and $y$ axis, the $z$ axis can be determined using the right-hand (or left-hand) rule. According to different definitions, the coordinate system is divided into left-handed and right-handed. The third axis of the left-hand rule is opposite to the right-hand rule. Most 3D libraries use right-handed coordinates. Based on basic linear algebra knowledge, we can talk about the operations between vectors/vectors, vectors/numbers, such as scalar multiplication, vector addition, subtraction, inner product, outer product and so on. For $a,b \in \mathbb{R}^3$, the inner product of $a,b$ can be written as:$$ a \cdot b = a^Tb = \sum_{i=1}^3 a_i b_i = |a||b| \cos(&lt;a,b&gt;)$$, where $ &lt;a. b&gt; $ refers to the angle between the vector $a, b$. The inner product can also describe the projection relationship between vectors. $$ a \times b = \begin{Vmatrix} e_1 &amp; e_2 &amp; e_3 \\ a_1 &amp; a_2 &amp; a_3 \\ b_1 &amp; b_2 &amp; b_3 \end{Vmatrix} = \begin{bmatrix} a_2b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{bmatrix} = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \\ a_3 &amp; 0 &amp; -a_1 \\ -a_2 &amp; a_1 &amp; 0 \end{bmatrix} b = a \wedge b$$. The result of the outer product is a vector whose direction is perpendicular to the two vectors, and the length is $|a||b|\sin(&lt;a,b&gt;)$, which is also the area of the quadrilateral of the two vectors. From the outer product operation, we introduce the $\wedge$ operator here, which means writing $a$ as a skew-symmetric matrix. You can take $\wedge$ as a skew-symmetric symbol. It turns the outer product $a \times b$ into the multiplication of the matrix and the vector $a \wedge b$ is a linear operation. This symbol will be used frequently in the following sections. It is a one-to-one mapping, meaning that for any vector, it corresponds to a unique anti-symmetric matrix, and vice versa: $$ a \wedge = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \\ a_3 &amp; 0 &amp; -a_1 \\ -a_2 &amp; a_1 &amp; 0 \end{bmatrix}$$. At the same time, note that the vector operations such as addition, subtraction, inner and outer products can be calculated even when we do not have their coordinates. For example, although the inner product can be expressed by the sum of the two vectors’ product when we know the coordinates, the length and angle can also be calculated even if their coordinates are unknown. Therefore, the inner product result of the two vectors is independent of the selection of the coordinate system. Euclidean Transforms between Coordinate Systems: We often define a variety of coordinate systems in the real scene. In robotics, you define one coordinate system for each link and joint; in 3D mapping, we also define a coordinate system for each cuboid and cylinder. If we consider a moving robot, it is common practice to set a stationary inertial coordinate system (or world coordinate system), such as the $x_W, y_W, z_W$ defined in the picture above. Meanwhile, the camera or robot is a moving coordinate system, such as coordinate system defined by $x_C, y_C, z_C$. We might ask: a vector $p$ in the camera system may have coordinates $p_c$; and in the world coordinate system, its coordinates maybe $p_w$. Then what is the conversion between two coordinates? It is necessary to first obtain the coordinate values of the point in the camera system and then use the transform rule to do the coordinate transform. We need a mathematical way to describe this transformation. As we will see later, we can describe it with a transform matrix $T$. Intuitively, the motion between two coordinate systems consists of a rotation plus a translation, which is called rigid body motion. Obviously, the camera movement is rigid. During the rigid body motion, the length and angle of the vector will not change. Imagine that you throw your phone into the air and there may be differences in spatial position and orientation. But the length and the angle of each face will not change. At this point, we say that the phone’s motion is Euclidean. The Euclidean transform consists of rotation and translation. Let’s first consider the rotation. We have a unit-length orthogonal base $(e_1, e_2, e_3)$. After a rotation it becomes $(e_1’, e_2’, e_3’)$. Then, for the same vector $a$ (the vector does not move with the rotation of the coordinate system). its coordinates in these two coordinate systems are $[a_1, a_2, a_3]^T$ and $[a_1’, a_2’, a_3]^T$. Because the vector itself has not changed, according to the definition of coordinates, there are: $$ [e_1, e_2, e_3] \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = [e_1’, e_2’, e_3’] \begin{bmatrix} a_1’ \\ a_2’ \\ a_3’ \end{bmatrix}$$. To describe the relationship between the two coordinates, we multiply the left and right side of the above equation by $\begin{bmatrix} e_1^T \\ e_2^T \\ e_3^T \end{bmatrix}$, then the matrix on the left becomes an identity matrix, so: $$\begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} \triangleq Ra’$$. We take the intermediate matrix out and define it as a matrix $R$. This matrix consists of the inner product between the two sets of bases, describing the same vector’s coordinate transformation relationship before and after the rotation. It can be said that the matrix $R$ describes the rotation itself. So we call it the rotation matrix. Meanwhile, the components of the matrix are the inner product of the two coordinate system bases. Since the base vector’s length is $1$, it is actually the cosine of the angle between the base vectors. So this matrix is also called direction cosine matrix. The rotation matrix has some special properties. In fact, it is an orthogonal matrix with a determinant of $1$. Conversely, an orthogonal matrix with a determinant of 1 is also a rotation matrix. So you can define a set of $n$ dimensional rotation matrices as follows: $$SO(n) = \{ R \in \mathbb{R}^{n \times n}| RR^T = I, det(R) =1 \} $$. $SO(n)$ refers to the special orthogonal group. This set consists of a rotation matrix of $n$ dimensional space, in particular, $SO(3)$ refers to the rotation of the three-dimensional space. In this way, we can talk directly about the rotation transformation between the two coordinate systems without having to start from the bases. Since the rotation matrix is orthogonal, its inverse (i. e. , transpose) describes an opposite rotation. According to the above definition, there are:$$a’ = R^{-1}a = R^Ta$$. Obviously, the $R^T$ represents an opposite rotation. In the Euclidean transformation, there is a translation in addition to rotation. Consider the vector $a$ in the world coordinate system. After a rotation (depicted by $R$) and a translation of $t$, we get $a’$. Then we can put the rotation and translation together, and have:$$a’= Ra+t$$,where $t$ is called a translation vector. Compared to the rotation, the translation part simply adds the translation vector to the coordinates after the rotation, which si very simple. By the above formula, we completely describe the coordinate transformation relationship using a rotation matrix $R$ and a translation vector $t$. In practice, we may define the coordinate system 1 and 2, then the vector $a$ under the two coordinates is $a_1, a_2$. The relationship between the two systems should be: $$a_1 = R_{12} a_2 + t_{12}$$. Here, $R_{12}$ means the “rotation of the vector from system 2 to system 1”. About $t_{12}$, readers may just take it as a translation vector without wondering about its physical meaning. In fact, it corresponds to a vector from the system 1’s origin pointing to system 2’s origin, and the coordinates are taken under tsystem 1. So I suggest you to understand it as “a vector from 1 to 2”. But the reverse $t_{21}$, which is a vector from $2$’s origin to $1$’s origin, whose coordinates are taken in system $2$, is not equal to $-t_{12}$. It is also related to the rotation of the two systems. Therefore, when beginners ask the question “What are my coordinates?”, we need to clearly explain this sentence’s meaning. Here, “my coordinates” normally refers to the vector from the world system $W$ pointing to the origin of the camera system $C$, and then take the coordinates in the world’s base. Corresponding to the mathematical symbol, it should be the value of $t_{WC}$. For the same reason, it is not $-t_{CW}$ but actually $-R^T_{CW} t_{CW}$. Transform Matrix and Homogeneous Coordinates: The formula $a’ = Ra+t$ fully expresses the rotation and the translation of Euclidean space, but there is still a small problem: the transformation relationship here is not a linear relationship. Suppose we made two transformations: $R_1,t_1$ and $R_2,t_2$:$$b = R_1 a + t_1, c = R_2 b + t_2$$. So the transformation from $a$ to $c$ is: $$c = R_2 (R_1 a + t_1) + t_2$$. This form is not elegant after multiple transformations. Therefore, we introduce homogeneous coordinates and transformation matrices, rewriting the formula:$$ \begin{bmatrix} a’ \\ 1 \end{bmatrix} = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \begin{bmatrix} a \\ 1 \end{bmatrix} \triangleq T \begin{bmatrix} a \\ 1 \end{bmatrix}$$. This is a mathematical trick: we add $1$ at the end of the 3D vector and turn it into a 4D vector called homogeneous coordinates. For this four-dimensional vector, we can write the rotation and translation matrix, making the whole relationship a linear relationship. In this formula, the matrix $T$ is called transform matrix. We temporarily use $\tilde{a}$ to represent the homogeneous coordinates of $a$. Then, relying on homogeneous coordinates and transformation matrices, the superposition of the two transformations can have a good form: $$\tilde{b} = T_1 \tilde{a}, \tilde{c} = T_2 \tilde{b} \Rightarrow T_2 T_1 \tilde{a}$$. But the symbols that distinguish between homogeneous and non-homogeneous coordinates are annoying, because here we only need to add 1 at the end of the vector or remove 1 to turn it into a normal vector. So, without ambiguity, we will write it directly as $b = Ta$ and by default we just assume a homogeneous coordinate conversion is made if needed. The transformation matrix $T$ has a special structure: the upper left corner is the rotation matrix, the right side is the translation vector, the lower-left corner is $0$ vector, and the lower right corner is $1$. This set of transform matrix is also known as the special Euclidean group: $$SE(3) = \{ T = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \in \mathbb{R}^{4 \times 4}| R \in SO(3), t \in \mathbb{R}^3 \}$$. Like $SO(3)$, the inverse of the transformation matrix represents an inverse transformation: $$T^{-1} = \begin{bmatrix} R^T &amp; -R^T t \\ 0^T &amp; 1 \end{bmatrix}$$. Again, we use the notation of $T_{12}$ to represent a transformation from 2 to 1. Because the conversion between homogeneous and non-homogeneous coordinates is actually very easy, it is assumed that the conversion from homogeneous coordinates to normal coordinates is already done. Summary: First, we introduced the vector and its coordinate representation and introduced the operation between the vectors; then, the motion between the coordinate systems is described by the Euclidean transformation, which consists of translation and rotation. The rotation can be described by the rotation matrix $SO(3)$, while the translation is directly described by an $\mathbb{R}^3$ vector. Finally, if the translation and rotation are placed in a matrix, the transformation matrix $SE(3)$ is formed. References:  Introduction to Visual SLAM"
    }, {
    "id": 27,
    "url": "http://localhost:4000/homogeneous.html",
    "title": "Homogeneous Coordinates and Projective Geometry",
    "body": "2021/08/31 - Introduction: In this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry. Terminology: Most of the time when working with 3D, we are thinking in terms of Euclidean geometry-that is, coordinates in three-dimensional space ($X$, $Y$ and $Z$). However, there are certain situations where it is useful to think in terms of projective geometry instead. Projective geometry has an extra dimension, called $W$, in addition to the $X$, $Y$, and $Z$ dimensions. This four-dimensional space is called projective space and coordinates in projective space are called homogenous coordinates. For the purposes of 3D software, the terms projective and homogeous are basically interchangeable with 4D. Not Quaternions: Quaternions look a lot like homogeneous coordinates. Both are 4D vectors, commonly depicted as $(X,Y,Z,W)$. However, quaternions and homogeneous coordinates are different concepts, with different uses. An Analogy in 2D: First, let’s look at how projective geometry works in 2D, before we move on to 3D.  Imagine a projector that is projecting a 2D image onto a screen. It’s easy to identify the $X$ and $Y$ dimensions of the projected image.  Now, if you step back from the 2D image and look at the projector and the screen, you can see the $W$ dimension, too. The $W$ dimension is the distance from the projector to the screen.  So what does the $W$ dimension do, exactly? Imagine what would happen to the 2D image if you increased or decreased $W$-that is, if you increased or decreased the distance between the projector and the screen. If you move the projector closer to the screen, the whole 2D image becomes smaller. If you move the projector away from the screen, the 2D image becomes larger. As you can see, the value of $W$ affects the size (a. k. a scale) of the image. Applying it to 3D: There is no such thing as a 3D projector (yet), so its’ harder to imagine projective geometry in 3D, but the $W$ value works exactly the same as it does in 2D. When $W$ increases, the coordinates expands (scales up). When $W$ decreases, the coordinates shrinks (scales down). The $W$ is basically a scaling transformation for the 3D coordinates. When $W = 1$: The usual advice for 3D programming beginners is to always set $W=1$ whenever converting a 3D coordinate to 4D coordinate. The reason for that is that when you scale a coordinate by a 1 it doesn’t shrink or grow, it just stays the same size. So, when $W=1$, it has no effect on the $X$, $Y$, or $Z$ values. For this reason, when it comes to 3D computer graphics, coordinates are said to be correct only when $W=1$. If you tried to render with $W=0$ your program would crash when it attempted to divide by zero. With $W&lt;0$ everything would flip unside-down and back-to-front. Mathematically speaking, there is no such thing as an incorrect homogeneous coordinate. Using coordinates with $W=1$ is just a useful convention for the 3D computer grahics. The Math: Now, let’s look at some actual numbers, to see how the math works.  Let’s say that the projector is $3$ meters away from the screen, and there is a dot on the 2D image at the coordinate $(15, 21)$. This gives us the projective coordinate vector $(X,Y,W) = (15,21,3)$.  Now imagine that the projector was pushed closer to the screen so that the distance was $1$ meter. The closer the project gets to the screen, the smaller the image becomes. The projector has moved three times closer, so the image becomes three times smaller. If we take the original coordinate vector and divide all the values by three, we get the new vector where $W=1$:  $$(\frac{15}{3}, \frac{21}{3}, \frac{3}{3}) = (5,7,1)$$. The dot is now at coordinate $(5,7)$. This is how an incorrect homogeneous coordinate is converted to a correct coordinate: divide all the values by $W$. The process is exactly the same for 2D and 3D coordinates. Dividing all the values in a vector is done by a scalar multiplication with the reciprocal of the divisor. Here is a 4D example:  $$\frac{1}{5}(10, 20, 30, 5) = (\frac{10}{5}, \frac{20}{5}, \frac{30}{5}, \frac{5}{5}) = (2,4,6,1)$$ Uses of Homogeneous Coordinates in Computer Graphics: As mentioned earlier, in regard to 3D computer graphics, homogeneous coordinates are useful in certain situations. We will look at some of those situations here. Translation Matrices for 3D Coordinates:  A four-column matrix can only be multiplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. Rotation and scaling transformation matrices only require three columns. But, in order to do translation, the matrices need to have at least four columns. This is why transformations are often $4 \times 4$ matices. However, a matrix with four columns cannot be multiplied by a 3D vector, due to the rules of matrix multiplication. A four-column matrix can only be mulitplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. The 4th dimension $W$ is usually unchanged, when using homogeneous coordinates in matrix transformation. $W$ is set to $1$ when converting 3D coordinates into 4D, and is usually still $1$ after the transformation matrices are applied, at which point it can be converted back into a 3D coordinate by ignoring $W$. This is true for all translation, rotation, and scaling transformations, which by far are the most common types of transformations. The notable exception is projection matrices, which do affect the $W$ dimension. Perspective Transformation: In 3D, perspective is the phenomenon where an object appears smaller the further away it is from the camera. A far-away mountain can appear to be smaller than a cat, if the cat is close enough to the camera. Perspective is implemented in 3D computer graphics by using a transformation matrix that changes the $W$ element of each vertex. After the camera matrix is applied to each vertex, but before the projection matrix is applied, the $Z$ element of each vertex represents the distance away from the camera. Therefore, the larger $Z$ is, the more the vertex should be scaled down. The $W$ dimension affects the scale, so the projection matrix just changes the $W$ based on the $Z$ value. Here is an example of a perspective projection matrix being applied to a homogeneous coordinate:  $$ \begin{bmatrix} 1&amp;0&amp;0&amp;0 \\ 0&amp;1&amp;0&amp;0&amp; \\ 0&amp;0&amp;1&amp;0 \\ 0&amp;0&amp;1&amp;0 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1\end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1 \end{bmatrix} $$ Notice how the $W$ value is changed to $4$, which comes from the $Z$ value. After the perspective projection matrix is applied, each vertex undergoes perspective division. Perspective division is just a specific term for converting the homogeneous coordinate back to $W=1$, as explained earlier in the article. Continuing with the example above, the perspective division step would look like this:  $$\frac{1}{4}(2,3,4,4) = (0. 5,0. 75, 1,1)$$ After perspective division, the $W$ value is discarded, and we are left with a 3D coordinate that has been correctly scaled according to a 3D perspective projection. Positioning Directional Lights: One property of homogeneous coordinates is that they allow you to have points at infinity (infinite length vectors), which is not possible with 3D coordinates. Points at infinity occur when $W=0$. If you try to convert a $W=0$ homogeneous coordinate into a normal $W=1$ coordinate, it results in a bunch of divide-by-zero operations:  $$ \frac{1}{0}(2,3,4,0) = (\frac{2}{0}, \frac{3}{0}, \frac{4}{0}, \frac{0}{0})$$. This means that homogeneous coordinates with $W=0$ can not be converted back into 3D coordinates. What use does this have? Well, directional lights can be thought of as point lights that are infinitely far away. When a point light is infinitely far away, the rays of light become parallel, and all of the light travels in a single direction. This is basically the definition of a directional light. So, traditionally, in 3D graphics, directional lights are differentiated from point lights by the value of $W$ in the position vector of the light. If $W=1$, then it is a point light. If $W=0$, then it is a directional light. This is more of a traditional convention, rather than a useful way to write lighting code. Directional lights and point lights are usually implemented with separate code, because they behave differently. Summary: Homogeneous coordinates have an extra dimension called $W$, which scales the $X$, $Y$, and $Z$ dimensions. Matrices for translation and perspective projection can only be applied to homogeneous coordinates, which is why they are so common in 3D computer graphics. The $X$, $Y$, and $Z$ values are said to be correct when $W=1$. Any homogeneous coordinates can be converted to have $W=1$ by dividing all four dimensions by the $W$ value, except if $W=0$. When $W=0$, the coordinate represents a point at infinity (a vector with infinite length), and this is often used to denote the direction of directional lights. References:  Tomdalling’s Blog Post Image Processing and Computer Vision Lecture Notes"
    }, {
    "id": 28,
    "url": "http://localhost:4000/3d-cnn.html",
    "title": "3D CNN",
    "body": "2021/08/31 - def print_hi(name) puts  Hi, #{name} endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




</ul>
<form class="bd-search hidden-sm-down" onsubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small" id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
    

    
    
    
<!-- Begin post excerpts, let's highlight the first 4 posts on top -->
<div class="row remove-site-content-margin">
    
    <!-- latest post -->
    
    <div class="col-md-6">
    <div class="card border-0 mb-4 box-shadow">   
    <a href="/disparity.html">
    <div class="topfirstimage" style="background-image: url( /assets/images/depth/12.png); height: 200px;    background-size: cover;    background-repeat: no-repeat;"></div>     
    </a>
    <div class="card-body px-0 pb-0 d-flex flex-column align-items-start">
    <h2 class="h4 font-weight-bold">
    <a class="text-dark" href="/disparity.html">Depth from Disparity</a>
    </h2>
    <p class="excerpt">
        We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include st...
    </p>
    <div>
        <small class="d-block text-muted">
            In <span class="catlist">
                
                <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                
                </span>                   
        </small>
        <small class="text-muted">
            Sep 05, 2021
        </small>
    </div>
    </div>
    </div>
    </div>
    
    <div class="col-md-6">
        
        <!-- second latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/d2.html">
                 <img class="w-100" src="http://localhost:4000/assets/images/depth/12.png" alt="Depth from Disparity">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/d2.html">Depth from Disparity</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 05, 2021
                    </small>
                </div>
            </div>
        
        <!-- third latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/ssl.html">
                 <img class="w-100" src="/assets/images/depth/12.png" alt="Self-Supervised Depth Estimation">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/ssl.html">Self-Supervised Depth Estimation</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 05, 2021
                    </small>
                </div>
            </div>
        
        <!-- fourth latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/intrinsic.html">
                <img class="w-100" src="/assets/images/camera/camera.jpeg" alt="Dissecting the Camera Matrix (Part 2)">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/intrinsic.html">Dissecting the Camera Matrix (Part 2)</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 05, 2021
                    </small>
                </div>
            </div>
        
    </div>
    
</div>
    
<!-- Sticky - add sticky tag to the post you want to highlight here - tags: [sticky] -->
 

 

 

 

 

 

 

 

 

 

 

<div class="jumbotron jumbotron-fluid jumbotron-home pt-0 pb-0 mt-3 mb-2rem bg-lightblue position-relative">
    <div class="pl-4 pr-0 h-100 tofront">
        <div class="row justify-content-between">
            <div class="col-md-6 pt-6 pb-6 pr-lg-4 align-self-center">
                <h1 class="mb-3">Depth from Disparity</h1>
                <p class="mb-3 lead">
                    We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include st...
                </p>
                <a href="/d4.html" class="btn btn-dark">Read More</a>
            </div>
            <img class="col-md-6 d-none d-md-block pr-0" src="/assets/images/depth/12.png" align="middle">	
            </div>
        </div>
    </div>
</div> 

 

 

<div class="jumbotron jumbotron-fluid jumbotron-home pt-0 pb-0 mt-3 mb-2rem bg-lightblue position-relative">
    <div class="pl-4 pr-0 h-100 tofront">
        <div class="row justify-content-between">
            <div class="col-md-6 pt-6 pb-6 pr-lg-4 align-self-center">
                <h1 class="mb-3">Homogeneous Coordinates and Projective Geometry</h1>
                <p class="mb-3 lead">
                    IntroductionIn this article, I’m going to explain homogeneous coordinates (a.k.a 4D coordinates) as simply as I can. It is now time t...
                </p>
                <a href="/homogeneous.html" class="btn btn-dark">Read More</a>
            </div>
            <img class="col-md-6 d-none d-md-block pr-0" src="/assets/images/home.png" align="middle">	
            </div>
        </div>
    </div>
 

 




    


 <!--endif page url is / -->
    


<!-- Now the rest of the posts with the usual loop but with an offset:4 on the first page so we can skeep the first 4 posts displayed above -->
    
<div class="row mt-3">
   
    <div class="col-md-8 main-loop">
        
        <h4 class="font-weight-bold spanborder"><span>All Stories</span></h4>
        

        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/disparity.html">Depth from Disparity</a>
	</h2>
	<p class="excerpt">
	   We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 05, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/disparity.html">
	<img height="150px" width="200px" src="/assets/images/depth/12.png" alt="Depth from Disparity">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/d2.html">Depth from Disparity</a>
	</h2>
	<p class="excerpt">
	   We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 05, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/d2.html">
	<img height="150px" width="200px" src="/assets/images/depth/12.png" alt="Depth from Disparity">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/ssl.html">Self-Supervised Depth Estimation</a>
	</h2>
	<p class="excerpt">
	   We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 05, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/ssl.html">
	<img height="150px" width="200px" src="/assets/images/depth/12.png" alt="Self-Supervised Depth Estimation">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/intrinsic.html">Dissecting the Camera Matrix (Part 2)</a>
	</h2>
	<p class="excerpt">
	   In this blog post, we will study the intrinsic camera matrix. We will examine two equivalent interpretations: as a description of the virtual camera’s geometry and a sequence of simpl...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 05, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/intrinsic.html">
	<img height="150px" width="200px" src="/assets/images/camera/camera.jpeg" alt="Dissecting the Camera Matrix (Part 2)">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/dense.html">Dense Depth Using Stereo Vision</a>
	</h2>
	<p class="excerpt">
	   What exactly is this all-important dense depth that stereo vision makes possible? When we talk about dense depth, we mean we can take the video we capture from our car's cameras and t...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 05, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/dense.html">
	<img height="150px" width="200px" src="/assets/images/depth/12.png" alt="Dense Depth Using Stereo Vision">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/camera.html">Dissecting the Camera Matrix (Part 1)</a>
	</h2>
	<p class="excerpt">
	   This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices, and and I will try to untangle the issues that can crop-up with different coord...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 05, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/camera.html">
	<img height="150px" width="200px" src="/assets/images/camera/camera.jpeg" alt="Dissecting the Camera Matrix (Part 1)">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/d3.html">Depth from Disparity</a>
	</h2>
	<p class="excerpt">
	   We will be looking at depth estimation as a multi-image problem. Multi-view problems span different problem domains: these include stereo vision, Structure from Motion, optical flow e...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 04, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/d3.html">
	<img height="150px" width="200px" src="/assets/images/depth/12.png" alt="Depth from Disparity">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/inverse.html">Inverse Projection Transformation</a>
	</h2>
	<p class="excerpt">
	   When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 03, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/inverse.html">
	<img height="150px" width="200px" src="/assets/images/depth/12.png" alt="Inverse Projection Transformation">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/depth.html">Depth Estimation: Basics and Intuition</a>
	</h2>
	<p class="excerpt">
	   IntroductionIn computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 02, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/depth.html">
	<img height="150px" width="200px" src="/assets/images/depth/1.jpg" alt="Depth Estimation: Basics and Intuition">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/rigid2.html">3D Rigid Body Motion (Part 2)</a>
	</h2>
	<p class="excerpt">
	   Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 01, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/rigid2.html">
	<img height="150px" width="200px" src="/assets/images/rigid/1.png" alt="3D Rigid Body Motion (Part 2)">
	</a>
	</div>

</div>

        
        
        
        <div class="mt-5">
         <!-- Pagination links -->
            
            <ul class="pagination"> 
              
                <li class="page-item disabled"><span class="prev page-link">«</span></li>
              

              
                
                <li class="page-item disabled"><span class="webjeda page-link">1</span></li>
                
              
                
                <li class="page-item"><a class="page-link" href="/page2">2</a></li>
                
              

              
                <li class="page-item"><a class="page-link" href="/page2">Next »</a></li>
              
            </ul>
                  
        </div>
        
    </div>
    
    <div class="col-md-4">
        <div class="sticky-top sticky-top-offset">
    <h4 class="font-weight-bold spanborder"><span>Featured</span></h4>  
    <ol class="list-featured">				
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/disparity.html" class="text-dark">Depth from Disparity</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/d2.html" class="text-dark">Depth from Disparity</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/ssl.html" class="text-dark">Self-Supervised Depth Estimation</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/intrinsic.html" class="text-dark">Dissecting the Camera Matrix (Part 2)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/dense.html" class="text-dark">Dense Depth Using Stereo Vision</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/camera.html" class="text-dark">Dissecting the Camera Matrix (Part 1)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/d3.html" class="text-dark">Depth from Disparity</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/inverse.html" class="text-dark">Inverse Projection Transformation</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/depth.html" class="text-dark">Depth Estimation: Basics and Intuition</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/rigid2.html" class="text-dark">3D Rigid Body Motion (Part 2)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/rigid.html" class="text-dark">3D Rigid Body Motion (Part 1)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/3d-cnn.html" class="text-dark">3D CNN</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#deep%20learning">deep learning</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
           
    </ol>
</div>     
    </div>
    
</div>





    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/custom.js"></script>
    

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Seri Lee</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                
            </div>
            </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
