<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Mundana Free Jekyll Theme | Seri Lee Blog</title>

    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Mundana Free Jekyll Theme | Seri Lee Blog</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Mundana Free Jekyll Theme">
<meta property="og:locale" content="en_US">
<meta name="description" content="A great Jekyll theme developed by Sal @wowthemesnet.">
<meta property="og:description" content="A great Jekyll theme developed by Sal @wowthemesnet.">
<link rel="canonical" href="http://localhost:4000/">
<meta property="og:url" content="http://localhost:4000/">
<meta property="og:site_name" content="Seri Lee Blog">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Mundana Free Jekyll Theme">
<script type="application/ld+json">
{"description":"A great Jekyll theme developed by Sal @wowthemesnet.","headline":"Mundana Free Jekyll Theme","url":"http://localhost:4000/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"}},"@type":"WebSite","name":"Seri Lee Blog","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

	
    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <link rel="stylesheet" href="/assets/css/custom.css">
    
    <!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">-->
    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="/assets/js/lib.js" defer></script>
    <script> 
	    MathJax = {
		    loader: {load: ['input/tex', 'ui/menu']},
		    tex: {
			    inlineMath: {'[+]': [['$','$']]},
			    displayMath: {'[+]':[['$$', '$$']]}
			}
	    }
   </script>
    <script type="text/javascript" id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
 <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>
  <script>
    function convert() {
      //
      //  Get the MathML input string, and clear any previous output
      //
      var input = document.getElementById("input").value.trim();
      output = document.getElementById('output');
      output.innerHTML = '';
      //
      //  Convert the MathMl to an HTML node and append it to the output
      //
      output.appendChild(MathJax.mathml2chtml(input));
      //
      //  Then update the document to include the adjusted CSS for the
      //    content of the new equation.
      //
      MathJax.startup.document.clear();
      MathJax.startup.document.updateDocument();
    }
  </script> 
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body class=" homefirstpage ">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>Seri Lee Blog</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Author</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>




            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about.html",
    "title": "About",
    "body": "Made by Seri @porfolio. "
    }, {
    "id": 2,
    "url": "http://localhost:4000/author-sal.html",
    "title": "Sal",
    "body": "                        Sal /span&gt;&lt;/h2&gt;        https://sites. google. com/snu. ac. kr/sally20921porfolio         Hi, I am Seri, the author of this blog.       &lt;/div&gt;                        &lt;/div&gt;    Posts by Sal:                   		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						            		Depth Estimation: Basics and Intuition	: 		  Introduction	 			In 				computer vision, 								Sep 02, 2021						            		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						            		3D Rigid Body Motion (Part 1)	: 		   The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle. 	 			In 				computer vision, 								Aug 31, 2021						            		Homogeneous Coordinates and Projective Geometry	: 		  IntroductionIn this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry. 	 			In 				computer vision, 								Aug 31, 2021						            		3D CNN	: 		  def print_hi(name) puts  Hi, #{name} endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT. 	 			In 				deep learning, 								Aug 31, 2021						        &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;: "
    }, {
    "id": 3,
    "url": "http://localhost:4000/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                             Seri :       (View Posts)      Hi, I am Seri, the author of this blog.                           &nbsp;       &nbsp;                                      "
    }, {
    "id": 4,
    "url": "http://localhost:4000/categories.html",
    "title": "Categories",
    "body": "          Categories               deep learning:                                  		3D CNN	: 		  def print_hi(name) puts  Hi, #{name} endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT. 	 			In 				deep learning, 								Aug 31, 2021						                              computer vision:                                  		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						                                 		Depth Estimation: Basics and Intuition	: 		  Introduction	 			In 				computer vision, 								Sep 02, 2021						                                 		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						                                 		3D Rigid Body Motion (Part 1)	: 		   The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle. 	 			In 				computer vision, 								Aug 31, 2021						                                 		Homogeneous Coordinates and Projective Geometry	: 		  IntroductionIn this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry. 	 			In 				computer vision, 								Aug 31, 2021						                                             Featured:    				                                          Inverse Projection Transformation                          In                     computer vision,                                                                                           Depth Estimation: Basics and Intuition                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 2)                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 1)                          In                     computer vision,                                                                                           3D CNN                          In                     deep learning,                                                                   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/contact.html",
    "title": "Contact",
    "body": "  Please send your message to Seri Lee Blog. We will reply as soon as possible!   "
    }, {
    "id": 6,
    "url": "http://localhost:4000/",
    "title": "Mundana Free Jekyll Theme",
    "body": "                  	    &lt;img width= 550px  height= 200px  src=/assets/images/depth/12. png style= height: 210px;  align= middle &gt;        Inverse Projection Transformation  :       When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in whic. . .               In                 computer vision,                                        Sep 03, 2021                                                                                                                             Depth Estimation: Basics and Intuition          :                       In                         computer vision,                                                                  Sep 02, 2021                                                                                                                                     3D Rigid Body Motion (Part 2)          :                       In                         computer vision,                                                                  Sep 01, 2021                                                                                                                                    3D Rigid Body Motion (Part 1)          :                       In                         computer vision,                                                                  Aug 31, 2021                                                         Homogeneous Coordinates and Projective Geometry                  IntroductionIn this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time t. . .                 Read More      	  	   &lt;img class= col-md-6 d-none d-md-block pr-0  src=/assets/images/home. png align= midde &gt;                        All Stories:                   		Inverse Projection Transformation	: 		  When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a. . . 	 			In 				computer vision, 								Sep 03, 2021						                  		Depth Estimation: Basics and Intuition	: 		  Introduction	 			In 				computer vision, 								Sep 02, 2021						                  		3D Rigid Body Motion (Part 2)	: 		  Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f. . . 	 			In 				computer vision, 								Sep 01, 2021						                  		3D Rigid Body Motion (Part 1)	: 		   The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle. 	 			In 				computer vision, 								Aug 31, 2021						                  		Homogeneous Coordinates and Projective Geometry	: 		  IntroductionIn this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry. 	 			In 				computer vision, 								Aug 31, 2021						                  		3D CNN	: 		  def print_hi(name) puts  Hi, #{name} endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT. 	 			In 				deep learning, 								Aug 31, 2021						                                                  Featured:    				                                          Inverse Projection Transformation                          In                     computer vision,                                                                                           Depth Estimation: Basics and Intuition                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 2)                          In                     computer vision,                                                                                           3D Rigid Body Motion (Part 1)                          In                     computer vision,                                                                                           3D CNN                          In                     deep learning,                                                               &lt;/div&gt; "
    }, {
    "id": 7,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 8,
    "url": "http://localhost:4000/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 9,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 10,
    "url": "http://localhost:4000/inverse/",
    "title": "Inverse Projection Transformation",
    "body": "2021/09/03 -  Depth and Inverse Projection : RGB + Depth = 3D Back-Projected Points When an image of a scene is captured by a camera, we lose depth information as objects and points in 3D space are mapped onto a 2D image plane. This is also known as projective transformation , in which points in the world are converted to pixels on a 2D plane. &lt;/p&gt;However, what if we want to do the inverse ? That is, we want to recover and reconstruct the scene given only 2D image. To do that, we would need to know the depth or $Z$-component of each corresponding pixels. Depth can be represented as an image as shown in the figure above, with brigther intensity denoting points further away.  In this blog post, we will take a tour and understand the mathematics and concepts of performing back-projection from 2D pixel coordinates to 3D points. We will assume that a depth map is provided to perform the 3D reconstruction. The concept that we will go through are camera calibration parameters, projective transformation using intrinsic and its inverse, and coordinate transformation between frames.  Central Projection of Pinhole Camera Model : First and foremost, understanding the geometrical model of the camera projection serves as the core idea. What we are ultimately interested in is the depth, parameter $Z$. Here, we consider the simplest pinhole camera model with no skew or distortion factor. 3D points are mapped to the image plane $(u,v) = f(X,Y,Z)$. The complete mathematical model that describes this transformation can be written as $p = K[R|t]*P$. $$s \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \begin{bmatrix} f_x &amp; 0 &amp; u_0 \\ 0 &amp; f_y &amp; v_0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} r_{11} &amp; r_{12} &amp; r{13} &amp; t{1} \\ r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2 \\ r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3\end{bmatrix} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}$$, where $p$ is the projected point on the image plane $K$ is the camera intrinsics matrix $[R|t]$ is the extrinsic parameters describing the relative transformation of the point in the world frame to the camera frame $P$ or $[X, Y, Z, 1]$ represents the 3D point expressed in a predefined world coordinate system in Euclidean space Aspect ratio scaling $s$ controls how pixels are scaled in the $x$ and $y$ direction as focal length chnanges Intrinsic Parameter Matrix :  Camera Projective Geometry The matrix $K$ is responsible for projecting 3D points to the image plane. To do that, the following quantities must be defined as:* Focal length $(f_x, f_y)$: measure the position of the image plane with respect to the camera center. * Principal point $(u_0, v_0)$; the optical center of the image plane* Skew factor: the misalignment from a square pixel if the image plane axes are not perpendicular. In our example, this is set to zero. The most common way of solving all the paramters is using the checkerboard method, where several 2D-3D correspondences obtained through matching and solving the unknown parameters by means of PnP, Direct Linear Transform or RANSAC to improve robustness. With all the unknowns determined, we can finally proceed to recover the 3D points $(X,Y,Z)$ by applying the inverse.  Back-Projection : ## References- MEDIUM "
    }, {
    "id": 11,
    "url": "http://localhost:4000/depth/",
    "title": "Depth Estimation: Basics and Intuition",
    "body": "2021/09/02 - Introduction: In computer vision, depth is extracted from 2 prevalent methodologies. Namely, depth from monocular images (static or sequential) or depth from stereo images by exploiting epipolar geometry. This article will focus on giving readers a background into depth estimation and the problems associated with it. How We View the World: Let’s start with how humans perceive depth in general. This will give us some valuable insights on depth estimation since many of the methods were derived from our human vision system. Both machine and human vision share similarities in the way image is formed. Theoretically, when light rays from a source hit surfaces, it reflects off and directs towards the back of our retina, projecting them and our eye processes them as 2D just like how an image is formed on an image plane. So how do we actually measure distance and understand our environment in 3D when the projected scene is in 2D? The mechanism at work here is that our brain starts to reason about the incoming visual signals by recognizing patterns such as the size, texture and motion about the scene known as depth cues. There is no distance information about the image but somehow we can interpret and recover depth information effortlessly. These cues allow us to view objects and surfaces which are supposedly on flat images as 3D. How to Destroy Depth: Interpreting these depth cues begins with how scenes are projected to perspective view in humans and camera vision. On the other hand, an orthographic projection to front view or side view is one which destroys all depth information. Consider the above image. An observer could disentangle which aspect of the house is nearer to him/her as seen in the left image. However, it is totally impossible to distinguish relative distances from the right image. Even the background might be lying on the same plane as the house. Judging Depth Using Cues: There are basically 4 categories of depth cues: static monocular depth from motion, binocular and physiological cues. We subconsciously take advantage of these signals to perceive depth remarkably well. Pictorial Depth Cues: Our ability to perceive depth from a single image depends on the spatial arrangement of things in the scene. Below, I have summarized some of the hints that enable us to reason about the distance of different objects. It may already feel natural to you from your daily interaction.  Size of objects Texture Linear PerspectiveAn interesting study was conducted at UC, Berkeley and they show experimentally that when the horizon is viewable, there is an overwhelming tendency for us to exploit this property to quickly perceive depth. Depth Cues from Motion (Motion Parallax): This should not be surprising to you as well. When you, as an observer, is in motion, things around you pass by faster than the one that is farther away. The farther something appears, the slower it seems to pass away from the observer. Depth Cues from Stereo Vision (Binocular Parallax): The difference in view observed by your left and right eye is known as retina disparity. This phenomenon is also known as stereopsis: ability to perceive depth due to 2 different perspectives of the world. By comparing iamges from the retinas in the two eyes, the brain computes distance. The greater the disparity, the closer the things are around you. Depth Estimation in Computer Vision: The goal of depth estimation is to obtain a representation of the spatial structure of a scene, recovering the three-dimensional shape and appearance of objects in imagery. This is also known as the inverse problem, where we seek to recover some unknowns given insufficient information to fully specify the solution. So how do machines actually perceive depth? Can we somehow transfer some of the ideas discussed above? The earliest algorithm with impressive results begin with depth estimation using stereo vision back in the 90s. A lot of progress was made on dense stereo correspondence algorithm. Researchers were able to utilize geometry to constrain and replicate the idea of stereopsis mathemtically and at the same time running at real-time. As for monocular depth estimation, it recently started to gain popularity by using neural networks to learn representation that distills depth directly. Through neural networks, depth cues are implicitly learned through gradient-based methods. Besides this, there has been great advancement in self-supervised depth estimation. In this method, a model is trained to predict depth by means of optimizing a proxy signal. No ground truth label is needed in the training process. Most research either exploits geometrical cues such as multi-view geometry or epipolar geometry to learn depth. Depth Estimation from Stereo Vision: The main idea of solving depth using a stereo camera involves the concept of triangulation and stereo matching. The former depends on good calibration and rectification to constrain the problem so that it can be modelled on a 2D plane known as epipolar plane, which greatly reduces the latter (stereo matching) to a line search along the epipolar line.  Analogous to binocular parallax, once we are able to match pixel correspondences between the 2 views, the next task is to obtain a representation that encodes the difference. This representation is known as disparity. The formula to obtain depth from disparity can be worked out from similar triangles. To summarize the steps:  identify similar points from feature descriptors Match feature correspondence using a matching cost function Using epipolar geometry, find and match correspondence in one picture frame to the other. A matching cost function is used to measure the pixel dissimilarity.  Compute disparity from known correspondences $d = x1 - x2$ as shown in the above image.  Compute depth from the known disparity $ z = (f*b)/d$. Age of Deep Learning: Deep learning excels in high-level perceptual and cognitive task such as recognition, detection and scene understanding. Depth perception falls into this category and likewise should be a natural way forward. The seminal work of estimating depth directly from a monocular image started from Saxena. They learned to regress depth directly from monocular cues in 2D images via supervised learning, by minimizing a regression loss. Since then, many varieties of approaches have been proposed to improve the representation learning by proposing new architectures or loss functions. Self-Supervised Monocular Depth Estimation using Stereo: In this framework, the model will predict the disparities $d_l$ and $d_r$ only from the left RGB, $I_l$. Similar to the above method, a spatial transformer network warps the RGB image pair $I_l$, $I_r$ using the disparity. So the paired view can be synthesized and a reconstruction loss between the reconstructed views $I_{pred_l}$ and $I_{pred_r}$ and the target views $I_l$ and $Ir$ is used to supervise the training. Self-Supervised Depth Estimation using SfM Framework: This method frames the problem as learning to generate a novel view from a video sequence. The task of the neural network is to generate the target view $I_t$ from source view by taking image at different time step $I, I_{t-1}, I_{t+1}$ and applying a learnt transformation from a pose network to perform the image warping. Training was made possible by treating the warped view synthesis as supervision in a differentiable manner using a spatial transformer network. At inference time, the depth CNN would predict depth from a single RGB image. Do note that these methods do have some shortcomings such as unable to determine scale and modelling moving objects as described in the next section. CNN Depth Cues and Bias Learnt: Understanding and deciphering the black box has been an ongoing research in interpretable machine learning. In the context of depth estimation, a few works have started investigating what depth cues do neural network relies on or the inductive biased learnt from a particular dataset. They found out that:  Position of objects relative to ground contact point provides strong contextual information Shape does not matter but shadow does In an experiment, by placing an arbitrary object with artificial casted shadow, the model would estimate depth reasonably even if it is not available during training. Why is Measuring Depth So Difficult?: Let’s try to understand some of the fundamental problems of depth estimation. The main culprit lies in the projection of 3D views to 2D images where depth information is lost. Another problem is deeply seeded when there are motion and moving objects. Depth Estimation is Ill-Posed: On many depth estimation paper, many authors mention that the problem of estimating depth from a single RGB image is ill-posed inverse problem. What this means is that many 3D scenes observed in the world can indeed correspond to the same 2D plane. Scale-Ambiguity for Monocular Depth Estimatio: Recall that adjusting the focal length will proportionally scale the points on the image plane. Now, suppose we scale the entire scene, $X$ by some factor $k$ and at the same time, scale the camera matrices $P$ by the factor of $1/k$, the projections of the scene points in the image remains exactly the same. $$ x = PX = (1/k)P * kX $$That is to say, we can never recover the exact scale of the actual scene from the image alone! Note that this issue exists for monocular base techniques, as the scale can be recovered for a stero rig with a known baseline. Projection Ambiguity: Suppose we perform a geometric transformation of the scene. It is possible that after the transformation, these points will map to the same location on the plane. There exists not only projective ambiguity, but also affine and similarity ambiguity as well. Properties that Degrade Matching: For stereo based or multi-view depth estimation that requires triangulation, it usually involves the pipeline of Detect-Describe-Match. Matching becomes very difficult when the scene is taken from an extremely different viewpoint or varying changes in illumination between images. An extreme case is given in the above picture. These problematic cases include textureless regions (where many pixels will ahve the same pixel intensity), reflective surfaces, repetitive patterns or occlusions. Also. violating the Lambertian property (Lambertian surfaces refer to surfaces that appear to have teh same brightness no matter where it is viewed from). When images that show the same scene from 2 different view, the corresponding brightness intensity may not be equivalent due to non-ideal diffuse reflection. Moving Objects Violate the Static Assumption for SfM Methods: Dynamic objects in the scene further complicates the estimation process. Depth estimation via Structure from Motion involves a moving camera and consecutive static scenes. This assumption must hold for matching and aligning pxiels. This assumption breaks when there are moving objects in the scene. To this end, many researchers have looked into several methods to model moving objects in the scene by incorporating velocity information using optical flow or by using instance segmentation mask to model the object’s motion from one frame to another. References:  medium article"
    }, {
    "id": 12,
    "url": "http://localhost:4000/rigid2/",
    "title": "3D Rigid Body Motion (Part 2)",
    "body": "2021/09/01 - Rotation Vectors and Euler AnglesRotation Vectors: With a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-freedom 3D rigid body motion? Obviously, the matrix representation has at least the following disadvantages:  $SO(3)$ has a rotation matrix of 9 qunatities, but a 3D rotation only has 3 degrees of freedom. Therefore, the matrix expression is redundant.  Similarly, the transformation matrix expresses 6 degree-of-freedom transformation with 16 quantities. So, is there a more compact representation? The rotation matrix itself has constraints: it must be an orthogonal matrix with a determinant of 1. The same is true for the transformation matrix. These constraints make the solution more difficult when you want to estimate or optimize a rotation matrix/transform matrix. Therefore, we hope that there is a way to describe rotation and translation more compactly. For example, is it feasible to express the rotation with a three-dimensional vector and express transformation with a six-dimensional vector?Obviously, a rotation can be described by a rotation axis and a rotation angle. Thus, we can use a vector whose direction is parallel with the axis of rotation, and the length is equal to the angle of rotation, which is called the rotation vector (or angle-axis/axis-angle). Only a three-dimensional vector is needed here to describe the rotation. Similarly, we may also use a rotation vector and a translation vector and a to express a transformation for a transformation matrix. The variable at this time is exactly six dimensions. Consider a rotation represented by $R$. If described by a rotation vector, assuming that the rotation axis is a unit-length vector $n$ and the angle is $\theta$, then the vector $\theta n$ can also describe this rotation. So, we have to ask, what is the connection between the two expressions? In fact, it is not difficult to derive their conversion relationship. The conversion from the rotation vector to the rotation matrix is shown by the Rodrigues’ formula. Since the derivation process is a little complicated, it is not described here. Only the result of the conversion is given. $$ R = \cos \theta I + (1-\cos \theta) n n^T + \sin \theta n^{\wedge}$$. The symbol $\wedge$ is a vector to skew-symmetric conversion. Conversely, we can also calculate the conversion from a rotation matrix to a rotation vector. For the corner $\theta$, taking the trace of both sides, we have:$$\begin{split} &amp; tr(R) = \cos \theta tr(I) + (1-\cos \theta) tr(nn^T) + \sin \theta tr(n^{\wedge}) \\ &amp; = 3 \cos \theta + (1 - \cos \theta) \\ &amp; = 1+ 2 \cos \theta \end{split}$$. Therefore, $$\theta = \arccos (\frac{tr(R) - 1}{2})$$. Regarding the axis $n$, since the rotation axis does not change after the rotation, we have: $$Rn=n$$. So, the axis $n$ is the eigenvector corresponding to the amtrix $R$’s eigenvalue $1$. Solving this equation and normalizing it gives the axis of rotation. References:  Introduction to Visual SLAM"
    }, {
    "id": 13,
    "url": "http://localhost:4000/rigid/",
    "title": "3D Rigid Body Motion (Part 1)",
    "body": "2021/08/31 -  The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle. Introduction: In this article, I will introduce one of the fundamental problems of visual SLAM: How to describe a rigid body’s motion in 3-dimensional space? Intuitively, we certainly know that this consists of one rotation plus one translation. The translation part does not really have any problems, but the rotation part is questionable. I will introduce the meaning of rotation matrices, quaternions, Euler angles and how they are computed and transformed. Rotation Matrix: Points, Vectors, and Coordinate Systems: The space of our daily life is 3-dimensional, so we are born to be used to 3D movements. The 3D space consists of three axes, so the position of one spatial point can be specified by three coordinates. However, we should now consider a rigid body, which has its position and orientation. The camera can also be viewed as a rigid body in three dimensions, so what we care about in Visual SLAM are the problem of the camera’s position and orientation. Combined, we can say, “the camera is at the $(0,0,0)$ point, facing the front”. Let’s describe this in a mathematical term. We start from the basic content: points and vectors. Points are the basic element in space, no length, no volume. Connecting the two points forms a vector. A vector can be thought of as an arrow pointing from one point to another. Here we need to warn you not to confuse the vector with its coordinates. A vector is one thing in space, such as $a$. Here, $a$ does not need to be associated with several real numbers. We can naturally talk about the plus or minus operation of two vectors, without relating to any real numbers. Only when we specify a coordinate system in this 3D space can we talk about the vector’s coordinates in this system, finding several real numbers corresponding to this vector. With the knowledge of linear algebra, the coordinates of a point in 3D space can be described as $\mathbb{R}^3$. How to do we describe this? Suppose that in this linear space, we fined a set of base $(e_1, e_2, e_3)$, then, an arbitrary vector $a$ has a coordinate under this base: $$ a = \begin{bmatrix} e_1 &amp; e_2 &amp; e_3 \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = a_1 e_1 + a_2 e_2 + a_3 e_3 $$. Here, $(a_1, a_2, a_3)^T$ is called $a$’s coordinates. The coordinates’ specific values are related to the vector itself and the selection of the bases. In $\mathbb{R}^3$, the coordinate system usually consists of $3$ orthogonal coordinate axes (it can also be non-orthogonal, but it is rare in practice). For example, given $x$ and $y$ axis, the $z$ axis can be determined using the right-hand (or left-hand) rule. According to different definitions, the coordinate system is divided into left-handed and right-handed. The third axis of the left-hand rule is opposite to the right-hand rule. Most 3D libraries use right-handed coordinates. Based on basic linear algebra knowledge, we can talk about the operations between vectors/vectors, vectors/numbers, such as scalar multiplication, vector addition, subtraction, inner product, outer product and so on. For $a,b \in \mathbb{R}^3$, the inner product of $a,b$ can be written as:$$ a \cdot b = a^Tb = \sum_{i=1}^3 a_i b_i = |a||b| \cos(&lt;a,b&gt;)$$, where $ &lt;a. b&gt; $ refers to the angle between the vector $a, b$. The inner product can also describe the projection relationship between vectors. $$ a \times b = \begin{Vmatrix} e_1 &amp; e_2 &amp; e_3 \\ a_1 &amp; a_2 &amp; a_3 \\ b_1 &amp; b_2 &amp; b_3 \end{Vmatrix} = \begin{bmatrix} a_2b_3 - a_3 b_2 \\ a_3 b_1 - a_1 b_3 \\ a_1 b_2 - a_2 b_1 \end{bmatrix} = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \\ a_3 &amp; 0 &amp; -a_1 \\ -a_2 &amp; a_1 &amp; 0 \end{bmatrix} b = a \wedge b$$. The result of the outer product is a vector whose direction is perpendicular to the two vectors, and the length is $|a||b|\sin(&lt;a,b&gt;)$, which is also the area of the quadrilateral of the two vectors. From the outer product operation, we introduce the $\wedge$ operator here, which means writing $a$ as a skew-symmetric matrix. You can take $\wedge$ as a skew-symmetric symbol. It turns the outer product $a \times b$ into the multiplication of the matrix and the vector $a \wedge b$ is a linear operation. This symbol will be used frequently in the following sections. It is a one-to-one mapping, meaning that for any vector, it corresponds to a unique anti-symmetric matrix, and vice versa: $$ a \wedge = \begin{bmatrix} 0 &amp; -a_3 &amp; a_2 \\ a_3 &amp; 0 &amp; -a_1 \\ -a_2 &amp; a_1 &amp; 0 \end{bmatrix}$$. At the same time, note that the vector operations such as addition, subtraction, inner and outer products can be calculated even when we do not have their coordinates. For example, although the inner product can be expressed by the sum of the two vectors’ product when we know the coordinates, the length and angle can also be calculated even if their coordinates are unknown. Therefore, the inner product result of the two vectors is independent of the selection of the coordinate system. Euclidean Transforms between Coordinate Systems: We often define a variety of coordinate systems in the real scene. In robotics, you define one coordinate system for each link and joint; in 3D mapping, we also define a coordinate system for each cuboid and cylinder. If we consider a moving robot, it is common practice to set a stationary inertial coordinate system (or world coordinate system), such as the $x_W, y_W, z_W$ defined in the picture above. Meanwhile, the camera or robot is a moving coordinate system, such as coordinate system defined by $x_C, y_C, z_C$. We might ask: a vector $p$ in the camera system may have coordinates $p_c$; and in the world coordinate system, its coordinates maybe $p_w$. Then what is the conversion between two coordinates? It is necessary to first obtain the coordinate values of the point in the camera system and then use the transform rule to do the coordinate transform. We need a mathematical way to describe this transformation. As we will see later, we can describe it with a transform matrix $T$. Intuitively, the motion between two coordinate systems consists of a rotation plus a translation, which is called rigid body motion. Obviously, the camera movement is rigid. During the rigid body motion, the length and angle of the vector will not change. Imagine that you throw your phone into the air and there may be differences in spatial position and orientation. But the length and the angle of each face will not change. At this point, we say that the phone’s motion is Euclidean. The Euclidean transform consists of rotation and translation. Let’s first consider the rotation. We have a unit-length orthogonal base $(e_1, e_2, e_3)$. After a rotation it becomes $(e_1’, e_2’, e_3’)$. Then, for the same vector $a$ (the vector does not move with the rotation of the coordinate system). its coordinates in these two coordinate systems are $[a_1, a_2, a_3]^T$ and $[a_1’, a_2’, a_3]^T$. Because the vector itself has not changed, according to the definition of coordinates, there are: $$ [e_1, e_2, e_3] \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = [e_1’, e_2’, e_3’] \begin{bmatrix} a_1’ \\ a_2’ \\ a_3’ \end{bmatrix}$$. To describe the relationship between the two coordinates, we multiply the left and right side of the above equation by $\begin{bmatrix} e_1^T \\ e_2^T \\ e_3^T \end{bmatrix}$, then the matrix on the left becomes an identity matrix, so: $$\begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} \triangleq Ra’$$. We take the intermediate matrix out and define it as a matrix $R$. This matrix consists of the inner product between the two sets of bases, describing the same vector’s coordinate transformation relationship before and after the rotation. It can be said that the matrix $R$ describes the rotation itself. So we call it the rotation matrix. Meanwhile, the components of the matrix are the inner product of the two coordinate system bases. Since the base vector’s length is $1$, it is actually the cosine of the angle between the base vectors. So this matrix is also called direction cosine matrix. The rotation matrix has some special properties. In fact, it is an orthogonal matrix with a determinant of $1$. Conversely, an orthogonal matrix with a determinant of 1 is also a rotation matrix. So you can define a set of $n$ dimensional rotation matrices as follows: $$SO(n) = \{ R \in \mathbb{R}^{n \times n}| RR^T = I, det(R) =1 \} $$. $SO(n)$ refers to the special orthogonal group. This set consists of a rotation matrix of $n$ dimensional space, in particular, $SO(3)$ refers to the rotation of the three-dimensional space. In this way, we can talk directly about the rotation transformation between the two coordinate systems without having to start from the bases. Since the rotation matrix is orthogonal, its inverse (i. e. , transpose) describes an opposite rotation. According to the above definition, there are:$$a’ = R^{-1}a = R^Ta$$. Obviously, the $R^T$ represents an opposite rotation. In the Euclidean transformation, there is a translation in addition to rotation. Consider the vector $a$ in the world coordinate system. After a rotation (depicted by $R$) and a translation of $t$, we get $a’$. Then we can put the rotation and translation together, and have:$$a’= Ra+t$$,where $t$ is called a translation vector. Compared to the rotation, the translation part simply adds the translation vector to the coordinates after the rotation, which si very simple. By the above formula, we completely describe the coordinate transformation relationship using a rotation matrix $R$ and a translation vector $t$. In practice, we may define the coordinate system 1 and 2, then the vector $a$ under the two coordinates is $a_1, a_2$. The relationship between the two systems should be: $$a_1 = R_{12} a_2 + t_{12}$$. Here, $R_{12}$ means the “rotation of the vector from system 2 to system 1”. About $t_{12}$, readers may just take it as a translation vector without wondering about its physical meaning. In fact, it corresponds to a vector from the system 1’s origin pointing to system 2’s origin, and the coordinates are taken under tsystem 1. So I suggest you to understand it as “a vector from 1 to 2”. But the reverse $t_{21}$, which is a vector from $2$’s origin to $1$’s origin, whose coordinates are taken in system $2$, is not equal to $-t_{12}$. It is also related to the rotation of the two systems. Therefore, when beginners ask the question “What are my coordinates?”, we need to clearly explain this sentence’s meaning. Here, “my coordinates” normally refers to the vector from the world system $W$ pointing to the origin of the camera system $C$, and then take the coordinates in the world’s base. Corresponding to the mathematical symbol, it should be the value of $t_{WC}$. For the same reason, it is not $-t_{CW}$ but actually $-R^T_{CW} t_{CW}$. Transform Matrix and Homogeneous Coordinates: The formula $a’ = Ra+t$ fully expresses the rotation and the translation of Euclidean space, but there is still a small problem: the transformation relationship here is not a linear relationship. Suppose we made two transformations: $R_1,t_1$ and $R_2,t_2$:$$b = R_1 a + t_1, c = R_2 b + t_2$$. So the transformation from $a$ to $c$ is: $$c = R_2 (R_1 a + t_1) + t_2$$. This form is not elegant after multiple transformations. Therefore, we introduce homogeneous coordinates and transformation matrices, rewriting the formula:$$ \begin{bmatrix} a’ \\ 1 \end{bmatrix} = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \begin{bmatrix} a \\ 1 \end{bmatrix} \triangleq T \begin{bmatrix} a \\ 1 \end{bmatrix}$$. This is a mathematical trick: we add $1$ at the end of the 3D vector and turn it into a 4D vector called homogeneous coordinates. For this four-dimensional vector, we can write the rotation and translation matrix, making the whole relationship a linear relationship. In this formula, the matrix $T$ is called transform matrix. We temporarily use $\tilde{a}$ to represent the homogeneous coordinates of $a$. Then, relying on homogeneous coordinates and transformation matrices, the superposition of the two transformations can have a good form: $$\tilde{b} = T_1 \tilde{a}, \tilde{c} = T_2 \tilde{b} \Rightarrow T_2 T_1 \tilde{a}$$. But the symbols that distinguish between homogeneous and non-homogeneous coordinates are annoying, because here we only need to add 1 at the end of the vector or remove 1 to turn it into a normal vector. So, without ambiguity, we will write it directly as $b = Ta$ and by default we just assume a homogeneous coordinate conversion is made if needed. The transformation matrix $T$ has a special structure: the upper left corner is the rotation matrix, the right side is the translation vector, the lower-left corner is $0$ vector, and the lower right corner is $1$. This set of transform matrix is also known as the special Euclidean group: $$SE(3) = \{ T = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix} \in \mathbb{R}^{4 \times 4}| R \in SO(3), t \in \mathbb{R}^3 \}$$. Like $SO(3)$, the inverse of the transformation matrix represents an inverse transformation: $$T^{-1} = \begin{bmatrix} R^T &amp; -R^T t \\ 0^T &amp; 1 \end{bmatrix}$$. Again, we use the notation of $T_{12}$ to represent a transformation from 2 to 1. Because the conversion between homogeneous and non-homogeneous coordinates is actually very easy, it is assumed that the conversion from homogeneous coordinates to normal coordinates is already done. Summary: First, we introduced the vector and its coordinate representation and introduced the operation between the vectors; then, the motion between the coordinate systems is described by the Euclidean transformation, which consists of translation and rotation. The rotation can be described by the rotation matrix $SO(3)$, while the translation is directly described by an $\mathbb{R}^3$ vector. Finally, if the translation and rotation are placed in a matrix, the transformation matrix $SE(3)$ is formed. References:  Introduction to Visual SLAM"
    }, {
    "id": 14,
    "url": "http://localhost:4000/homogeneous/",
    "title": "Homogeneous Coordinates and Projective Geometry",
    "body": "2021/08/31 - Introduction: In this article, I’m going to explain homogeneous coordinates (a. k. a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry. Terminology: Most of the time when working with 3D, we are thinking in terms of Euclidean geometry-that is, coordinates in three-dimensional space ($X$, $Y$ and $Z$). However, there are certain situations where it is useful to think in terms of projective geometry instead. Projective geometry has an extra dimension, called $W$, in addition to the $X$, $Y$, and $Z$ dimensions. This four-dimensional space is called projective space and coordinates in projective space are called homogenous coordinates. For the purposes of 3D software, the terms projective and homogeous are basically interchangeable with 4D. Not Quaternions: Quaternions look a lot like homogeneous coordinates. Both are 4D vectors, commonly depicted as $(X,Y,Z,W)$. However, quaternions and homogeneous coordinates are different concepts, with different uses. An Analogy in 2D: First, let’s look at how projective geometry works in 2D, before we move on to 3D.  Imagine a projector that is projecting a 2D image onto a screen. It’s easy to identify the $X$ and $Y$ dimensions of the projected image.  Now, if you step back from the 2D image and look at the projector and the screen, you can see the $W$ dimension, too. The $W$ dimension is the distance from the projector to the screen.  So what does the $W$ dimension do, exactly? Imagine what would happen to the 2D image if you increased or decreased $W$-that is, if you increased or decreased the distance between the projector and the screen. If you move the projector closer to the screen, the whole 2D image becomes smaller. If you move the projector away from the screen, the 2D image becomes larger. As you can see, the value of $W$ affects the size (a. k. a scale) of the image. Applying it to 3D: There is no such thing as a 3D projector (yet), so its’ harder to imagine projective geometry in 3D, but the $W$ value works exactly the same as it does in 2D. When $W$ increases, the coordinates expands (scales up). When $W$ decreases, the coordinates shrinks (scales down). The $W$ is basically a scaling transformation for the 3D coordinates. When $W = 1$: The usual advice for 3D programming beginners is to always set $W=1$ whenever converting a 3D coordinate to 4D coordinate. The reason for that is that when you scale a coordinate by a 1 it doesn’t shrink or grow, it just stays the same size. So, when $W=1$, it has no effect on the $X$, $Y$, or $Z$ values. For this reason, when it comes to 3D computer graphics, coordinates are said to be correct only when $W=1$. If you tried to render with $W=0$ your program would crash when it attempted to divide by zero. With $W&lt;0$ everything would flip unside-down and back-to-front. Mathematically speaking, there is no such thing as an incorrect homogeneous coordinate. Using coordinates with $W=1$ is just a useful convention for the 3D computer grahics. The Math: Now, let’s look at some actual numbers, to see how the math works.  Let’s say that the projector is $3$ meters away from the screen, and there is a dot on the 2D image at the coordinate $(15, 21)$. This gives us the projective coordinate vector $(X,Y,W) = (15,21,3)$.  Now imagine that the projector was pushed closer to the screen so that the distance was $1$ meter. The closer the project gets to the screen, the smaller the image becomes. The projector has moved three times closer, so the image becomes three times smaller. If we take the original coordinate vector and divide all the values by three, we get the new vector where $W=1$:  $$(\frac{15}{3}, \frac{21}{3}, \frac{3}{3}) = (5,7,1)$$. The dot is now at coordinate $(5,7)$. This is how an incorrect homogeneous coordinate is converted to a correct coordinate: divide all the values by $W$. The process is exactly the same for 2D and 3D coordinates. Dividing all the values in a vector is done by a scalar multiplication with the reciprocal of the divisor. Here is a 4D example:  $$\frac{1}{5}(10, 20, 30, 5) = (\frac{10}{5}, \frac{20}{5}, \frac{30}{5}, \frac{5}{5}) = (2,4,6,1)$$ Uses of Homogeneous Coordinates in Computer Graphics: As mentioned earlier, in regard to 3D computer graphics, homogeneous coordinates are useful in certain situations. We will look at some of those situations here. Translation Matrices for 3D Coordinates:  A four-column matrix can only be multiplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. Rotation and scaling transformation matrices only require three columns. But, in order to do translation, the matrices need to have at least four columns. This is why transformations are often $4 \times 4$ matices. However, a matrix with four columns cannot be multiplied by a 3D vector, due to the rules of matrix multiplication. A four-column matrix can only be mulitplied with a four-element vector, which is why we often use homogeneous 4D vectors instead of 3D vectors. The 4th dimension $W$ is usually unchanged, when using homogeneous coordinates in matrix transformation. $W$ is set to $1$ when converting 3D coordinates into 4D, and is usually still $1$ after the transformation matrices are applied, at which point it can be converted back into a 3D coordinate by ignoring $W$. This is true for all translation, rotation, and scaling transformations, which by far are the most common types of transformations. The notable exception is projection matrices, which do affect the $W$ dimension. Perspective Transformation: In 3D, perspective is the phenomenon where an object appears smaller the further away it is from the camera. A far-away mountain can appear to be smaller than a cat, if the cat is close enough to the camera. Perspective is implemented in 3D computer graphics by using a transformation matrix that changes the $W$ element of each vertex. After the camera matrix is applied to each vertex, but before the projection matrix is applied, the $Z$ element of each vertex represents the distance away from the camera. Therefore, the larger $Z$ is, the more the vertex should be scaled down. The $W$ dimension affects the scale, so the projection matrix just changes the $W$ based on the $Z$ value. Here is an example of a perspective projection matrix being applied to a homogeneous coordinate:  $$ \begin{bmatrix} 1&amp;0&amp;0&amp;0 \\ 0&amp;1&amp;0&amp;0&amp; \\ 0&amp;0&amp;1&amp;0 \\ 0&amp;0&amp;1&amp;0 \end{bmatrix} \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1\end{bmatrix} = \begin{bmatrix} 2 \\ 3 \\ 4 \\ 1 \end{bmatrix} $$ Notice how the $W$ value is changed to $4$, which comes from the $Z$ value. After the perspective projection matrix is applied, each vertex undergoes perspective division. Perspective division is just a specific term for converting the homogeneous coordinate back to $W=1$, as explained earlier in the article. Continuing with the example above, the perspective division step would look like this:  $$\frac{1}{4}(2,3,4,4) = (0. 5,0. 75, 1,1)$$ After perspective division, the $W$ value is discarded, and we are left with a 3D coordinate that has been correctly scaled according to a 3D perspective projection. Positioning Directional Lights: One property of homogeneous coordinates is that they allow you to have points at infinity (infinite length vectors), which is not possible with 3D coordinates. Points at infinity occur when $W=0$. If you try to convert a $W=0$ homogeneous coordinate into a normal $W=1$ coordinate, it results in a bunch of divide-by-zero operations:  $$ \frac{1}{0}(2,3,4,0) = (\frac{2}{0}, \frac{3}{0}, \frac{4}{0}, \frac{0}{0})$$. This means that homogeneous coordinates with $W=0$ can not be converted back into 3D coordinates. What use does this have? Well, directional lights can be thought of as point lights that are infinitely far away. When a point light is infinitely far away, the rays of light become parallel, and all of the light travels in a single direction. This is basically the definition of a directional light. So, traditionally, in 3D graphics, directional lights are differentiated from point lights by the value of $W$ in the position vector of the light. If $W=1$, then it is a point light. If $W=0$, then it is a directional light. This is more of a traditional convention, rather than a useful way to write lighting code. Directional lights and point lights are usually implemented with separate code, because they behave differently. Summary: Homogeneous coordinates have an extra dimension called $W$, which scales the $X$, $Y$, and $Z$ dimensions. Matrices for translation and perspective projection can only be applied to homogeneous coordinates, which is why they are so common in 3D computer graphics. The $X$, $Y$, and $Z$ values are said to be correct when $W=1$. Any homogeneous coordinates can be converted to have $W=1$ by dividing all four dimensions by the $W$ value, except if $W=0$. When $W=0$, the coordinate represents a point at infinity (a vector with infinite length), and this is often used to denote the direction of directional lights. References:  Tomdalling’s Blog Post Image Processing and Computer Vision Lecture Notes"
    }, {
    "id": 15,
    "url": "http://localhost:4000/3d-cnn/",
    "title": "3D CNN",
    "body": "2021/08/31 - def print_hi(name) puts  Hi, #{name} endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




</ul>
<form class="bd-search hidden-sm-down" onsubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small" id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
    
 
    
<!-- Begin post excerpts, let's highlight the first 4 posts on top -->
<div class="row remove-site-content-margin">
    
    <!-- latest post -->
    
    <div class="col-md-6">
    <div class="card border-0 mb-4 box-shadow">   
    <a href="/inverse/">
	    <!--<div class="topfirstimage" style="background-image: url( /assets/images/depth/12.png); height: 200px;    background-size: cover;    background-repeat: no-repeat;"></div>-->
    <img width="550px" height="200px" src="/assets/images/depth/12.png" style="height: 210px;" align="middle">
    </a>
    <div class="card-body px-0 pb-0 d-flex flex-column align-items-start">
    <h2 class="h4 font-weight-bold">
    <a class="text-dark" href="/inverse/">Inverse Projection Transformation</a>
    </h2>
    <p class="excerpt">
        When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in whic...
    </p>
    <div>
        <small class="d-block text-muted">
            In <span class="catlist">
                
                <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                
                </span>                   
        </small>
        <small class="text-muted">
            Sep 03, 2021
        </small>
    </div>
    </div>
    </div>
    </div>
    
    <div class="col-md-6">
        
        <!-- second latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/depth/">
                 <img height="110px" width="180px" src="http://localhost:4000/assets/images/depth/1.jpg" alt="Depth Estimation: Basics and Intuition" align="middle">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/depth/">Depth Estimation: Basics and Intuition</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 02, 2021
                    </small>
                </div>
            </div>
        
        <!-- third latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/rigid2/">
                 <img width="180px" height="110px" src="/assets/images/rigid/1.png" alt="3D Rigid Body Motion (Part 2)">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/rigid2/">3D Rigid Body Motion (Part 2)</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Sep 01, 2021
                    </small>
                </div>
            </div>
        
        <!-- fourth latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/rigid/">
                <img width="180px" height="110px" src="/assets/images/rigid.png" alt="3D Rigid Body Motion (Part 1)">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/rigid/">3D Rigid Body Motion (Part 1)</a>
                    </h2>
                    <small class="d-block text-muted">
                        In <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Aug 31, 2021
                    </small>
                </div>
            </div>
        
    </div>
    
</div>
    
<!-- Sticky - add sticky tag to the post you want to highlight here - tags: [sticky] -->
 

 

 

 

 

<div class="jumbotron jumbotron-fluid jumbotron-home pt-0 pb-0 mt-3 mb-2rem bg-lightblue position-relative">
    <div class="pl-4 pr-0 h-100 tofront">
        <div class="row justify-content-between">
            <div class="col-md-6 pt-6 pb-6 pr-lg-4 align-self-center">
                <h1 class="mb-3">Homogeneous Coordinates and Projective Geometry</h1>
                <p class="mb-3 lead">
                    IntroductionIn this article, I’m going to explain homogeneous coordinates (a.k.a 4D coordinates) as simply as I can. It is now time t...
                </p>
                <a href="/homogeneous/" class="btn btn-dark">Read More</a>
            </div>
	    <!--<div class="col-md-6 d-none d-md-block pr-0" style="background-size:contain;background-repeat:no-repeat;background-image:url(/assets/images/home.png);">-->
	     <img class="col-md-6 d-none d-md-block pr-0" src="/assets/images/home.png" align="midde">
            </div>
        </div>
    </div>
</div> 

 




    


 <!--endif page url is / -->
    


<!-- Now the rest of the posts with the usual loop but with an offset:4 on the first page so we can skeep the first 4 posts displayed above -->
    
<div class="row mt-3">
   
    <div class="col-md-8 main-loop">
        
        <h4 class="font-weight-bold spanborder"><span>All Stories</span></h4>
        

        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/inverse/">Inverse Projection Transformation</a>
	</h2>
	<p class="excerpt">
	   When an image of a scene is captured by a camera, we lose depth information. This is also known as projective transformation, in which points in the world are converted to pixels on a...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 03, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/inverse/">
	<img height="150px" width="200px" src="/assets/images/depth/12.png" alt="Inverse Projection Transformation">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/depth/">Depth Estimation: Basics and Intuition</a>
	</h2>
	<p class="excerpt">
	   Introduction
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 02, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/depth/">
	<img height="150px" width="200px" src="/assets/images/depth/1.jpg" alt="Depth Estimation: Basics and Intuition">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/rigid2/">3D Rigid Body Motion (Part 2)</a>
	</h2>
	<p class="excerpt">
	   Rotation Vectors and Euler AnglesRotation VectorsWith a rotation matrix to describe the rotation, is it enough to use a $4 \times 4$ transformation matrix to represent a 6-degree-of-f...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Sep 01, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/rigid2/">
	<img height="150px" width="200px" src="/assets/images/rigid/1.png" alt="3D Rigid Body Motion (Part 2)">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/rigid/">3D Rigid Body Motion (Part 1)</a>
	</h2>
	<p class="excerpt">
	     The goal of this article is to introduce the rigid body geometry in 3-dimensional space: rotation matrix, transformation matrix, quaternion, and Euler angle.
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Aug 31, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/rigid/">
	<img height="150px" width="200px" src="/assets/images/rigid.png" alt="3D Rigid Body Motion (Part 1)">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/homogeneous/">Homogeneous Coordinates and Projective Geometry</a>
	</h2>
	<p class="excerpt">
	   IntroductionIn this article, I’m going to explain homogeneous coordinates (a.k.a 4D coordinates) as simply as I can. It is now time to take a closer look at projective geometry.
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Aug 31, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/homogeneous/">
	<img height="150px" width="200px" src="/assets/images/home.png" alt="Homogeneous Coordinates and Projective Geometry">
	</a>
	</div>

</div>

        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/3d-cnn/">3D CNN</a>
	</h2>
	<p class="excerpt">
	   def print_hi(name)  puts "Hi, #{name}"endprint_hi('Tom')#=&gt; prints 'Hi, Tom' to STDOUT.
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#deep%20learning">deep learning</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Aug 31, 2021
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/3d-cnn/">
	<img height="150px" width="200px" src="/assets/images/cnn.png" alt="3D CNN">
	</a>
	</div>

</div>

        
        
        
        <div class="mt-5">
         <!-- Pagination links -->
                  
        </div>
        
    </div>
    
    <div class="col-md-4">
        <div class="sticky-top sticky-top-offset">
    <h4 class="font-weight-bold spanborder"><span>Featured</span></h4>  
    <ol class="list-featured">				
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/inverse/" class="text-dark">Inverse Projection Transformation</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/depth/" class="text-dark">Depth Estimation: Basics and Intuition</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/rigid2/" class="text-dark">3D Rigid Body Motion (Part 2)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/rigid/" class="text-dark">3D Rigid Body Motion (Part 1)</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#computer%20vision">computer vision</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/3d-cnn/" class="text-dark">3D CNN</a>
                </h6>
                <span class="d-block text-muted">
                    In <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#deep%20learning">deep learning</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
           
    </ol>
</div>     
    </div>
    
</div>





    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/custom.js"></script>
    

    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>Seri Lee</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                
            </div>
            </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
