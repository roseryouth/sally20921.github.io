---
layout: post
title:  "Group Normalization"
author: seri
categories: [ deep learning ]
image: assets/images/depth/12.png
tags: featured 
---
<h2> Introduction </h2>
In this blog post, we will learn about <span class="blue"> Group Normalization </span>. Batch Normalization is used in most state-of-the-art computer vision to stabilize training. Batch Normalization normalizes the features based on the <dfn> min </dfn> and <dfn> variance </dfn> in a mini-batch. This has helped improve model performance, reducing training time and also helped very deep models converge.

But this technique also suffers from drawbacks-if the batch size is too small, training becomes unstable with Batch Normalization. Through this blog post, I hope to introduce Group Normalization as an alternative to Batch Normalization and help the readers develop an intuition for cases where Group Normalization could perform better than Batch Normalization. 

<h2> Drawbacks of Batch Normalization </h2>

<blockquote> We all know that Batch Normalization has beend established as a very effective component in deep learning. Batch Normalization normalizes the features by the mean and variance computed within a batch. But despite its great success, Batch Normalization exhibits drawbacks that are also caused by its distinct behavior of normalizing along the batch dimension. In particular, it is required for Batch Normalization to work with sufficiently large batch size. A small batch size leads to inaccurate estimation of the batch statistics and reducing BN's batch size increases the model error dramatically. </blockquote>

Essentially, what that means is that BN is not very effective if the batch sizes are too small. Especially for Computer Vision applications other than image classification such as object detection, segmentation, video classification, the restriction on batch sizes are more demanding and it is difficult to have higher batch sizes.

Espeically in such cases, Group Normalization can be used as a strong alternative to BN. Or, there could be cases where you might want to try a bigger capacity model leaving less space in the GPU to fit a bigger batch size. In such cases as well, you might want to try Group Normalization as an alternative.

<h2> Introduction to Group Normalization </h2>
<blockquote> Group Normalization divides the channels into groups and computes within each group the mean and variance for normalization. Group Normalization is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. </blockquote>
 
Essentially, Group Normalization takes away the dependence on batch size for normalization and in doing so mitigates the problem suffered by BN. It is essential for the readers to realize that instead of normalizing across the batch dimension, Group Normalization normalizes across the groups (or channel dimensions). 

<h2> Other Normalization Techniques </h2>

<h2> References </h2>
<ul> 
<li><a href="https://amaarora.github.io/2020/08/09/groupnorm.html"> blog post </a></li>
<li><a href="https://towardsdatascience.com/what-is-group-normalization-45fe27307be7"> medium article </a></li>
</ul>


