---
layout: post
title:  "3D Packing for Self-Supervised Depth Estimation"
author: seri
categories: [ paper ]
image: assets/images/cnn.png
tags: featured
---

<h2> Self-Supervised Scale-Aware SfM </h2>
In self-supervised monocular SfM training, we aim to learn:
<ul><li> a monocular depth model $f_D: I \rightarrow D$, that predicts the scale-ambiguous depth $\hat{D} = f_D(I(p))$ for every pixel $p$ in the target image $I$; </li>
<li> a monocular ego-motion estimator $f_x: (I_t, I_S) \rightarrow x_{t \rightarrow S}$ that predicts the set of 6-DoF rigid transformations for all $s \in S$ given by $x_{t \rightarrow s} = givne by $x_{t \rightarrow s} = \begin{pmatrix} R & t \\ 0 & 1 \end{pmatrix} \in SE(3)$ between the target image $I_t$ and the set of source images $I_s \in I_S$ considered as part of the temporal context.</li></ul>
In practice, we use the frames $I_{t-1}$ and $I_{t+1}$ as source images, although using a larger context is possible. Note that in the case of monocular SfM both depth and pose are estimated up to an unknown scale factor, due to the inherent ambiguity of the photometric loss.


