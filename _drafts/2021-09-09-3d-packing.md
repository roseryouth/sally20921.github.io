---
layout: post
title:  "3D Packing for Self-Supervised Depth Estimation"
author: seri
categories: [ paper ]
image: assets/images/cnn.png
tags: featured
---

<h2> Self-Supervised Scale-Aware SfM </h2>

In self-supervised monocular SfM training, we aim to learn: <ul><li> a monocular depth model $f_D = I \rightarrow D$ that predicts the scale-ambiguous depth $\hat{D} = f_D(I(p))$ for every pixel $p$ in the target image $I$;</li> <li> a monocular ego-motion estimator $f_x: (I_t, I_s) \rightarrow x_{t \rightarrow s}$ that predicts the set of 6-DoF rigid transformations for all $s \in S$ given by $x_{t \rightarrow s} = \begin{pmatrix} R & t \\ 0 & 1 \end{pmatrix} \in SE(3)$, between the target image $I_t$ and a set of source images $I_s \in I_S$ considered as part of the temporal context.</li></ul> In practice, we use the frames $I_{t-1}$ and $I_{t+1}$ as source images, although using a larger context is possible. Note that in the case of monocular SfM, both depth and pose are estimated up to an unknown scale factor, due to the inherent ambiguity of the photometric loss. 

<h3> Self-Supervised Objective </h3>
Following the work of Zhou et al., we train the depth and pose network simultaneously in a self-supervised manner. In this work, however, we learn to recover the inverse depth $f_d: I \rightarrow f_D^{-1}(I)$ instead, along with the ego-motion estimator $f_x$. The overall self-supervised objective consists of an appearance matching loss term $L_p$ that is imposed between the synthesized image $\hat{I_t}$ and the target image $I_t$, and a depth regularization term $L_s$ that ensures edge-aware smoothing in the depth estimates $\hat{D_t}$. The objective takes the following form:

$$
L(I_t, \hat{I_t}) = L_p(I_t, I_s) \bigodot M_p \bigodot M_t + \lambdda_1 L_s(\hat{D_t})
$$ 

where $M_t$ is a binary mask that avoids computing the photometric loss on the pixels that do not have a valid mapping, and $\bigodot$ denotes element-wise multiplication. Additionally, $\lambda_1$ enforces a weighted depth regularization on the objective. The overall loss is averaged per-pixel, pyramid-scale and image batch during training. 

<picture>
<img src="{{site.baseurl}}/assets/images/packnet.png">
</picture>
<span class="caption"> The proposed scale-aware self-supervised monocular structure-from-motion architecture. This paper introduces PackNet as a novel depth network, and optionally include weak velocity supervision at training time to produce scale-aware depth and pose models.</span>

<h4> Appearance Matching Loss </h4>
The pixel-level similarity between the target image $I_t$ and the synthesized target image $\hat{I_t}$ is estimated using the Structural Similarity (SSIM) term combined with a L1 pixel-wise loss term, inducing an overall photometric loss given by the equation below.

$$
L_p(I_t, \hat{I_t}) = \alpha \frac{1-SSIM(I_t, \hat{I_t})}{2} + (1 - \alpha) \lVert I_t - \hat{I_t} \rVert
$$ 

While multi-view projective geometry provides strong cues for self-supervision, errors due to parallax in the scene have an undesirable effect incurred on the photometric loss. We mitigate these undesirable effects by calculating the minimum photometric loss per pixel for each source image in the context $I_S$, so that:

$$ L_p(I_t, I_S) = \min_{I_S} (I_t, \hat{I_t})
$$ 

The intuition is that the same pixel level will not be occluded or out-of-bounds in all context images, and that the association with minimal photometric loss should be the correct one. Furthermore, we also mask out static pixels by removing those which have a warped $L_p(I_t, \hat{I_t})$ higher than their corresponding unwarped photometric loss $L_p(I_t, I_s)$, calculated using the original source image without view synthesis. This auto-mask removes pixels whose appearance does not change between frames, which include static scenes and dynamic objects with no relative motion, since these will have smaller photometric loss when assuming no ego-motion. 

$$
M_p = \min_{I_S} L_p(I_t, I_s) \geq L_p(I_t, \hat{I_t})
$$

<h4> Depth Smoothness Loss </h4>
In order to regularize the depth in texture-less low-image gradient regions, we incorporate an edge-aware term. The loss is weighted for each of the pyramid-levels, and is decayed by a factor of 2 on down-sampling, starting with a weight of 1 for the 0th pyramid level. 

$$
L_s(\hat{D_t}) = |\delta_x \hat{D_t}|e^{-|\delta_x I_t|} + |\delta_y \hat{D_t}| e^{|\delta_y I_t|}
$$

<h3> Scale-Aware SfM </h3>
As previously mentioned, both the monocular depth and ego-motion estimators $f_d$ and $f_x$ predict scale-ambiguous values, due to the limitations of the monocular SfM training objective. In other words, the scene depth and the camera ego-motion can only be estimated up to an unknown and ambiguous scale factor. This is also reflected in the overall learning objective, where the photometric loss is agnostic to the metric depth of the scene. Furthermore, we note that all previous approaches which operate in the self-supervised monocular regime suffer from this limitation, and resort to artificially incorporating this scale factor at test-time, using LiDAR measurements. 

<h4> Velocity Supervision Loss </h4>

<h2> PackNet: 3D Packing for Depth Estimation </h2>
Standard convolutional architectures use aggresive striding and pooling to increase their receptive field size. However, this potentially decreases the model performance for tasks requiring fine-grained representations. Similarly, traditional upsampling strategies fail to propagate and preserve sufficient details at the decoder layers to recover accurate depth predictions. In contrast, we propose a novel encoder-decoder architecture, called PackNet, that introduces new 3D packing and unpacking blocks to leran to jointly preserve and recover important spatial information for depth estimation. This is in alignments with recent observations that information loss is not a necessary condition to learn representations capable of generalizing to different scenarios. In fact, progressive expansion and contraction in a fully invertible manner, without discarding <span class="highlight-pink"> uninformative </span> input variability, has been shown to increase performance in a variety of tasks. We first describe the different blocks of our proposed architecture, and then proceed to show how they are integrated together in a single model for monocular depth estimation.

<h3> Packing Block </h3>
The packing block starts by folding the spatial dimensions of convolutional feature maps into extra feature channels via a <span class="monospace"> Space2Depth </span> operation. The resulting tensor is at a reduced resolution, but in contrast to striding or pooling, this transformation is invertible and comes at no loss. Next, we learn to compress this concatenated feature space in order to reduce its dimensionality to a desired number of output channels. As shown in experiments, 2D convolutions are not designed to directly leverage the tiled structure of this feature space. Instead, we propose to first learn to expand this structured representation via a 3D covolutional layer. The resulting higher dimensional feature space is then flattened (by simple reshaping) before a final 2D convolutional contraction layer. This structured feature expansion-contraction, inspired by invertible networks, although we do not ensure invertibility, allows our architecture to dedicate more parameters to learn how to compress key spatial details that need to be preserved for high resolution depth decoding. 

<h3> Unpacking Block </h3>
Symmetrically, the unpacking block learns to decompress and unfold packed convolutional feature channels back into higher resolution spatial dimensions during the decoding process. The unpacking block replaces convolutional feature feature upsampling, typically performed via nearest-neighbor or with learnable transposed convolutional weights. It is inspired 

