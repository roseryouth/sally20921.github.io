---
layout: post
title:  "Dissecting the Camera Matrix (Part 1)"
author: seri
categories: [ computer vision ]
image: assets/images/camera/camera.jpeg
tags: featured 
---

This blog post covers the process of decomposing a camera matrix into intrinsic and extrinsic matrices, and and I will try to untangle the issues that can crop-up with different coordinate conventions.   <!--more-->

<h2> Camera Calibration and Decomposition </h2>
Primarily, camera calibration is about finding the quantities internal to the camera that affect the imaging process. Here are some of the factors that will be taken care of:
<ul>
<li> <span class="blue"> image center </span>: we need to find the position of the image center in the image. Wait, isn't the image center located at $(width/2, height/2)$? Well, no. Unless we calibrate the camera, the image will almost always appear to be off-center.</li>
<li> <span class="blue"> focal length </span>: this is a very important parameter. Remember how people using DSLR cameras tend to <mark> focus </mark> on things before capturing the image? this parameter is directly related to the <mark> focus </mark> of the camera and it is very critical.</li>
<li> <span class="blue"> scaling factors </span>: the scaling factors for row pixels and column pixels might be different. If we don't take care of this thing, the image will look stretched (either horizontally or vertically).</li>
<li> <span class="blue"> skew factor: this refers to shearing. the image will look like a parallelogram otherwise. </li>
<li> <span class="blue"> lens distortion: this refers to the pseudo zoom effect that we see near the center of any image. </li> 

<small class="sidenote"> shearing refers to a transformation in which all points along a given line $L$ remain fixed while other points are shifted parallel to $L$ by a distance proportional to their perpendicular distance from $L$. </small>

<h2> Pinhole Camera Model </h2>
<picture><img src="{{site.baseurl}}/assets/images/pinhole.png"></picutre>

Before we jump into anything, let's see where this all began. When we capture an image, we are basically mapping the 3D scene to a 2D scene. It means that every point in the 3D world gets mapped to the 2D plane of our image. This is called the <span class="red"> pinhole camera model </span>. It basically describes the relationship between the coordinates of the 3D point and its projection on the 2D image. This, of course, is the ideal case where <span class="highlight-yellow"> there is absolutely no distortion </span> of any kind. Every camera is modeled based on this, and every camera aspires to simulate this as close as possible. But in the real world, we have to deal with things like geometric distortions, blurring, finite sized apertures, etc. 

<picture><img src="{{site.baseurl}}/assets/images/pinhole2.png"></picture>

The figure shown here depicts a pinhole camera model. The camera is placed at the origin $O$. The point $P$ represents a point in the real world. We are trying to capture that onto a 2D plane. The <span class="rainbow"> image plane </span> represents the 2D plane that you get after capturing the image. The image plane actually contains the image that you see after capturing a picture. So basically, we are trying to map every 3D point to a point on the image plane. In this case, the point $P$ gets mapped to $P_c$. The distance between the origin $O$ and this image plane is called the <span class="highlight-sketch"> focal length </span> of the camera. This is the parameter you modify when you adjst the <mark> focus </mark> of the camera. 

<h2> Intrinsic and Extrinsic Parameters </h2> 
In the above figure, we want to estimate $(u,v)$ from $(X,Y,Z)$. Let's say the focal length is denoted by $f$. If you look at the triangle formed using the origin-$P_c$-and the $Z$-axis with the origin-$P$ and $Z$-axis, you will notice that they are similar triangles. This means that $u$ depends on the $f$, $X$ and $Z$. Similarly, $v$ depends on $f$, $Y$ and $Z$. 

$$
u = fX/Z
v = fY/Z
$$

Next, if the origin of the 2D image coordinate system does not coincide with where the $Z$-axis intersects the image plane, we need to translate $P_c$ into the desired origin. Let this translation be defined by $(t_u, t_v)$. So now, $u$ and $v$ are given by:

$$
u = fX/Z + t_u
v = fY/Z + t_v
$$

So up until now, we have something that can translate $(X,Y,Z)$ to $(u,v)$. Let's denote this matrix $M$. So we can write:

$$ 
P_c = MP
$$


Since this is a camera image, we need to express it in inches. For this, we will need to know the resolution of the camera in pixels/inch. If the pixels are square the resolution will be identical in both $u$ and $v$ directions of the camera image coordinates. However, for a more general case, we assume rectangular pixels with resolution $m_u$ and $m_v$ pixels/inch in $u$ and $v$ directions respectively. Therefore, to measure $P_c$ in pixels, its $u$ and $v$ coordinates should be multiplied by $m_u$ and $m_v$ respectively. So now, this new transformation matrix depends on $f, X, Y, Z, t_u, t_v, m_u, m_v$. Let's denote this by:

$$
P_c = KP
$$

Here, $K$ is called the <span class="typewriter"> intrinsic parameter matrix </span> for the camera.

Now, if the camera does not have its center of projection at $(0,0,0)$ and is oriented in an arbitrary fashion (not necessarily $z$-perpendicular to the image plane), then we need roation and translation to make the camera coordinate system coincide with the configuration in that pinhole camera figure. Let the rotation applied to coincide the principal axis with $Z$-axis given by a $3 \times 3$ rotation matrix $R$. Then the matrix is formed by first applying the translation followed by the rotation is given by the $3 \times 4$ matrix.

$$
E = (R|RT)
$$

This is called the <span class="typewriter"> extrinsic parameter matrix for the camera </span>. So, the complete camera transformation can now be represented as: 

$$ 
K(R|RT) = KR(I|T)
$$

Hence, $P_c$ the projection of $P$ is given by:

$$
P_c = KR(I|T)P = CP
$$

$C$ is a $3 \times 4$ matrix usually called the complete camera calibration matrix. So basically, camera calibration matrix is used to transform a 3D point in the real world to a 2D point on the image plane considering all the things like focal length of the camera, distortion, resolution, shifting of origin, etc. 

<h2> References </h2>
<ul> 
<li><a href="https://ksimek.github.io/2012/08/14/decompose/"> ksimek blog </a></li>
<li><a href="https://prateekvjoshi.com/2014/05/31/understanding-camera-calibration/"> prateekvjoshi blog </a></li>
</ul>


