---
layout: post
title: Natural Language Processing
categories: Etc  
tags: [Hands-On Natural Language Processing with PyTorch]
excerpt: 
---

# NLP and Text Embeddings
* There are many different ways of representing text in deep learning. 
* Embeddings help to numerically define the actual meaning of certain words.
* In this chapter, we will explore text embeddings and learn how to create embeddings using a continuous BoW model.
* We will then move on to discuss n-grams and how they can be used within models.
* We will alos cover various ways in which tagging, chunking, and tokenization can be used to split up NLP into its various constituent parts.
* Finally, we will look at TF-IDF language models and how they can be useful in weighting our models toward infrequently occurring words.

# Embeddings for NLP
* Words do not have a natural way of representing their meaning. 
* In images, we already have representations in rich vectors, so it would clearly be beneficial to have a similarly rich vector representation of words. 

