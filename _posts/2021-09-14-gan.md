---
layout: post
title:  "All About Training GAN"
author: seri
categories: [ computer vision ]
image: assets/images/gan.png
tags: featured
excerpt: "I wrote a short article about what I learned training GANs. GAN is well known for its instability in training and there are pitfalls worth knowing."
---

<!--more-->


<h2> 1.  Generative Adversarial Networks </h2>

Ultimately, if everything goes well, the generator learns the true distribution of the training data and becomes really good at generating fake images. The discriminator should not be able to distinguish between real and fake images. 

Another way to look at the GAN setup is that the discriminator is trying to guide the generator by telling what real images look like. The two networks try to achieve what is called the Nash Equilibrium with respect to each other. 

<h2> 2.  Training GANs </h2>

GAN networks are a dynamic system where the optimization process is seeking not a minimum, but a equilibrium between two forces. There are no good objective metrics for evaluating whether a GAN is performing well during training, e.g. reviewing the loss is not sufficient. Instead the best approach is to visually inspect the generated examples and use subjective evaluation. 

Other quantitative measures, such as Inception Score (IS) or Frechet Inception Distance (FID) rely on pretrained models with a specific set of object classes. They lack an upper bound (which means hypothetically the highest possible score is infinity).

<h3> 2.1  Look at the Loss </h3>

In a discriminative model,the loss measures the accuracy of the prediction and we use it to monitor the progress of training. However, the loss in GAN measures how well we are doing compared with our opponent. Often, the generator cost increases but the image quality is actually improving. 


If you see the discriminator loss rapidly approaching, there is probably no chance of recovery and it is time to change something.

<h3> 2.1  Look at the Gradients </h3>
Monitor the gradients along with the losses in the networks. These can give you a good idea about the progress of training and can even help in debugging if things are not really working well. 

Ideally, the generator should receive large gradients early in the training because it needs to learn how to generate real-looking data. The discriminator on the other hand does not always get large gradients early on, because it can easily distinguish real and fake images. 

If the gradients at the layer of generator are too small, learning might be slow or not happening at all. The generator should get large gradients early on and the discriminator getting consistently high gradients at the top layer once the generator has been trained enough. 


<h2> 3. Detecting GAN Failure Modes </h2> 
The reason why GANs are difficult to train is that both generator and the discriminator are trained simultaneously in a zero-sum game. This means that improvements to one model come at the expense of the other model. 
The goal of training two models involves finding a point of equilibrium between the two competing concerns. It also means that everytime the parameters of one model are updated the nature of the optimization problem that is being solved is updated as well. The technical challenge of training two competing neural networks at the same time is that they can fail to converge. 

<ul><li class="highlight"> <b> Convergence Failure </b></li>
<div class="indent"> The fact that GANs are composed by two networks, and each of them has its loss function leads to GANs unstability. In GAN architecture, the discriminator tries to minimize a cross-entropy while the generator tries to maximize it. When discriminator confidence is high and the discriminator starts to reject the samples that are produced by the generator, generator's gradient vanishes. 

This scenario happens when the generator score reaches near zero and the discriminator score reaches near one. The discriminator is overpowering the generator. If the score does not recover from these values for many iterations, it is better to stop training. </div>

<li class="highlight"><b> Mode Collapse </b> </li>
<div class="indent"> Mode collapse is when the GAN produces a small variety of images with many duplicates. This happens when the generator is unable to learn a rich feature representation because  it learns to associate similar outputs to multiple different inputs. The most promising way to check for mode collapse is to inspect the generated images. If there is little diversity in the output and some of them are almost identical, there is likely mode collapse. If you observe this happening, you should try to increase the ability of the generator to create more diverse outputs or impair the discriminator by randomly giving false labels to real images.

Another type of behavior you should look out for is when the generator oscillates between generating specific examples in the domain. They progress from generating one kind of sample to generating another kind of sample without eventually reaching equilibrium.
</div>

<li class="highlight"> <b> Diminisheed Gradient </b> </li></ul>
<div class="indent"> This situation happens when the discriminator gets too successful that the generator gradient vanishes and learns nothing.


<h2> Lessons I Learned </h2>
<div class="three"> Use a batch size smaller than or equal to 64.</div>
<div class="textbox"> In my experience, using bigger batch sizes often hurt the performance. I suspect it fuels the problem of discriminator getting too good at discriminating the real and fake images, since large batch size means providing a lot of examples to train on. </div>

<div class="three"> Add noise to both real and synthetic data. </div>
<div class="textbox"> It is well known that making the training of discriminator more difficult is beneficial for the overall stability. Adding noise increases the complexity of the discriminator training and stabilizes the data distribution of the two competing networks. </div>

<div class="three"> Use Label Smoothing </div>
<div class="textbox"> If the label for real images is set to 1, change it to a lower value like 0.9. This solution discourages the discriminator from being overconfident. </div>

<div class="three"> Different learning rates for the generator and discriminator a.k.a. Two Time-Scale Update Rule </div>
<div class="textbox"> In my experience, choosing a higher learning rate for the discriminator(i.e. 0.0004) and a lower one(i.e. 0.0001) for the generator works well in practice. I guess the reason is that the generator has to make small steps to fool the discriminator so it does not choose fast but not precise solutions to win the adversarial game. </div>

<div class="three"> Use some kind of normalization method </div>
<div class="textbox"> For me, applying Spectral Normalization, a particular kind of normalization applied on the convolutional kernels, greatly helped the stability of training.</div>

<blockquote> I learned that hyperparameter tuning takes a lot of time and patience especially for training GANs. </blockquote>



