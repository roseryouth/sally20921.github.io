---
layout: post
title: Automatic Differentiation and Autograd
categories: PyTorch
tags: [Automate the Boring Stuff with Python] 
---

# Computational Graph
* The nodes ins a computation graph are basically operators. 
* The variables *b,c,* and *d* are created as a result of mathematical operations, where variables *a,w1,w2,w3* and *w4* are initialized by the user itself.

# Computing the gradients
* Each node of the computation graph, with the exception of leaf nodes, can be considered as a function which takes some inputs and produces an output. 

# PyTorch Autograd
## Tensor
* Tensor is a data structure which is a fundamental building block of PyTorch.
* Tensor are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU. 
* While some ways can let you explicitly define that the *requires_grad* in the constructor itself, others require you to set it manually after creation of the Tensor.
* *requires_grad* is contagious. It means that when a Tensor is created by operating on other Tensors, the *requires_grad* of the resultant Tensor would be set to True given at least one of the tensors used for creation has it's *requires_grad* set to True.

* Each Tensor has a an attribute called *grad_fn*, which refers to the mathematical operator that create the variable.
* If *requires_grad* is set to False, *grad_fn* would be None.
* If our Tensor is a leaf node, then the *grad_fn* is also None.

## Function
* All mathematical operations in PyTorch are implemented by the *torch.nn.Autograd.Function* class. This class has two important member functions we need to look at.
* The first is *forward* function, which simply computes the output using its inputs.
* The *backward* function takes the incoming gradient coming from the part of the network in front of it.
* In order to compute derivatives in our neural network, we generally call *backward* on the *Tensor* representing our loss.
* One thing to note here is that PyTorch gives an error if you call *backward()* on vector-valued Tensor.
* This means you can only call *backward* on a scalar valued Tensor.
* The mathematical entity used for such cases is called Jacobian.

* There are two ways to overcome this.
* If you just make a small change in the above code setting *L* to be the sum of all the errors, our problem will be resolved.
* Second way is, for some reason you have to absolutely call *backward* on a vector function, you can pass a *torch.ones* of size of shape of the tensor you are trying to call backward with.

## nn.Module vs nn.Functional 

* You first define an *nn.Module* object, then invoke it's forward method to run it.
* On the other hand, *nn.funcional* provides some layers / activations in form of functions that can be directly called on the input rather than defining an objection.

## Understanding Stateful-ness

