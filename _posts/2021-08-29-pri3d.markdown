---
layout: post
title:  "Pri3D: Can 3D Priors Help 2D Representation Learning?"
author: seri
categories: [ paper, rgb-d, ssl ]
image: /assets/images/pri3d.png
---

![](/assets/images/pri3d.png)
* Pri3D leverages 3D priors for downstream 2D image understanding tasks: during pre-training, we incorporate view-invariant and geometric priors from color-geometry information given by RGB-D datasets, imbuing geometric priors into learned features. We show that these 3D-imbued learned features can effectively transfer to improved performance on 2D tasks such as semantic segmentation, object detection, and instance segmentation. 

## Abstract
> Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3D shapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints. We introduce an approach to learn view-invariant, geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view image constraints and image-geometry constraints to encode 3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learning on the image-based tasks of semantic segmentation, instance segmentation and object detection on real-world indoor datasets, but moreover, provides significant improvement in the low data regime. We show significant improvement of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against baselines on ScanNet.

## Introduction 
In recent years, we have seen rapid progress in learning-based approaches for semantic understanding of 3D scenes, particularly in the tasks of 3D semantic segmentation, 3D object detection, and 3D semantic instance segmentation. Such approaches leverage geometric observations, exploiting the representation of points, voxels, or meshes to obtain accurate 3D semantics. These have shown significant promise towards realizing applications such as depth-based scene understanding for robotics, as well as augmented or virtual reality. In parallel to the development of such methods, the availability of large-scale RGB-D datasets has further accelerated the research area. 

One advantage of learning directly in 3D in contrast to learning solely from 2D images is that methods operate in metric 3D space; hence, it is not necessary to learn view-dependent effects and/or projective mappings. This allows training 3D neural networks from scratch in a relatively short time frame and typically requires a (relatively) small number of training samples; e.g. state-of-the-art 3D neural networks can be trained with around 1000 scenes from ScanNet. Our main idea is to leverage these advantages in the form of 3D priors for image-based scene understanding. 

Simultaneously, we have seen tremendous progress on representation learning in the image domain, mostly powered by the success of recent contrastive learning based methods. The exploration in 2D representation learning heavily relies on the paradigm of instance discrimination, where different augmented copies of the same instance are drawn closer. Different invariances can be encoded from those low-level augmentations such as random cropping, flipping and scaling, as well as color jittering. However, despite the common belief that 3D view invariance is an essential property for a capable visual system, there remains little study linking the 3D priors and 2D representation learning. The goal of our work is to explore the combination of contrastive representation learning with 3D priors, and offer some preliminary evidence towards answering an important question: can 3D priors help 2D representation learning?  

To this end, we introduce Pri3D, which aims to learn with 3D priors in a pre-training stage and subsequently use them as initialization for fine-tuning on image-based downstream tasks such as semantic segmentation, detection, and instance segmentation. More specifically, we introduce geometric constraints to a contrastive learning scheme, which are enabled by multi-view RGB-D data that is readily available. We propose to exploit geometric correlations implicit multi-view constraints between different images through the correspondence of pixels which correspond to the same geometry, as well as explicit correspondence of geometric patches which correspond to image regions. This imbues geometric knowledge into the learned representations of the image inputs which can then be leveraged as pre-trained features for various image-based vision tasks, particularly in the low training data regime.

We demonstrate our approach by pre-training on ScanNet under these geometric constraints for representation learning, and show that such self-supervised pre-training (i.e., no semantic labels are used) results in improved performance of 2D semantic segmentation, instance segmentation and detection tasks. We demonstrate this not only on ScanNet data, but also generalizing to improved performance on NYUv2 semantic segmentation, instance segmentation and detection tasks. Moreover, leveraging such geometric priors for pre-training provides robust features which can consistently improve performance under a wide range of amount of training data available. While we focus on indoor scene understanding in this paper, we believe our results can shed light on the paradigm of representation learning with 3D priors and open new opportunities towards more general 3D-aware image understanding. 

![](/assets/images/pri3d-overview.png)
* Method Overview. During pre-training, we use geometric constraints from RGB-D reconstructions to learn 3D priors for image-based representations. Specifically, we propose a contrastive learning formulation that models multi-view correspondences (View-Invariant Contrastive Loss) as well as geometry-to-image alignments (Geometric Prior Contrastive Loss). Our Pri3D pre-training strategy embeds geometric priors into the learned representations (in a form of pre-trained 2D convolutional network weights) that can be further leveraged for downstream 2D-only image understanding tasks. 

## Learning Representations from 3D Priors 
In this section, we introduce Pri3D: our key idea is to leverage constraints from RGB-D reconstructions, now readily available in various datasets, to embed 3D priors in image-based representations. From a dataset of RGB-D sequences, each sequence consists of depth and color frames, ${D_i}$ and ${C_i}$, respectively, as well as automatically-computed 6-DoF camera pose alignments ${T_i}$ (mapping from each camera space to world space) from state-of-the-art SLAM, all resulting in a reconstructed 3D surface geometry $S$. Specifically, we observe that multi-view constraints can be exploited in order to learn view-invariance without the need of costly semantic labels. In addition, we learn features through geometric representations given by the obtained geometry in RGB-D scans, again, without the need of human annotations. For both, we use state-of-the-art contrastive learning in order to constrain the multi-modal input for training. We show that these priors can be embedded in the image-based representations such that the learned features can be used as pre-trained features for purely image-based perception tasks; i.e., we can perform tasks such as image segmentation or instance segmentation on a single RGB image. An overview approach is shown in Figure 2.

### View-Invariant Learning
In 2D contrastive pre-training algorithms, a variety of data augmentations are used for finding positive matching pairs, such as MoCo and SimCLR. For instance, they use the random crops as self-supervised constraints within the same image for positive pairs, and correspondences to crops from other images as negative pairs. Our key idea is that with the availability of 3D data for training, we can leverage geometric knowledge to provide matching constraints between multiple images that see the same points. To this end, we use the ScanNet RGB-D dataset which provides a sequence of RGB-D images with camera poses computed by a state-of-the-art SLAM method, and reconstructed surface geometry $S$. Note that both the pose alignments and the 3D reconstructions were obtained in a fully-automated fashion without any user input.

For a given RGB-D sequence in the train set, our method then leverages the 3D data to finding pixel-level correspondences between 2D frames. We consider all pairs of frames $(i,j)$ from the RGB-D sequence. We then back-project frame $i$'s depth map $D_i$ to camera space, and transform the points into the world space by $T_i$. The depth values of frame $j$ are similarly transformed into world space. Pixel correspondences between the two frames are then determined as those whose 3D world locations like within 2cm of each other. We use the pairs of frames which have at least 30% overlap, with overlap computed as number of corresponding pixels in both frames divided by total number of points in the two frames. In total, we sample around 840k pairs of images from the ScanNet training data. 

In the training phase, a pair of sampled images is input to a shared 2D network backbone. In our experiments, we use a UNet-style backbone with ResNet architecture as an encoder, but note that our method is agnostic to the underlying encoder backbone. We then consider the feature map from decoder of the 2D backbone, where its size is half of the input resolution. For each image in the pair, we use the aforementioned pixel-to-pixel correspondences which refer to the same physical 3D point. Note that these correspondences may have different color values due to view-dependent lighting effects but represent the same 3D world location; additionally, the regions surrounding the correspondences appear different due to different viewing angles. In this fashion, we treat these pairs of correspondences as positive samples in contrastive learning; we use all non-matching pixels as negatives. Non-matching pixels are also defined within the set of correspondences. For a pair of frames with $n$ pairs of correspondences as positive samples, we use all $n(n-1)$ negative pairs (each of $n$ pixels from the first frame with each $n-1$ non-matching pixel from the second). Non-matching pixel-voxels are defined similarly but from a pair of frame and 3D chunk.

Between the features of matching and non-matching pixel locations, we then compute a PointInfoNCE loss, which is defined as: $$L_p = -\sum_{(a,b) \in M}\log{\frac{\exp(f_a \cdot f_b / \tau}{\sum_{(\cdot,k) \in M}\exp(f_a \cdot f_k /\tau}}$$, where $M$ is the set of pairs of pixel correspondences, ahd $f$ represents the associated feature vector of a pixel in the feature map. By leveraging multi-view correspondences, e apply implicit 3D priors-without any explicit 3D learning, we imbue view-invariance in the learned image-based features. 

### Geometric Prior
In addition to multi-view constraints, we also leverage explicit geometry-color correspondences inherent to the RGB-D data during training. For an RGB-D train sequence, the geometry-color correspondences are given by associating the surface reconstruction $S$ with the RGB frames of the sequence. For each frame $i$, we compute its view frustum in the world space. A volumetric chuck $V_i$ of $S$ is then cropped from the axis-aligned bounding box of the view frustum. We represent $V_i$ as a 2cm resolution volumetric occupancy grid from the surface. We thus consider pairs of color frames and geometric chuncks $(C_i, V_i)$.

From the color-geometry pairs $(C_i, V_i)$, we compute pixel-voxel correspondences by projecting the depth values for each pixel in the corresponding frame $D_i$ into world space to find an associated occupied voxel in $V_i$ that lies within 2cm of the 3D location of the pixel.

During training, we leverage the color-geometry correspondences with a 2D network backbone and a 3D network backbone. We use a UNet-style architecture with ResNet encoder for the 2D network backbone, and a UNet-sytle sparse convolutional 3D network backbone. Similarly to view-invariant training, we also take the output from the decoder of the 2D network backbone where its output size is half the input resolution. We then use the pixel-voxel correspondences in $(C_i, V_i)$ for contrastive learning, with positives as all matching pixel-voxel pairs and negatives as all non-matching pixel-voxel pairs. We apply the PointInfoNCE loss with $f_i$ as the 2D feature of a pixel, and $f_j$ is the feature vector from its 3D correspondence, and $M$ the set of 2D-3D pixel-voxel correspondences pairs. 

### Joint Learning
We can leverage not only the view-invariant constraints and geometric priors during training, but also learn jointly from the combination of both constraints. We can thus employ a shared 2D network backbone and a 3D network backbone, with the 2D network backbone constrained by both view-invariant constraints and as the 2D part of geometric prior constraint. 

During training, we consider $(C_i, C_j, V_i, V_j)$ of overlapping color frames $C_i$ and $C_j$ as well as $V_i$ and $V_j$ which have geometric correspondence with $C_i$, $C_j$ respectively. The shared 2D network backbone then processes $C_i$, $C_j$ and computes the view-invariant loss from Section 3.1. At the same time, $V_i$ and $V_j$ are processed by the 3D sparse convolutional backbone, with the loss relative to the features of $C_i$ and $C_j$ respectively. This embeds both constraints into the learned 2D representation. 


