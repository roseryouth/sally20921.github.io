---
layout: post
title:  "RandomRooms: Unsupervised Pre-training from Synthetic Shapes and "
categories: [ Paper ]
image: assets/images/demo1.jpg
---

## Abstract
> 3D point cloud understanding has made great progress in recent years. However, one major bottleneck is the scarcity of annotated real datasets, especially compared to 2D object detection tasks, since a large amount of labor is involved in annotating the real scans of a scene. A promising solution to this problem is to make better use of the synthetic dataset, which consists of CAD object models, to boost the learning on real datasets. This can be achieved by the pre-training and fine-tuning procedure. However, recent work on 3D pre-training exhibits failure when transfer features learned on synthetic objects to other real-world applications. In this work, we put forward a new method called RandomRooms to accomplish this objective. In particular, we propose to generate random layouts of a scene by making use of the objects in the synthetic CAD dataset and learn the 3D scene representation by applying object-level contrastive learning on two random scenes generated from the same set of synthetic objects. The model pre-trained in this way can serve as a better initialization when later fine-tuning on the 3D object detection task. Empirically, we show consistent improvement in downstream 3D detection tasks on several base models, especially when less training data are used, which strongly demonstrates the effectiveness and generalization of our method. Benefiting from the rich semantic knowledge and diverse objects from synthetic data, our method establishes the new state-of-the-art on widely-used 3D detection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide a new perspective for bridging object and scene-level 3D understanding. 

![](/assets/images/rr-title.png)
* The main idea of *RandomRooms*. To generate two different layouts, we randomly place the same set of objects sampled from synthetic datasts in rectangular rooms. With the proposed object-level contrastive learning, models pretrained on these pseudo scenes can serve as a better initialization for downstream 3D object detection task.


## Introduction 
Recent years have witnessed great progress in 3D deep learning, especially on 3D point clouds. With the emergence of powerful models, we are now able to make significant breakthroughs on many point cloud tasks, ranging from object-level understanding ones to scene-level understanding ones, such as 3D object detection and 3D semantic segmentation. These scene-level tasks are considered to be more complicated and more important as they often require higher level understanding compared to object level tasks like shape classification. One of the most important tasks for 3D point cloud scene understanding is the 3D object detection, which aims at localizing the objects of interest in the point cloud of the scene and telling the category they belong to. However, one major bottleneck that hinders the researchers from moving forward is the lack of large-scale real datasets, considering the difficulty in collecting and labeling high-quality 3D scene data. Compared to 2D object detection task where we have large annotated real datasets COCO, the real datasets here we use for 3D object detection task are much smaller in scales, and generating a synthesized scene dataset also involves a heavy workload in modeling and rendering.

A preferred solution is to utilize synthetic CAD object models to help the learning of 3D object detector since it is much easier to access such type of data. Considering we have no annotation of bounding box for synthetic CAD data, this idea can be achieved in a similar way as the unsupervised pre-training for 2D vision takss where we first pretrain on a large-scale dataset in an unsupervised manner and then fine-tune on a smaller annotated dataset. Yet, most previous works focus on the pretraining for single object level tasks, such as reconstruction, shape classification or part segmentation, or on some low-level tasks like registration. A recent work, namely PointContrast, first explores the possibility of pre-training in the context of 3D representation learning for higher level scene understanding tasks, i.e. 3D detection and segmentation. Nevertheless, they conduct the pre-training on the real scene dataset and provide a failure case when pre-training the backbone model on ShapeNet, which consists of synthetic CAD object models. They attribute this unsuccessful attempt to two reasons, that is, the domain gap between real and synthetic data as well as the insufficiency of capturing point-level representation by directly training on single objects. Despite these difficulties, it is still desirable to make the ShapeNet play the role of ImageNet in 2D vision since it is easy to obtain a large number of synthetic CAD models. 

In this work, we put forward a new framework to show the possibility of using a synthetic CAD model dataset, i.e. ShapeNet, for the 3D pre-training before fine-tuning on downstream 3D object detection task. To this end, we propose a method named RandomRoom. In particular, we propose to generate two different layouts using one set of objects which are randomly sampled out of the ShapeNet dataset. Having these two scenes that are made up of the same set of objects, we can then perform the contrastive learning at the object level to learn the 3D scene representation.

Different from PointContrast where the contrastive learning is performed at the point level, our approach has two advantages. One is to remove the requirement of point correspondence between two views, which is indispensable in PointContrast given that it is necessary to exploit such information to obtain positive and negative pairs for the contrastive learning. This requirement limits the applications of PointContrast, since the CAD model datasets like ShapeNet and many other real-world datasets like SUN RGB-D cannot provide such information. The other advantage is that our method can support more diverse backbone models. Most state-of-the-art models on tasks like 3D object detection apply PointNet++ style models as their backbone, and replacing it with Sparse Res-UNet may lead to the drop of accuracy, according to the PointContrast. However, PointContrast cannot well support the pre-training of PointNet++ style model as the UNet-like models, since the point correspondence may be missing after each abstraction level in PointNet++. With the proposed RandomRoom, we are enabled to perform contrastive learning at the level of objects and thus better support the pretraining of PointNet++ like models as we no longer need to keep the point correspondence for contrastive learning like PointContrast.

Our method is straightforward yet effective. We conduct the experiments on the 3D object detection task where only the geometric information is available for input as the models in CAD datasets do not carry color information. The results of empirical study strongly demonstrate the effectiveness of our method. In particular, we achieve the state-of-the-art of 3D object detection on two widely used benchmarks, ScanNetV2 and SUN-RGBD. Furthermore, our method can achieve even more improvements when much less training samples are used, demonstrating that our model can learn a better initialization for 3D object detection. 

![](/assets/images/rr.png)

## RandomRooms
In this section, we describe the details of the proposed RandomRooms method. We first briefly review existing contrastive representation learning methods and illustrate the intuition of our method in Section 3.1. Then, we describe how to use synthetic objects to construct random rooms in 3.2. In Section 3.3, we show our pretrain task for learning scene level representation from the pseudo scenes. The overview of our framework is represented in Figure 2.

### Overview of Contrastive Learning 
We begin by reviewing the existing contrastive representation learning methods for 2D and 3D understanding to illustrate the motivation of our method. 

Contrastive learning is at the core of several recent methods on unsupervised learning, which exhibits promising performance on both 2D and 3D tasks and shows impressive generalization ability as a new type of pre-training method for various downstream tasks. The key ingredient of contrastive learning is constructing positive and negative pairs to learn discriminative representation, which inherits the idea of conventional contrastive learning in metric learning literature. Given an input $x$ and its positive pair $x_{+}$ and a set of negative examples ${x_i}$, a commonly used training objective for contrastive representation learning is based on InfoNCE: $$L_{contrastive} = -log{\frac{\exp(\phi(x) \cdot \phi(x_{+})/\tau)}{\sum_i \exp(\phi(x) \cdot \phi(x_i)/\tau)}}$$ where $\phi$ is the encoder network that maps the input to a feature vector and $\tau$ is a temperature hyper-parameter. Intuitively, the contrastive learning methods supervise models by encouraging the features of the different views of the same sample to be close to each other and distinguishable from other samples. Hence the quality of positive pairs and negative examples is a critical factor is a critical factor to learn the encoder. 

Since category annotations are not available in the unsupervised learning scenario, a common practice is using different augmentations of an input as the positive pairs and treating all other samples as negative samples. Although this design has proven to be effective in image representation learning, we argue there is a better solution to construct positive pairs for 3D understanding. One fundamental difference between 2D and 3D data is that the spatial structures of pixels do not reflect the actual geometric structures of the objects, but the spatial structures in 3D data always faithfully illustrate the layouts in the real world. This property suggests that it may be easier to manipulate or *augment* 3D data compared to 2D images. Inspired by the rendering techniques in computer graphics, we propose to generate positive pairs of 3D scenes by randomly manipulating the layouts of 3D objects in a scene. Since we only need 3D objects instead of the whole scene in this process, our method makes it possible to use 3D object models to promote scene level representation learning. 

It is worth noting that a recent work, namely PointContrast, explores 3D contrastive representation learnign by using 3D point clouds from different views as the positive pair, where a point level contrastive loss is designed. This method is baed on the multi-view point cloud sequences provided in ScanNetV2. Instead, our method focuses on leveraging object level 3D data, which are easier to collect and have more diverse categories.

### Random Rooms from Synthetic Objects
Compared to ScanNetV2, which contains ~15k objects from 17 categories, synthetic shape datasets like ShapeNet provide a more plentiful source for 3D understanding. For example, ShapeNetCore contains ~52k objects from 55 categories. Therefore, the primary goal of this paper is to study how to use synthetic CAD models collected by ShapeNet to improve downstream tasks like 3D detection and segmentation on real-world datasets.

Previous work shows that directly pre-training on ShapeNet will not yield performance improvement on downstream detection and segmentation task. We suspect the main reason is the domain gap between the single object classification taks on ShapeNet and the multiple objects localization task on real-world datsets. In order to bridge the gap, we propose to generate pseudo scenes (we name them *random rooms*) from synthetic objects to construct the training data that are helpful ofr scene level understanding. 

Given a set of randomly sampled objects, we generate a random room following the three steps:
* **Object Augmentation**: We first resize the object to a random size in $[0.5m, 2.0m]$ to ensure the objects have similar sizes as objects in ScanNetV2. Then, we apply commonly used object point cloud augmentation techniques including rotation, point dropping, and jittering.

* **Layout Generation**: For the ease of implementation, we place objects in a rectangular room. The size of the room is adaptively adjusted according to the overall area of the augmented objects. The layout is generated based on two simple principles: 1) non-overlapping: any two objects should not occupy the same space in the room; 2) gravity: objects should not float in the air, and larger objects should not be placed over the smaller ones. In turn, we place objects in the descending order of the area. Insired by *Tetris*, for each object, we first randomly choose a position in the X-Y plane that satisfies the above principles, then determine the location (the Z value) based on the current maximum height of the position. The object will not be placed in a position if the current maximum height of the position exceeds 2m. 

* **Scene Augmentation**: Lastly, we apply data augmentation like rotation along the $Z$ axis, point dropping, jittering to the whole scene. To make the generated scenes more similar to the real scenes, we also add the floor and walls as confounders. 

Some examples of the random rooms are illustrated in Figure 6.

### Representation Learning from Random Rooms
To utilize the generated random rooms, we devise an object-level contrastive learning (OCL) method, which learns discriminative representation without category annotations.

Givene $n$ randomly sampled objects $\\{ x_1, x_2, \dots, x_n \\}$, we first generate two random rooms $R_A = \\{x^A_1, x^A_2, \dots, x^A_n\\}$ and $R_B = \\{x^B_1, x^B_2, \dots, x^B_n\\}$ by conducting the above mentioned steps individually. Then we employ the point cloud encoder-decoder network $\mathcal{M}$ (e.g. PointNet++ with feature propagation layers) to extract per-point features of the two scenes $F_A = \mathcal{M}(R_A)$ and $F_B = \mathcal{M}(R_B)$. Since the random room is constructed by several individual objects, the instance labels can be naturally defined. The goal of object-level contrastive learning is to exploit instance labels as a source of free and plentiful supervisory signals for training a rich representation for point cloud understanding. To obtain the feature of each object, we apply the average pooling operation $\mathcal{A}$ on per-point features belonging to this object: $$ \begin{split} \\{h^A_1, h^A_2, \dots, h^A_n\\} = \mathcal{A}(F_A), \\ \\{h^B_1, h^B_2, \dots, h^B_n\\} = \mathcal{A}(F_B) \end{split}$$. Similar to the common practice in contrastive learning, the object features are projected onto a unit hypersphere using a multi-layer perceptron network (MLP) followed by L2 normalization. The object-level contrastive learning objective can be written as $$ L_{OCL} = - \frac{1}{n}\sum_{i=1}^n \log{\frac{\exp(f^A_i \cdot f^B_i /\tau)}{\sum_{f \in \mathcal{F} \exp(f^A_i \cdot f/\tau)}}} - \frac{1}{n}\sum_{i=1}^n \log{\frac{\exp(f^B_i \cdot f^A_i /\tau)}{\sum_{f \in \mathcal{F} \exp(f^B_i \cdot f/\tau)}}} $$, where $f_i^A = \phi(h_i^A)$ and $f_i^B = \phi(h_i^B)$ are the projected features of the $i$-th object in $R_A$ and $R_B$ respectively, $\phi$ is the projection head and $\mathcal{F}$ is the set of all projected features in the batch. Note that compared to point-level contrastive learning task in PointContrast, our method further utilizes the instance-level knowledge thanks to the generation mechanism of RandomRooms. We argue that objec-level contrastive learning introduces more semantic knowledge and can be more helpful for downstream localization tasks. 


![](/assets/images/rr-00.png)

## Experiments
One primary goal of representation learning is to learn the representation that can transfer to downstream tasks. To apply our RandomRooms method to scene level understanding taks like 3D object detection, we adopt the *unsupervised pre-training* + *supervised fine-tuning* pipeline. Specifically, we first pre-train the backbone model on ShapeNet using our method, then we use the pre-trained weights as the initialization and further fine-tune the model on the downstream 3D object detection task. 

### Pre-training Setups
We perform the pre-training on ShapeNet, a dataset composed of richly-annotated shapes represented by 3D CAD models of objects from 55 common categories. To generate the random room, we first need to randomly sample multiple objects from the dataset. The number of objects we sample is a random integer from 12 to 18, which is similar to the average number of objects in ScanNetV2 scenes. Then for each sampled object, we perform the random room generation algorithm mentioned in Section 3.2. The object-level contrastive learning loss is used to train the model in an unsupervised manner. 

For the downstream 3D object detection task, we use the backbone model which take as input 40,000 points. Following the network configurations in these two works, we use the 1024-point feature as the output of the backbone models and perform contrastive learning one this feature. During pre-training, we use the Adam optimizer with initial learning 0.001. We train the model for 300 epochs and the learning rate is multiplied by 0.1 at the 100-th and 200-th epoch. The batch size is set to 16 such that roughly 200~300 unique objects are involved in the contrastive learning at every iteration.



