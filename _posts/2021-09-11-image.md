---
layout: post
title:  "Image Processing: the Basics"
author: seri
categories: [ computer vision ]
image: assets/images/depth/1.jpg
tags: featured
excerpt: "This blog post deals with the basics of image processing. Through digital processing you can avoid problems such as the build-up of noise or distortion. Digital image processing may also be modeled in the form of multidimensional systems."
---

<!--more-->

<h2> Introduction: What is a Digital Image? </h2>
A digital image is a numeric representation of a two-dimensional image and is made of picture elements called pixels, arranged in rows and columns. These numeric values are the intensity or brightness values that are associated with the pixels. 

The concept of pixel is closely related with image sensors. Image sensor is a device that converts the light energy into an electric signal. In digital cameras, these sensors are arranged in the form of a 2D array on a chip. Mostly there is a one-to-one correspondence between a pixel and a sensor meaning each sensor produces 1 pixel. Thus, the total number of pixels in an image is equal to the total number of sensors on a chip. Sometimes, multiple sensors are used to produce a pixel of information.

8MP camera means there are $8 \times 10^6$ image sensors in the camera chip and if there is a one-to-one correspondence between a pixel and a sensor then the image contains $8 \times 10^6$ pixels. Larger the pixel, better will be the image quality.

<h2> Grayscale and Color Image </h2>

A grayscaled image is the one where pixels contains only the intensity information and not wavelength. For example, a pixel value of 165 represents the amount of light captured by the sensor (which is basically the pixel). For an 8-bit image, there will be $2^8 = 256$ intensity levels where $0$ means no light(black) and $255$ means white light and in between levels are the representative of the shades of gray.

A color image, on the other hand, contains intensity information corresponding to three wavelengths red, green and blue collectively called primary colors or channels. The reason for choosing these 3 colors lies in the fact that cone cells in the human eye, that is responsible for color vision, are senstive to red, green and blue light. Now combining the primary colors in various intensity proportions produces all visible colors.

The drastic advancement in color image formation came with the introduction of digital sensors like CCD or CMOS. In 1976, Bryce Bayer invented the <span color="blue"> Bayer Filter </span> that revolutionized color image formation. Bayer Filter is a color filter array (CFA) in which RGB color filters are arranged in a pattern on the sensor. 

In the Bayer Filter, there are twice as many green elements compared to red and blue elements to mimic the human eye's greater resolving power with the green light. In order to construct an RGB picture, we calculate green and blue values for each red pixel, blue and red values for each green pixel and so on. This happens via interpolation or color demosaicing algorith.

<h2> Understanding Image Histograms </h2>

An image histogram tells us how the intensity values are distributed in an image. For 1D histogram, we plot the intensity values on the $x$-axis and the number of pixels corresponding to intensity values on the $y$-axis. We are taking only one feature into our consideration, i.e. grayscale intensity values of the pixel. 

For a dark image, the histogram will cover mostly the leftside and center of the graph, while for a bright image, the histogram mostly rests on the right side and center of the graph. Let's see how to plot histogram for an image using OpenCV.

```python
cv2.calcHist(image, channel, mask, bins, range):
	'''
	Parameters
	_____
	image: list
		input image
	channel: list
		index of the channel ([0],[1],[2])
	mask: provide if you want to calculate histogram for specific region otherwise pass None
	bins: int
		number of bins to use for each channel, should pass 255
	range: [0,256]
		range of intensity values for 8-bit pass as [0,256]

	Returns
	______
	numpy.ndarray [n_bins, 1]

	cf. why 1? % of pixels 
	'''
```
We then can plot the output using matplotlib.
```python 
import cv2
import matplotlib.pyplot as plt
image = cv2.imread('input.jpg',0)
hist = cv2.calcHist([image], [0], None, [256], [0,256])
plt.plot(hist)
```
<h2> Image Demosaicing or Interpolation Methods </h2>
According to Wikipedia, <span class="blue"> interpolation </span> is a method of constructing new data points within the range of a discrete set of known data points. Image interpolation refers to the "guess" of intensity values at missing locations.

The big question is why we need interpolation if we are able to capture intensity values at all pixels using image sensor? Well, there could be a scenario where we have to project low-resolution image to a high-resolution screen. For example, we prefer watching videos in the full-screen mode. Also, there coul be image inpainting, image warping or geometric transformations and so on. 

Interpolation algorithms can be classified as 

<div color="mermaid">
stateDiagram-v2
    Interpolation --> Adaptive
    Interpolation --> NonAdaptive
    NonAdaptive --> NearestNeighbor
    NonAdaptive --> Billinear
    NonAdaptive --> BiCubic
    Adaptive --> EdgeSensing
    Adaptive --> ColorCorrection
</div>

Non-adaptive perform interpolation in a fixed pattern for every pixel, while adaptive algorithms detect local spatial features, like edges, pixel neighborhood and make effective choices depending on the algorithm.

<h3> Nearest Neighbor Interpolation </h3>
The basic idea is simple:
<blockquote> We assign unknown pixels to the nearest known pixel. </blockquote>

The figure below shows an example of the Nearest Neighbor Interpolation. 
<picture><img src="{{site.baseurl}}/assets/images/NN.png"></picture>

The process can be summarized as folows:
1. Project $4 \times 4$ image on the $2 \times 2$ image. We can easily find out the coordinates of each unknown pixel.
2. Compare the above calculated coordinates of each unknown pixel with the input image pixels to find out the nearest pixel. 

This is the fastest interpolation method as it involves little calculation. This results in a blocky image. 

<h3> Bilinear Interpolation </h3>

Bilinear interpolation means applying a linear interpolation in two directions. Thus, it uses 4 nearest neighbors, takes their weighted average to produce the output.

<h2> Spatial Filtering </h2>
Now, we will discuss another image enhancement method known as Spatial Filtering, that transforms the intensity of a pixel according to the intensities of the neighboring pixels. 

First, let's dicuss what a spatial filter is. A spatial filter is a window with some height and width that is usually much less than that of the image. (c.f. mostly $3 \times 3$, $5 \times 5$ sized filters are used). The value of the filters are called coefficients or weights. There are other terms to call filter such as mask, kernel, template, or window. 

Now, let's see the mechanism of Spatial Filtering. The spatial filtering can be characterized as <span class="blink"> shift-and-multiply </span> operation. We place the filter over a portion of an image. Then we multiply the filter weights (or coefficients) with the corresponding image pixel values, and sum these up. The center image pixel value is replaced with the result obtained. Then shift the filter to a new location and repeat the process again. 

This process is actually known as <span class="red"> correlation </span> in mathematics, but when talking about computer vision and deep learning, we refer to this as <span class="blue"> convolution </span> operation. This should not be confused with mathematics convolution. 

<div class="sidenote"> The mathematics convolution is similar to correlation except that the mask is first flipped both horizontally and vertically. </div>

If you want the output image to be of the same size as that of the input, then you must change the padding to
```python
m, n = kernel.shape
pad_y = (m-1)//2
pad_x = (n-1)//2
```
<h3> Understanding Frequency in Images </h3>
Before we move forward, let's discuss an important concept <span class="underline"> frequency </span>, which is widely used in spatial filtering. Frequency in images is the rate of change of intensity values. Thus, a high-frequency image is the one where the intensity values quickly change from one pixel to the next. On the other hand, a low-frequency image may be the one tha is relatively uniform in brightness or where intensity changes very slowly. Most images contain both high-frequency and low-frequency components. 
So, another domain other than frequency domain exists. This is obtained by applying a Fourier Trnasformation on image that is currently in spatial domain which is either a 2D-matrix (grayscale image) or 3D vectors of 2D matrices (RGB image). But why do we need a domain other than the spatial domain? Many times, image processing  tasks are best performed in a domain other than the spatial domain. It is easier to detect some features in a particular domain. 

It's not hard to conclude that edges in an image represents high frequency because the intensity changes drastically across an edge. Based on the frequency, we can classify the filters as <span class="highlight-pink"> Low Pass Filters </span> and <span class="highlight-green"> High Pass Filters </span>. 

Low Pass Filters block high-frequency parts of an image and thus results in blurring or image smoothing. On the other hand, a high pass filter enhances high-frequency parts of an image (i.e. edges) and thus results in image sharpening. 

<h3> Adding Different Noises to an Image </h3>
Image noise is a random variation in the intensity values. Thus, by randomly inserting some values in an image, we can reproduce any noise pattern. For example:

<ul><li> adding Gaussian noise </li></ul>

```python
import cv2 
import numpy as np

img = cv2.imread('input.png') 
# [H,W,C] => returns a numpy array

# generate Gaussian noise
gauss = np.random.normal(0,1,img.size)
# np.random.normal(loc=0.0, scale=1.0, size=None)
# if the given shape is (m,n,k), then m*n*k samples are drawn
guass = gauss.reshape(img.shape[0], img.shape[1])

# add the Gaussian noise to the image
img_gauss = cv2.add(img, gauss)

# display the image
cv2.imshow('gauss', img_gauss)
cv2.waitKey(0)
```

<h3> Smoothing Filters </h3>
Low Pass Filters are mainly used for blurring and noise reduction. Both of these can serve as a useful pre-processing step in many applications. We are going to see some of the most commonly used blurring techniques.

<ul><li> <mark> <b> Averaging </b> </mark> </li>

<span class="indent">  In this, each pixel value in an image is replaced by the weighted average of the neighborhood (defined by the filter mask) intensity values. The most commonly used filter is the Box Filter which has equal weights. </span> 

<li> <mark> <b> Median Blurring </b> </mark> </li>

<span class="indent">  This is a non-linear filtering technique. As clear from the name, this takes a median of all the pixels under the kernel area and replaces the central element with the median value. This is quite effective in reducing certain type of noise with considerably less edge blurring as compared to other linear filters of the same size. </span>

Because we are taking a median, the output image will have no new pixel values other than that in the input image. 

<div class="sidenote"> For an even number of entries, there is more than one possible median, thus kernel size must be odd and greater than 1 for simplicity. </div>

<h3> Gaussian Blurring </h3>
 



<h2> References </h2>
<ul><li><a href="https://theailearner.com/2018/10/06/what-is-a-digital-image/"> TheAILearner </a></li>
</ul>

